{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 5: Credit Card Fraud Detection\n",
    "\n",
    "**Release Date:** 6 October 2023\n",
    "\n",
    "**Due Date:** 23:59, 21 October 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In class, we discussed about logistic regression, and how it can be useful as a classification algorithm. In this problem set, we get some hands-on practice by implementing logistic regression on a Credit Card Fraud Detection dataset. Note that for this problem set, you should only be using the scikit-learn (sklearn) library for the last part (Tasks 5.x) on SVM.\n",
    "\n",
    "**Required Files**:\n",
    "\n",
    "* ps5.ipynb\n",
    "* credit_card.csv\n",
    "* restaurant_data.csv\n",
    "\n",
    "**Honour Code**: Note that plagiarism will not be condoned! You may discuss with your classmates and check the internet for references, but you MUST NOT submit code/report that is copied directly from other sources!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important\n",
    "\n",
    "Similar to PS0, your implementation in the following tasks **should NOT involve any iteration, including `map` and `filter`, or recursion**. Instead, you should work with the operations available in NumPy. Solutions that violate this will be penalised.\n",
    "\n",
    "- You are allowed to use any mathematical functions, but this does not mean that you are allowed to use any NumPy function (there are NumPy functions that arenâ€™t mathematical functions). For example, `np.vectorize` is not allowed since it is iterative. If you are in doubt about which functions are allowed, you should ask in the forum.\n",
    "\n",
    "There is, however, an exception for **Tasks 3.4 and 3.5**. In the pseudo-code for the algorithm required, there is an explicit for loop. Hence, only for these task you may use **a single for/while loop** to iterate for the number of epochs required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Fraudulent credit card transaction is a common phenomenon in many parts of the world and can lead to potentially large amounts of losses for both companies and customers. Therefore, we hope to help credit card companies recognize those fraudulent transactions so that customers are not charged for items that they did not purchase.\n",
    "\n",
    "We are given a dataset that contains transactions made by credit cards holders in `credit_card.csv`. If we think about what type of data might be included in the input variables under the given context, we might realize that those input variables are likely to include word descriptions, such as shop name or locations. In this problem set, we don't need to worry about language processing as the data are pre-processed to contain only numeric values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "Take a look at the columns in the dataset `credit_card.csv`. We have V1-V20, 'Amount', and 'Time' as input features, and 'Class' as output which takes the value 1 if it's fraud and 0 otherwise. This dataset presents 492 frauds out of 284,807 transactions. That means, there are 284,808 rows (including the header) in the csv file.\n",
    "\n",
    "We will use this dataset to implement logistic regression using batch and stochastic gradient descent for binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial imports and setup\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn import model_selection\n",
    "\n",
    "# Read credit card data into a Pandas dataframe for large tests\n",
    "\n",
    "dirname = os.getcwd()\n",
    "credit_card_data_filepath = os.path.join(dirname, 'credit_card.csv')\n",
    "restaurant_data_filepath = os.path.join(dirname, 'restaurant_data.csv')\n",
    "\n",
    "credit_df = pd.read_csv(credit_card_data_filepath)\n",
    "X = credit_df.values[:, :-1]\n",
    "y = credit_df.values[:, -1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics of Pandas\n",
    "\n",
    "[Pandas](https://pandas.pydata.org/) is an open source data analysis and manipulation tool in Python. In this problem set, we read the CSV into a [Pandas dataframe](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) to provide some nifty methods that makes it easier for us to handle large amounts of data. You can think of a dataframe as a large table that stores our dataset in a neat and optimized manner, making it fast for retrieval and manipulation and data. Using Pandas, we can quickly gain an overview of the type and values of the data stored, distributions of values within the dataset, and even ways to perform sampling.\n",
    "\n",
    "In the new few sections, we will explore some basic functions of Pandas to help us get started. You do not need to submit any codes for this section, and can simply run the cells to follow along."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the [`head`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.head.html) method on the dataframe, we can get an overview of the data. By default, the method returns the first 5 entries in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V13</th>\n",
       "      <th>V14</th>\n",
       "      <th>V15</th>\n",
       "      <th>V16</th>\n",
       "      <th>V17</th>\n",
       "      <th>V18</th>\n",
       "      <th>V19</th>\n",
       "      <th>V20</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.991390</td>\n",
       "      <td>-0.311169</td>\n",
       "      <td>1.468177</td>\n",
       "      <td>-0.470401</td>\n",
       "      <td>0.207971</td>\n",
       "      <td>0.025791</td>\n",
       "      <td>0.403993</td>\n",
       "      <td>0.251412</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>0.489095</td>\n",
       "      <td>-0.143772</td>\n",
       "      <td>0.635558</td>\n",
       "      <td>0.463917</td>\n",
       "      <td>-0.114805</td>\n",
       "      <td>-0.183361</td>\n",
       "      <td>-0.145783</td>\n",
       "      <td>-0.069083</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.717293</td>\n",
       "      <td>-0.165946</td>\n",
       "      <td>2.345865</td>\n",
       "      <td>-2.890083</td>\n",
       "      <td>1.109969</td>\n",
       "      <td>-0.121359</td>\n",
       "      <td>-2.261857</td>\n",
       "      <td>0.524980</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>0.507757</td>\n",
       "      <td>-0.287924</td>\n",
       "      <td>-0.631418</td>\n",
       "      <td>-1.059647</td>\n",
       "      <td>-0.684093</td>\n",
       "      <td>1.965775</td>\n",
       "      <td>-1.232622</td>\n",
       "      <td>-0.208038</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>1.345852</td>\n",
       "      <td>-1.119670</td>\n",
       "      <td>0.175121</td>\n",
       "      <td>-0.451449</td>\n",
       "      <td>-0.237033</td>\n",
       "      <td>-0.038195</td>\n",
       "      <td>0.803487</td>\n",
       "      <td>0.408542</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V13       V14       V15       V16       V17  \\\n",
       "0  0.098698  0.363787  ... -0.991390 -0.311169  1.468177 -0.470401  0.207971   \n",
       "1  0.085102 -0.255425  ...  0.489095 -0.143772  0.635558  0.463917 -0.114805   \n",
       "2  0.247676 -1.514654  ...  0.717293 -0.165946  2.345865 -2.890083  1.109969   \n",
       "3  0.377436 -1.387024  ...  0.507757 -0.287924 -0.631418 -1.059647 -0.684093   \n",
       "4 -0.270533  0.817739  ...  1.345852 -1.119670  0.175121 -0.451449 -0.237033   \n",
       "\n",
       "        V18       V19       V20  Amount  Class  \n",
       "0  0.025791  0.403993  0.251412  149.62      0  \n",
       "1 -0.183361 -0.145783 -0.069083    2.69      0  \n",
       "2 -0.121359 -2.261857  0.524980  378.66      0  \n",
       "3  1.965775 -1.232622 -0.208038  123.50      0  \n",
       "4 -0.038195  0.803487  0.408542   69.99      0  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credit_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can inspect the [value counts](https://pandas.pydata.org/docs/reference/api/pandas.Series.value_counts.html) of the 'Class' property in the dataframe to know the number of fraudulent and non-fraudulent transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    284315\n",
       "1       492\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the number of fraudulent and non-fraudulent transactions.\n",
    "credit_df['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing and selecting data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to NumPy, we can also index and select data on Pandas dataframes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, we can [select columns in the dataframe by their labels](https://pandas.pydata.org/docs/user_guide/indexing.html#basics). In the following example, we use `'Class'` to index the 'Class' column in the credit dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         0\n",
       "1         0\n",
       "2         0\n",
       "3         0\n",
       "4         0\n",
       "         ..\n",
       "284802    0\n",
       "284803    0\n",
       "284804    0\n",
       "284805    0\n",
       "284806    0\n",
       "Name: Class, Length: 284807, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select the 'Class' column in the credit dataframe\n",
    "credit_df['Class']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, we can use integer indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V13</th>\n",
       "      <th>V14</th>\n",
       "      <th>V15</th>\n",
       "      <th>V16</th>\n",
       "      <th>V17</th>\n",
       "      <th>V18</th>\n",
       "      <th>V19</th>\n",
       "      <th>V20</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.991390</td>\n",
       "      <td>-0.311169</td>\n",
       "      <td>1.468177</td>\n",
       "      <td>-0.470401</td>\n",
       "      <td>0.207971</td>\n",
       "      <td>0.025791</td>\n",
       "      <td>0.403993</td>\n",
       "      <td>0.251412</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>0.489095</td>\n",
       "      <td>-0.143772</td>\n",
       "      <td>0.635558</td>\n",
       "      <td>0.463917</td>\n",
       "      <td>-0.114805</td>\n",
       "      <td>-0.183361</td>\n",
       "      <td>-0.145783</td>\n",
       "      <td>-0.069083</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "\n",
       "         V8        V9  ...       V13       V14       V15       V16       V17  \\\n",
       "0  0.098698  0.363787  ... -0.991390 -0.311169  1.468177 -0.470401  0.207971   \n",
       "1  0.085102 -0.255425  ...  0.489095 -0.143772  0.635558  0.463917 -0.114805   \n",
       "\n",
       "        V18       V19       V20  Amount  Class  \n",
       "0  0.025791  0.403993  0.251412  149.62      0  \n",
       "1 -0.183361 -0.145783 -0.069083    2.69      0  \n",
       "\n",
       "[2 rows x 23 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtain the first 2 rows\n",
    "credit_df[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also [select columns in the dataframe that fulfils some condition](https://pandas.pydata.org/docs/user_guide/indexing.html#boolean-indexing). In the example below, `credit_df['Class'] == 0` returns a [Pandas Series](https://pandas.pydata.org/docs/reference/api/pandas.Series.html) of length 284807, which is the size of our dataset. It contains the value `True` if and only if the `Class` value for the particular entry is of value 0, and `False` otherwise. We can use this Boolean series to index the credit dataframe to return the rows where the `Class` field is 0. Does this remind you of how NumPy arrays operate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         True\n",
       "1         True\n",
       "2         True\n",
       "3         True\n",
       "4         True\n",
       "          ... \n",
       "284802    True\n",
       "284803    True\n",
       "284804    True\n",
       "284805    True\n",
       "284806    True\n",
       "Name: Class, Length: 284807, dtype: bool"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credit_df['Class'] == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V13</th>\n",
       "      <th>V14</th>\n",
       "      <th>V15</th>\n",
       "      <th>V16</th>\n",
       "      <th>V17</th>\n",
       "      <th>V18</th>\n",
       "      <th>V19</th>\n",
       "      <th>V20</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.991390</td>\n",
       "      <td>-0.311169</td>\n",
       "      <td>1.468177</td>\n",
       "      <td>-0.470401</td>\n",
       "      <td>0.207971</td>\n",
       "      <td>0.025791</td>\n",
       "      <td>0.403993</td>\n",
       "      <td>0.251412</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>0.489095</td>\n",
       "      <td>-0.143772</td>\n",
       "      <td>0.635558</td>\n",
       "      <td>0.463917</td>\n",
       "      <td>-0.114805</td>\n",
       "      <td>-0.183361</td>\n",
       "      <td>-0.145783</td>\n",
       "      <td>-0.069083</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.717293</td>\n",
       "      <td>-0.165946</td>\n",
       "      <td>2.345865</td>\n",
       "      <td>-2.890083</td>\n",
       "      <td>1.109969</td>\n",
       "      <td>-0.121359</td>\n",
       "      <td>-2.261857</td>\n",
       "      <td>0.524980</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>0.507757</td>\n",
       "      <td>-0.287924</td>\n",
       "      <td>-0.631418</td>\n",
       "      <td>-1.059647</td>\n",
       "      <td>-0.684093</td>\n",
       "      <td>1.965775</td>\n",
       "      <td>-1.232622</td>\n",
       "      <td>-0.208038</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>1.345852</td>\n",
       "      <td>-1.119670</td>\n",
       "      <td>0.175121</td>\n",
       "      <td>-0.451449</td>\n",
       "      <td>-0.237033</td>\n",
       "      <td>-0.038195</td>\n",
       "      <td>0.803487</td>\n",
       "      <td>0.408542</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284802</th>\n",
       "      <td>172786.0</td>\n",
       "      <td>-11.881118</td>\n",
       "      <td>10.071785</td>\n",
       "      <td>-9.834783</td>\n",
       "      <td>-2.066656</td>\n",
       "      <td>-5.364473</td>\n",
       "      <td>-2.606837</td>\n",
       "      <td>-4.918215</td>\n",
       "      <td>7.305334</td>\n",
       "      <td>1.914428</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.689256</td>\n",
       "      <td>4.626942</td>\n",
       "      <td>-0.924459</td>\n",
       "      <td>1.107641</td>\n",
       "      <td>1.991691</td>\n",
       "      <td>0.510632</td>\n",
       "      <td>-0.682920</td>\n",
       "      <td>1.475829</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284803</th>\n",
       "      <td>172787.0</td>\n",
       "      <td>-0.732789</td>\n",
       "      <td>-0.055080</td>\n",
       "      <td>2.035030</td>\n",
       "      <td>-0.738589</td>\n",
       "      <td>0.868229</td>\n",
       "      <td>1.058415</td>\n",
       "      <td>0.024330</td>\n",
       "      <td>0.294869</td>\n",
       "      <td>0.584800</td>\n",
       "      <td>...</td>\n",
       "      <td>1.214756</td>\n",
       "      <td>-0.675143</td>\n",
       "      <td>1.164931</td>\n",
       "      <td>-0.711757</td>\n",
       "      <td>-0.025693</td>\n",
       "      <td>-1.221179</td>\n",
       "      <td>-1.545556</td>\n",
       "      <td>0.059616</td>\n",
       "      <td>24.79</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284804</th>\n",
       "      <td>172788.0</td>\n",
       "      <td>1.919565</td>\n",
       "      <td>-0.301254</td>\n",
       "      <td>-3.249640</td>\n",
       "      <td>-0.557828</td>\n",
       "      <td>2.630515</td>\n",
       "      <td>3.031260</td>\n",
       "      <td>-0.296827</td>\n",
       "      <td>0.708417</td>\n",
       "      <td>0.432454</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.183699</td>\n",
       "      <td>-0.510602</td>\n",
       "      <td>1.329284</td>\n",
       "      <td>0.140716</td>\n",
       "      <td>0.313502</td>\n",
       "      <td>0.395652</td>\n",
       "      <td>-0.577252</td>\n",
       "      <td>0.001396</td>\n",
       "      <td>67.88</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284805</th>\n",
       "      <td>172788.0</td>\n",
       "      <td>-0.240440</td>\n",
       "      <td>0.530483</td>\n",
       "      <td>0.702510</td>\n",
       "      <td>0.689799</td>\n",
       "      <td>-0.377961</td>\n",
       "      <td>0.623708</td>\n",
       "      <td>-0.686180</td>\n",
       "      <td>0.679145</td>\n",
       "      <td>0.392087</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.042082</td>\n",
       "      <td>0.449624</td>\n",
       "      <td>1.962563</td>\n",
       "      <td>-0.608577</td>\n",
       "      <td>0.509928</td>\n",
       "      <td>1.113981</td>\n",
       "      <td>2.897849</td>\n",
       "      <td>0.127434</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284806</th>\n",
       "      <td>172792.0</td>\n",
       "      <td>-0.533413</td>\n",
       "      <td>-0.189733</td>\n",
       "      <td>0.703337</td>\n",
       "      <td>-0.506271</td>\n",
       "      <td>-0.012546</td>\n",
       "      <td>-0.649617</td>\n",
       "      <td>1.577006</td>\n",
       "      <td>-0.414650</td>\n",
       "      <td>0.486180</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.188093</td>\n",
       "      <td>-0.084316</td>\n",
       "      <td>0.041333</td>\n",
       "      <td>-0.302620</td>\n",
       "      <td>-0.660377</td>\n",
       "      <td>0.167430</td>\n",
       "      <td>-0.256117</td>\n",
       "      <td>0.382948</td>\n",
       "      <td>217.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>284315 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Time         V1         V2        V3        V4        V5  \\\n",
       "0            0.0  -1.359807  -0.072781  2.536347  1.378155 -0.338321   \n",
       "1            0.0   1.191857   0.266151  0.166480  0.448154  0.060018   \n",
       "2            1.0  -1.358354  -1.340163  1.773209  0.379780 -0.503198   \n",
       "3            1.0  -0.966272  -0.185226  1.792993 -0.863291 -0.010309   \n",
       "4            2.0  -1.158233   0.877737  1.548718  0.403034 -0.407193   \n",
       "...          ...        ...        ...       ...       ...       ...   \n",
       "284802  172786.0 -11.881118  10.071785 -9.834783 -2.066656 -5.364473   \n",
       "284803  172787.0  -0.732789  -0.055080  2.035030 -0.738589  0.868229   \n",
       "284804  172788.0   1.919565  -0.301254 -3.249640 -0.557828  2.630515   \n",
       "284805  172788.0  -0.240440   0.530483  0.702510  0.689799 -0.377961   \n",
       "284806  172792.0  -0.533413  -0.189733  0.703337 -0.506271 -0.012546   \n",
       "\n",
       "              V6        V7        V8        V9  ...       V13       V14  \\\n",
       "0       0.462388  0.239599  0.098698  0.363787  ... -0.991390 -0.311169   \n",
       "1      -0.082361 -0.078803  0.085102 -0.255425  ...  0.489095 -0.143772   \n",
       "2       1.800499  0.791461  0.247676 -1.514654  ...  0.717293 -0.165946   \n",
       "3       1.247203  0.237609  0.377436 -1.387024  ...  0.507757 -0.287924   \n",
       "4       0.095921  0.592941 -0.270533  0.817739  ...  1.345852 -1.119670   \n",
       "...          ...       ...       ...       ...  ...       ...       ...   \n",
       "284802 -2.606837 -4.918215  7.305334  1.914428  ... -0.689256  4.626942   \n",
       "284803  1.058415  0.024330  0.294869  0.584800  ...  1.214756 -0.675143   \n",
       "284804  3.031260 -0.296827  0.708417  0.432454  ... -0.183699 -0.510602   \n",
       "284805  0.623708 -0.686180  0.679145  0.392087  ... -1.042082  0.449624   \n",
       "284806 -0.649617  1.577006 -0.414650  0.486180  ... -0.188093 -0.084316   \n",
       "\n",
       "             V15       V16       V17       V18       V19       V20  Amount  \\\n",
       "0       1.468177 -0.470401  0.207971  0.025791  0.403993  0.251412  149.62   \n",
       "1       0.635558  0.463917 -0.114805 -0.183361 -0.145783 -0.069083    2.69   \n",
       "2       2.345865 -2.890083  1.109969 -0.121359 -2.261857  0.524980  378.66   \n",
       "3      -0.631418 -1.059647 -0.684093  1.965775 -1.232622 -0.208038  123.50   \n",
       "4       0.175121 -0.451449 -0.237033 -0.038195  0.803487  0.408542   69.99   \n",
       "...          ...       ...       ...       ...       ...       ...     ...   \n",
       "284802 -0.924459  1.107641  1.991691  0.510632 -0.682920  1.475829    0.77   \n",
       "284803  1.164931 -0.711757 -0.025693 -1.221179 -1.545556  0.059616   24.79   \n",
       "284804  1.329284  0.140716  0.313502  0.395652 -0.577252  0.001396   67.88   \n",
       "284805  1.962563 -0.608577  0.509928  1.113981  2.897849  0.127434   10.00   \n",
       "284806  0.041333 -0.302620 -0.660377  0.167430 -0.256117  0.382948  217.00   \n",
       "\n",
       "        Class  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3           0  \n",
       "4           0  \n",
       "...       ...  \n",
       "284802      0  \n",
       "284803      0  \n",
       "284804      0  \n",
       "284805      0  \n",
       "284806      0  \n",
       "\n",
       "[284315 rows x 23 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtain the credit dataframe where the 'Class' field is 0\n",
    "credit_df[credit_df['Class'] == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also [concatenate](https://pandas.pydata.org/docs/reference/api/pandas.concat.html) Pandas series or dataframes together! In this example, we explored how we can concat the first 2 rows and last 2 rows of the dataframe. Similar to NumPy, you would also need to specify the axis for concatenation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V13</th>\n",
       "      <th>V14</th>\n",
       "      <th>V15</th>\n",
       "      <th>V16</th>\n",
       "      <th>V17</th>\n",
       "      <th>V18</th>\n",
       "      <th>V19</th>\n",
       "      <th>V20</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.991390</td>\n",
       "      <td>-0.311169</td>\n",
       "      <td>1.468177</td>\n",
       "      <td>-0.470401</td>\n",
       "      <td>0.207971</td>\n",
       "      <td>0.025791</td>\n",
       "      <td>0.403993</td>\n",
       "      <td>0.251412</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>0.489095</td>\n",
       "      <td>-0.143772</td>\n",
       "      <td>0.635558</td>\n",
       "      <td>0.463917</td>\n",
       "      <td>-0.114805</td>\n",
       "      <td>-0.183361</td>\n",
       "      <td>-0.145783</td>\n",
       "      <td>-0.069083</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284805</th>\n",
       "      <td>172788.0</td>\n",
       "      <td>-0.240440</td>\n",
       "      <td>0.530483</td>\n",
       "      <td>0.702510</td>\n",
       "      <td>0.689799</td>\n",
       "      <td>-0.377961</td>\n",
       "      <td>0.623708</td>\n",
       "      <td>-0.686180</td>\n",
       "      <td>0.679145</td>\n",
       "      <td>0.392087</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.042082</td>\n",
       "      <td>0.449624</td>\n",
       "      <td>1.962563</td>\n",
       "      <td>-0.608577</td>\n",
       "      <td>0.509928</td>\n",
       "      <td>1.113981</td>\n",
       "      <td>2.897849</td>\n",
       "      <td>0.127434</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284806</th>\n",
       "      <td>172792.0</td>\n",
       "      <td>-0.533413</td>\n",
       "      <td>-0.189733</td>\n",
       "      <td>0.703337</td>\n",
       "      <td>-0.506271</td>\n",
       "      <td>-0.012546</td>\n",
       "      <td>-0.649617</td>\n",
       "      <td>1.577006</td>\n",
       "      <td>-0.414650</td>\n",
       "      <td>0.486180</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.188093</td>\n",
       "      <td>-0.084316</td>\n",
       "      <td>0.041333</td>\n",
       "      <td>-0.302620</td>\n",
       "      <td>-0.660377</td>\n",
       "      <td>0.167430</td>\n",
       "      <td>-0.256117</td>\n",
       "      <td>0.382948</td>\n",
       "      <td>217.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Time        V1        V2        V3        V4        V5        V6  \\\n",
       "0            0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388   \n",
       "1            0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361   \n",
       "284805  172788.0 -0.240440  0.530483  0.702510  0.689799 -0.377961  0.623708   \n",
       "284806  172792.0 -0.533413 -0.189733  0.703337 -0.506271 -0.012546 -0.649617   \n",
       "\n",
       "              V7        V8        V9  ...       V13       V14       V15  \\\n",
       "0       0.239599  0.098698  0.363787  ... -0.991390 -0.311169  1.468177   \n",
       "1      -0.078803  0.085102 -0.255425  ...  0.489095 -0.143772  0.635558   \n",
       "284805 -0.686180  0.679145  0.392087  ... -1.042082  0.449624  1.962563   \n",
       "284806  1.577006 -0.414650  0.486180  ... -0.188093 -0.084316  0.041333   \n",
       "\n",
       "             V16       V17       V18       V19       V20  Amount  Class  \n",
       "0      -0.470401  0.207971  0.025791  0.403993  0.251412  149.62      0  \n",
       "1       0.463917 -0.114805 -0.183361 -0.145783 -0.069083    2.69      0  \n",
       "284805 -0.608577  0.509928  1.113981  2.897849  0.127434   10.00      0  \n",
       "284806 -0.302620 -0.660377  0.167430 -0.256117  0.382948  217.00      0  \n",
       "\n",
       "[4 rows x 23 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([credit_df[:2], credit_df[-2:]], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing\n",
    "\n",
    "As you might have realised, managing Pandas is not so much different from how you would operate on NumPy or basic Python structures! We have provided some teaser on basic Pandas. Now, we will see how we can use Pandas to help us in resampling.\n",
    "\n",
    "In particular, you might find the [`sample` method](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sample.html) helpful in this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.1: Problem with imbalanced data\n",
    "\n",
    "Our first task is to describe in one or two sentences what problem we might encounter if we directly use the given dataset without processing it.\n",
    "\n",
    "Hint: consider the 'Class' column, and think about how a high accuracy prediction can be achieved in a wrong way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.2: Resampling methods\n",
    "\n",
    "When we are faced with the issue of imbalanced data, there are several ways to deal with it. A more direct way might be just to collect more data instances. We realized that in our case this doesn't work well because the events unevenly occur. We then look at how to resample the existing instances.\n",
    "\n",
    "In this problem set, you are introduced with three resampling methods: undersampling, oversampling, and SMOTE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concept 1.2.1: Undersampling\n",
    "\n",
    "<figure align=\"center\">\n",
    "<img src=\"imgs/undersampling.png\" alt=\"visualisation of undersampling\" width=\"50%\">\n",
    "<figcaption style=\"text-align:center\">Figure 1: Visualisation of undersampling.</figcaption>\n",
    "</figure>\n",
    "\n",
    "The figure above illustrates undersampling. In undersampling, we remove samples from the majority class. More specifically, we randomly take subsamples of the majority class such that the size of two classes is the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1.2.1: Random undersampling in practice\n",
    "\n",
    "Your task is to observe the data in `credit_card.csv` and randomly remove some observations of the majority class until the two classes balance out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V13</th>\n",
       "      <th>V14</th>\n",
       "      <th>V15</th>\n",
       "      <th>V16</th>\n",
       "      <th>V17</th>\n",
       "      <th>V18</th>\n",
       "      <th>V19</th>\n",
       "      <th>V20</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>541</th>\n",
       "      <td>406.0</td>\n",
       "      <td>-2.312227</td>\n",
       "      <td>1.951992</td>\n",
       "      <td>-1.609851</td>\n",
       "      <td>3.997906</td>\n",
       "      <td>-0.522188</td>\n",
       "      <td>-1.426545</td>\n",
       "      <td>-2.537387</td>\n",
       "      <td>1.391657</td>\n",
       "      <td>-2.770089</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.595222</td>\n",
       "      <td>-4.289254</td>\n",
       "      <td>0.389724</td>\n",
       "      <td>-1.140747</td>\n",
       "      <td>-2.830056</td>\n",
       "      <td>-0.016822</td>\n",
       "      <td>0.416956</td>\n",
       "      <td>0.126911</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>623</th>\n",
       "      <td>472.0</td>\n",
       "      <td>-3.043541</td>\n",
       "      <td>-3.157307</td>\n",
       "      <td>1.088463</td>\n",
       "      <td>2.288644</td>\n",
       "      <td>1.359805</td>\n",
       "      <td>-1.064823</td>\n",
       "      <td>0.325574</td>\n",
       "      <td>-0.067794</td>\n",
       "      <td>-0.270953</td>\n",
       "      <td>...</td>\n",
       "      <td>0.676502</td>\n",
       "      <td>-1.692029</td>\n",
       "      <td>2.000635</td>\n",
       "      <td>0.666780</td>\n",
       "      <td>0.599717</td>\n",
       "      <td>1.725321</td>\n",
       "      <td>0.283345</td>\n",
       "      <td>2.102339</td>\n",
       "      <td>529.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4920</th>\n",
       "      <td>4462.0</td>\n",
       "      <td>-2.303350</td>\n",
       "      <td>1.759247</td>\n",
       "      <td>-0.359745</td>\n",
       "      <td>2.330243</td>\n",
       "      <td>-0.821628</td>\n",
       "      <td>-0.075788</td>\n",
       "      <td>0.562320</td>\n",
       "      <td>-0.399147</td>\n",
       "      <td>-0.238253</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022937</td>\n",
       "      <td>-1.470102</td>\n",
       "      <td>-0.698826</td>\n",
       "      <td>-2.282194</td>\n",
       "      <td>-4.781831</td>\n",
       "      <td>-2.615665</td>\n",
       "      <td>-1.334441</td>\n",
       "      <td>-0.430022</td>\n",
       "      <td>239.93</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6108</th>\n",
       "      <td>6986.0</td>\n",
       "      <td>-4.397974</td>\n",
       "      <td>1.358367</td>\n",
       "      <td>-2.592844</td>\n",
       "      <td>2.679787</td>\n",
       "      <td>-1.128131</td>\n",
       "      <td>-1.706536</td>\n",
       "      <td>-3.496197</td>\n",
       "      <td>-0.248778</td>\n",
       "      <td>-0.247768</td>\n",
       "      <td>...</td>\n",
       "      <td>0.184372</td>\n",
       "      <td>-6.771097</td>\n",
       "      <td>-0.007326</td>\n",
       "      <td>-7.358083</td>\n",
       "      <td>-12.598419</td>\n",
       "      <td>-5.131549</td>\n",
       "      <td>0.308334</td>\n",
       "      <td>-0.171608</td>\n",
       "      <td>59.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6329</th>\n",
       "      <td>7519.0</td>\n",
       "      <td>1.234235</td>\n",
       "      <td>3.019740</td>\n",
       "      <td>-4.304597</td>\n",
       "      <td>4.732795</td>\n",
       "      <td>3.624201</td>\n",
       "      <td>-1.357746</td>\n",
       "      <td>1.713445</td>\n",
       "      <td>-0.496358</td>\n",
       "      <td>-1.282858</td>\n",
       "      <td>...</td>\n",
       "      <td>1.464378</td>\n",
       "      <td>-6.079337</td>\n",
       "      <td>-0.339237</td>\n",
       "      <td>2.581851</td>\n",
       "      <td>6.739384</td>\n",
       "      <td>3.042493</td>\n",
       "      <td>-2.721853</td>\n",
       "      <td>0.009061</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264636</th>\n",
       "      <td>161516.0</td>\n",
       "      <td>-4.627235</td>\n",
       "      <td>-4.754877</td>\n",
       "      <td>-0.801657</td>\n",
       "      <td>-2.857860</td>\n",
       "      <td>0.305404</td>\n",
       "      <td>0.424863</td>\n",
       "      <td>0.836060</td>\n",
       "      <td>0.606219</td>\n",
       "      <td>1.556488</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.133634</td>\n",
       "      <td>0.619439</td>\n",
       "      <td>1.181767</td>\n",
       "      <td>-0.249518</td>\n",
       "      <td>-0.408037</td>\n",
       "      <td>0.106877</td>\n",
       "      <td>-1.139666</td>\n",
       "      <td>0.407737</td>\n",
       "      <td>635.21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3981</th>\n",
       "      <td>3630.0</td>\n",
       "      <td>-1.341764</td>\n",
       "      <td>1.599954</td>\n",
       "      <td>0.723221</td>\n",
       "      <td>-0.115087</td>\n",
       "      <td>0.504078</td>\n",
       "      <td>1.057505</td>\n",
       "      <td>-0.333620</td>\n",
       "      <td>-2.364670</td>\n",
       "      <td>1.338909</td>\n",
       "      <td>...</td>\n",
       "      <td>2.755894</td>\n",
       "      <td>1.109133</td>\n",
       "      <td>-1.903565</td>\n",
       "      <td>0.141198</td>\n",
       "      <td>0.135775</td>\n",
       "      <td>0.533011</td>\n",
       "      <td>0.877154</td>\n",
       "      <td>-0.445400</td>\n",
       "      <td>29.81</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154794</th>\n",
       "      <td>102998.0</td>\n",
       "      <td>1.908254</td>\n",
       "      <td>-0.433910</td>\n",
       "      <td>-0.444024</td>\n",
       "      <td>0.442706</td>\n",
       "      <td>-0.377820</td>\n",
       "      <td>-0.018326</td>\n",
       "      <td>-0.601195</td>\n",
       "      <td>0.080311</td>\n",
       "      <td>2.239359</td>\n",
       "      <td>...</td>\n",
       "      <td>0.679445</td>\n",
       "      <td>1.673566</td>\n",
       "      <td>-1.464332</td>\n",
       "      <td>0.299009</td>\n",
       "      <td>0.282921</td>\n",
       "      <td>0.057390</td>\n",
       "      <td>0.337145</td>\n",
       "      <td>-0.203797</td>\n",
       "      <td>40.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5679</th>\n",
       "      <td>5935.0</td>\n",
       "      <td>-0.584058</td>\n",
       "      <td>1.067422</td>\n",
       "      <td>2.262792</td>\n",
       "      <td>0.347619</td>\n",
       "      <td>0.094599</td>\n",
       "      <td>-0.792903</td>\n",
       "      <td>1.009936</td>\n",
       "      <td>-0.437815</td>\n",
       "      <td>1.348704</td>\n",
       "      <td>...</td>\n",
       "      <td>2.033919</td>\n",
       "      <td>0.873194</td>\n",
       "      <td>-1.839956</td>\n",
       "      <td>-0.350578</td>\n",
       "      <td>0.338320</td>\n",
       "      <td>-0.503403</td>\n",
       "      <td>-1.174494</td>\n",
       "      <td>-0.191274</td>\n",
       "      <td>11.39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242656</th>\n",
       "      <td>151587.0</td>\n",
       "      <td>-3.738012</td>\n",
       "      <td>-4.213630</td>\n",
       "      <td>-0.449350</td>\n",
       "      <td>-0.414387</td>\n",
       "      <td>0.429774</td>\n",
       "      <td>-1.152745</td>\n",
       "      <td>0.184216</td>\n",
       "      <td>0.403446</td>\n",
       "      <td>-1.147710</td>\n",
       "      <td>...</td>\n",
       "      <td>0.803458</td>\n",
       "      <td>0.258017</td>\n",
       "      <td>-0.090885</td>\n",
       "      <td>-0.956096</td>\n",
       "      <td>-0.317877</td>\n",
       "      <td>1.671851</td>\n",
       "      <td>-0.880219</td>\n",
       "      <td>1.609276</td>\n",
       "      <td>524.25</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>984 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Time        V1        V2        V3        V4        V5        V6  \\\n",
       "541        406.0 -2.312227  1.951992 -1.609851  3.997906 -0.522188 -1.426545   \n",
       "623        472.0 -3.043541 -3.157307  1.088463  2.288644  1.359805 -1.064823   \n",
       "4920      4462.0 -2.303350  1.759247 -0.359745  2.330243 -0.821628 -0.075788   \n",
       "6108      6986.0 -4.397974  1.358367 -2.592844  2.679787 -1.128131 -1.706536   \n",
       "6329      7519.0  1.234235  3.019740 -4.304597  4.732795  3.624201 -1.357746   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "264636  161516.0 -4.627235 -4.754877 -0.801657 -2.857860  0.305404  0.424863   \n",
       "3981      3630.0 -1.341764  1.599954  0.723221 -0.115087  0.504078  1.057505   \n",
       "154794  102998.0  1.908254 -0.433910 -0.444024  0.442706 -0.377820 -0.018326   \n",
       "5679      5935.0 -0.584058  1.067422  2.262792  0.347619  0.094599 -0.792903   \n",
       "242656  151587.0 -3.738012 -4.213630 -0.449350 -0.414387  0.429774 -1.152745   \n",
       "\n",
       "              V7        V8        V9  ...       V13       V14       V15  \\\n",
       "541    -2.537387  1.391657 -2.770089  ... -0.595222 -4.289254  0.389724   \n",
       "623     0.325574 -0.067794 -0.270953  ...  0.676502 -1.692029  2.000635   \n",
       "4920    0.562320 -0.399147 -0.238253  ...  0.022937 -1.470102 -0.698826   \n",
       "6108   -3.496197 -0.248778 -0.247768  ...  0.184372 -6.771097 -0.007326   \n",
       "6329    1.713445 -0.496358 -1.282858  ...  1.464378 -6.079337 -0.339237   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "264636  0.836060  0.606219  1.556488  ... -0.133634  0.619439  1.181767   \n",
       "3981   -0.333620 -2.364670  1.338909  ...  2.755894  1.109133 -1.903565   \n",
       "154794 -0.601195  0.080311  2.239359  ...  0.679445  1.673566 -1.464332   \n",
       "5679    1.009936 -0.437815  1.348704  ...  2.033919  0.873194 -1.839956   \n",
       "242656  0.184216  0.403446 -1.147710  ...  0.803458  0.258017 -0.090885   \n",
       "\n",
       "             V16        V17       V18       V19       V20  Amount  Class  \n",
       "541    -1.140747  -2.830056 -0.016822  0.416956  0.126911    0.00      1  \n",
       "623     0.666780   0.599717  1.725321  0.283345  2.102339  529.00      1  \n",
       "4920   -2.282194  -4.781831 -2.615665 -1.334441 -0.430022  239.93      1  \n",
       "6108   -7.358083 -12.598419 -5.131549  0.308334 -0.171608   59.00      1  \n",
       "6329    2.581851   6.739384  3.042493 -2.721853  0.009061    1.00      1  \n",
       "...          ...        ...       ...       ...       ...     ...    ...  \n",
       "264636 -0.249518  -0.408037  0.106877 -1.139666  0.407737  635.21      0  \n",
       "3981    0.141198   0.135775  0.533011  0.877154 -0.445400   29.81      0  \n",
       "154794  0.299009   0.282921  0.057390  0.337145 -0.203797   40.00      0  \n",
       "5679   -0.350578   0.338320 -0.503403 -1.174494 -0.191274   11.39      0  \n",
       "242656 -0.956096  -0.317877  1.671851 -0.880219  1.609276  524.25      0  \n",
       "\n",
       "[984 rows x 23 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def random_undersampling(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    '''\n",
    "    Given credit card dataset with 0 as the majority for 'Class' column. Returns credit card data with two classes\n",
    "    having the same shape, given raw data read from csv.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pd.DataFrame\n",
    "        The potentially imbalanced dataset.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Undersampled dataset with equal number of fraudulent and legitimate data.\n",
    "    '''\n",
    "\n",
    "    frad_df = df[df['Class'] == 1]\n",
    "    num_fraud = df['Class'].value_counts()[1]\n",
    "    legit_sample = df[df['Class'] == 0].sample(num_fraud)\n",
    "    df = pd.concat([frad_df, legit_sample], axis=0)\n",
    "    return df\n",
    "\n",
    "random_undersampling(credit_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small data\n",
    "data1 = [[111.1, 10, 0], [111.2, 20, 0], [111.3, 10, 0], [111.4, 10, 0], [111.5, 10, 0], [111.6, 10, 1], [111.4, 10, 0], [111.5, 10, 1], [111.6, 10, 1]]\n",
    "df1 = pd.DataFrame(data1, columns = ['V1', 'V2', 'Class'])\n",
    "# checks that the randomly undersampled dataset has 3 fraudulent and 3 legitimate data points\n",
    "assert np.all(np.equal(random_undersampling(df1)['Class'].value_counts(), (3, 3)))\n",
    "\n",
    "# credit card data\n",
    "assert np.all(np.equal(random_undersampling(credit_df)['Class'].value_counts(), (492, 492)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you might realize the drawback of undersampling - by removing data randomly, you might have removed some valuable information from the dataset. Is there any way to do better without losing the valuable information?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concept 1.2.2: Oversampling\n",
    "\n",
    "<figure align=\"center\">\n",
    "<img src=\"imgs/oversampling.png\" alt=\"visualisation of oversampling\" width=\"50%\">\n",
    "<figcaption style=\"text-align:center\">Figure 2: Visualisation of oversampling.</figcaption>\n",
    "</figure>\n",
    "\n",
    "The figure above illustrates oversampling. In oversampling, we duplicate records of the minority class such that the size of two classes balance out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1.2.2: Oversampling in practice\n",
    "\n",
    "Your task is to observe the data in `credit_card.csv` and apply the oversampling technique to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_oversampling(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    '''\n",
    "    Given credit card dataset with 0 as the majority for 'Class' column. Returns credit card data with two classes\n",
    "    having the same shape, given raw data read from csv.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pd.DataFrame\n",
    "        The potentially imbalanced dataset.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Oversampled dataset with equal number of fraudulent and legitimate data.\n",
    "    '''\n",
    "    legit_df = df[df['Class'] == 0]\n",
    "    num_legit = df['Class'].value_counts()[0]\n",
    "    frad_sample = df[df['Class'] == 1].sample(num_legit, replace=True)\n",
    "    df = pd.concat([legit_df, frad_sample], axis=0)\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small data\n",
    "data1 = [[111.1, 10, 0], [111.2, 20, 0], [111.3, 10, 0], [111.4, 10, 0], [111.5, 10, 0], [111.6, 10, 1],\n",
    "        [111.4, 10, 0], [111.5, 10, 1], [111.6, 10, 1]]\n",
    "df1 = pd.DataFrame(data1, columns = ['V1', 'V2', 'Class'])\n",
    "assert np.all(np.equal(random_oversampling(df1)['Class'].value_counts(), (6, 6)))\n",
    "\n",
    "# credit card data\n",
    "assert np.all(np.equal(random_oversampling(credit_df)['Class'].value_counts(), (284315, 284315)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might realize the bright side of oversampling - you don't lose certain valuable information. At the same time, you also realize the drawback of oversampling - it can cause overfitting and a poor generalization of the test set. Now the question is, instead of simply duplicating records, is there any other way to increase the number of records in the minority set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concept 1.2.3: SMOTE (for further reading only)\n",
    "\n",
    "<figure align=\"center\">\n",
    "<img src=\"imgs/smote.png\" alt=\"visualisation of SMOTE\" width=\"50%\">\n",
    "<figcaption style=\"text-align:center\">Figure 3: Visualisation of SMOTE.</figcaption>\n",
    "</figure>\n",
    "\n",
    "The figure above illustrates Synthetic Minority Oversampling Technique (SMOTE).\n",
    "\n",
    "The SMOTE algorithm works in these four steps:\n",
    "\n",
    "1. Consider minority and majority instances in vector space.\n",
    "1. For each minority-class instance pair, interpolate their feature values.\n",
    "1. Randomly synthesize instances and label with minority class\n",
    "1. More instances added to minority class\n",
    "\n",
    "In this case, you increase the number of minority instances without simply duplicating the values. Now your newly inserted minority record is not an exact copy of an existing data point, but it is also not too different from the known observations in your minority class. Outside this problem set, when we want to do data resampling, we can use the [Python imbalanced-learn library](https://pypi.org/project/imbalanced-learn/). We will not be implementing SMOTE algorithm for this problem set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word of caution when resampling\n",
    "\n",
    "In the section above, we introduced three resampling techniques. One very important thing to note is, in practice you should first split dataset to trainâ€“test sets, then resample the train dataset before training the model. This is done to avoid data leakage (snooping).\n",
    "\n",
    "Data leakage can cause you to create overly optimistic if not completely invalid predictive models. It is when information from outside the training dataset is used to train the model. The additional information might allow the model to learn something that it otherwise would not know and in turn invalidate the estimated performance of the mode being constructed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting data\n",
    "\n",
    "Before we can start training our models, we need to **randomly** partition our dataset into training data and testing data. Remember that we are trying to make a model that can predict fraudulence of data points that the model has never seen. It would be unwise to measure the accuracy of the model using the same data it trained on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.1: Splitting data\n",
    "\n",
    "In this task, you need to implement `train_test_split`. This function takes `X`, `y`, and `test_size` as arguments, and output `X_train`, `X_test`, `y_train`, and `y_test` you obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(X: np.ndarray, y: np.ndarray, test_size: float=0.25):\n",
    "    '''\n",
    "    Randomly split the data into two sets.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: np.ndarray\n",
    "        (m, n) dataset (features).\n",
    "    y: np.ndarray\n",
    "        (m,) dataset (corresponding targets).\n",
    "    test_size: np.float64\n",
    "        fraction of the dataset to be put in the test dataset.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A tuple of four elements (X_train, X_test, y_train, y_test):\n",
    "    X_train: np.ndarray\n",
    "        (m - k, n) training dataset (features).\n",
    "    X_test: np.ndarray\n",
    "        (k, n) testing dataset (features).\n",
    "    y_train: np.ndarray\n",
    "        (m - k,) training dataset (corresponding targets).\n",
    "    y_test: np.ndarray\n",
    "        (k,) testing dataset (corresponding targets).\n",
    "    '''\n",
    "\n",
    "    m = X.shape[0]\n",
    "    k = int(m * test_size)\n",
    "    # shuffle the data since we appended the fraud data to the end of the dataframe directly\n",
    "    # to avoid the case where the training data contain disproportionately more fraud data / legitimate data\n",
    "    indices = np.arange(m)\n",
    "    np.random.shuffle(indices)\n",
    "    X = X[indices]\n",
    "    y = y[indices]\n",
    "    # split the data\n",
    "    X_train = X[:-k]\n",
    "    X_test = X[-k:]\n",
    "    y_train = y[:-k]\n",
    "    y_test = y[-k:]\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = [[111.1, 10, 0], [111.2, 20, 0], [111.3, 30, 0], [111.4, 40, 0], [111.5, 50, 0], [111.6, 60, 1],\n",
    "        [111.7, 70, 0], [111.8, 80, 1], [111.9, 90, 1]]\n",
    "df1 = pd.DataFrame(data1, columns = ['V1', 'V2', 'Class'])\n",
    "X1 = df1.iloc[:, :-1].to_numpy()\n",
    "y1 = df1.iloc[:, -1].to_numpy()\n",
    "expected1 = [7, 2, 7, 2]\n",
    "\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.25)\n",
    "\n",
    "assert len(X1_train) == expected1[0] and\\\n",
    "    len(X1_test) == expected1[1] and\\\n",
    "    len(y1_train) == expected1[2] and\\\n",
    "    len(y1_test) == expected1[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent\n",
    "\n",
    "Recall the gradient descent method you learned and applied in the previous problem set. Gradient descent is an iterative optimization method which finds the minimum of a differentiable function. It's often used to find the coefficients that minimize the cost function. Here is a brief recap of how that's done:\n",
    "\n",
    "<figure align=\"center\">\n",
    "<img src=\"imgs/gradient_descent.png\" alt=\"visualisation of gradient descent\" width=\"50%\">\n",
    "    <figcaption style=\"text-align:center\"><a href=\"https://medium.com/@divakar_239/stochastic-vs-batch-gradient-descent-8820568eada1\">Figure 4: Visualisation of gradient descent.</a></figcaption>\n",
    "</figure>\n",
    "\n",
    "We start at an initial weight vector $\\mathbf{w} = (w_0, w_1, ..., w_n)$. Then we take incremental steps along the steepest slope to get to the bottom where the minimum cost lies. Along the way, we keep updating the weight coefficients by computing the gradients using the training samples from the training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concept: Batch gradient descent\n",
    "\n",
    "In batch gradient descent, **all** the training data (entire batch) is taken into consideration in a single step. For one step of gradient descent, we calculate $w_j \\leftarrow w_j - \\alpha \\frac{\\partial J(\\mathbf{w})}{\\partial w_j}$ (where $\\alpha$ is the learning rate) simultaneously for all $j$ based on all the training examples and then use that mean gradient to update our parameters.\n",
    "\n",
    "By continuing this for iterations until convergence, or until it's time to stop, we reach the optimal or near optimal parameters $\\mathbf{w}$ leading to minimum cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concept: Stochastic gradient descent\n",
    "\n",
    "Think about what's a limitation of the (entire) batch gradient descent introduced earlier. What if the dataset is very large? In Stochastic Gradient Descent (SGD), we consider only the error on a single record $(x^{(*)}, y^{(*)})$ at a time. How does this work?\n",
    "\n",
    "During the training loop, we will randomly sample an instance in the dataset. This instance will be used to update the weights of the model $w_j \\leftarrow w_j - \\alpha \\frac{\\partial J(\\mathbf{w})}{\\partial w_j}$ (where $\\alpha$ is the learning rate) simultaneously for all $j$.\n",
    "\n",
    "Note that SGD does not necessarily decrease the batch loss in each iteration! However, on average, the loss will still decrease. Since we work on only a single record at any time, SGD provides the advantage of a much smaller memory requirement, and faster computation time. SGD also comes with its own set of disadvantages, for example, we lose benefits of any vectorization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concept: Gradient descent termination condition\n",
    "\n",
    "The next question is when to stop? The ideal case is to run until convergence, but convergence might be hard to obtain. Here are some criterias you can use:\n",
    "\n",
    "* Stop when error change is small and/or\n",
    "* Stop when error is small\n",
    "* Stop when maximum number of iterations is reached"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try to apply the batch gradient descent and stochastic gradient descent.\n",
    "Task 3.1 to 3.4 will gradually guide you to complete the logistic regression using batch gradient descent, while task 3.5 expects you to implement the stochastic one. For the task in this problem set, you can assume that the bias column has been added for all input `X`. That means given `X` as an argument, you don't need to manually add the bias column again in your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.1: Cost function\n",
    "\n",
    "Recall that for logistic regression, we want an error function for an individual value to be:\n",
    "\n",
    "<figure align=\"center\">\n",
    "<img src=\"imgs/piecewise_error.png\" alt=\"error function\" width=\"50%\">\n",
    "<figcaption style=\"text-align:center\">Figure 5: Cost function.</figcaption>\n",
    "</figure>\n",
    "\n",
    "We can simplify the condition and transform it into the equivalent cost function:\n",
    "\n",
    "<figure align=\"center\">\n",
    "<img src=\"imgs/error.png\" alt=\"error function as a line\" width=\"50%\">\n",
    "<figcaption style=\"text-align:center\">Figure 6: Cost without conditions.</figcaption>\n",
    "</figure>\n",
    "\n",
    "In this task, you need to implement `cost_function` $E$ as mentioned above. This function takes `X`, `y`, and `weight_vector` $\\mathbf{w}$ as arguments, and returns the error $E$. Note that for this task, the $E$ should account for **all** the training data.\n",
    "\n",
    "Here, we are using the $\\log$ function and we need to handle the case of computing $\\log(0)$. There are many ways to handle this. In this task, we will handle $\\log(0)$ by using the machine epsilon for numpy `float64` type, and use the trick $\\log(x + eps)$ which allows $x$ to be $0$. If $x$ was any other value, $\\log(x + eps)$ would be very close to $\\log(x)$. This helps to improve numerical stability in computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def cost_function(X: np.ndarray, y: np.ndarray, weight_vector: np.ndarray):\n",
    "    '''\n",
    "    Cross entropy error for logistic regression\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: np.ndarray\n",
    "        (m, n) training dataset (features).\n",
    "    y: np.ndarray\n",
    "        (m,) training dataset (corresponding targets).\n",
    "    weight_vector: np.ndarray\n",
    "        (n,) weight parameters.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Cost\n",
    "    '''\n",
    "\n",
    "    # Machine epsilon for numpy `float64` type\n",
    "    eps = np.finfo(np.float64).eps\n",
    "    \n",
    "    y_pred = sigmoid(np.dot(X, weight_vector))\n",
    "    first_term = -y * np.log(y_pred + eps)\n",
    "    second_term = (1 - y) * np.log(1 - y_pred + eps)\n",
    "    cost = np.mean(first_term - second_term)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = [[111.1, 10, 0], [111.2, 20, 0], [111.3, 10, 0], [111.4, 10, 0], [111.5, 10, 0], [111.6, 10, 1], [111.4, 10, 0], [111.5, 10, 1], [111.6, 10, 1]]\n",
    "df1 = pd.DataFrame(data1, columns = ['V1', 'V2', 'Class'])\n",
    "X1 = df1.iloc[:, :-1].to_numpy()\n",
    "y1 = df1.iloc[:, -1].to_numpy()\n",
    "w1 = np.transpose([0.002, 0.1220])\n",
    "\n",
    "assert np.round(cost_function(X1, y1, w1), 5) == np.round(1.29333, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.2: Weight update\n",
    "\n",
    "In this task, you need to implement `weight_update`. This function takes `X`, `y`, `alpha`, and a `weight_vector` as arguments, and output the new weight vector. Each call to the function should make one update on the weight vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_update(X: np.ndarray, y: np.ndarray, alpha: np.float64, weight_vector: np.ndarray) -> np.ndarray:\n",
    "    '''\n",
    "    Do the weight update for one step in gradient descent\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: np.ndarray\n",
    "        (m, n) training dataset (features).\n",
    "    y: np.ndarray\n",
    "        (m,) training dataset (corresponding targets).\n",
    "    alpha: np.float64\n",
    "        logistic regression learning rate.1\n",
    "    weight_vector: np.ndarray\n",
    "        (n,) weight parameters.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    New weight vector after one round of update.\n",
    "    '''\n",
    "\n",
    "    y_pred = sigmoid(np.dot(X, weight_vector))\n",
    "    gradient = np.dot(X.T, (y_pred - y)) / y.size\n",
    "    weight_vector -= alpha * gradient\n",
    "    return weight_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = [[111.1, 10, 0], [111.2, 20, 0], [111.3, 10, 0], [111.4, 10, 0], [111.5, 10, 0], [111.6, 10, 1],[111.4, 10, 0], [111.5, 10, 1], [111.6, 10, 1]]\n",
    "df1 = pd.DataFrame(data1, columns = ['V1', 'V2', 'Class'])\n",
    "X1 = df1.iloc[:, :-1].to_numpy()\n",
    "y1 = df1.iloc[:, -1].to_numpy()\n",
    "w1 = np.transpose([2.2000, 12.20000])\n",
    "a1 = 1e-5\n",
    "nw1 = np.array([2.199,12.2])\n",
    "\n",
    "assert np.array_equal(np.round(weight_update(X1, y1, a1, w1), 3), nw1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.3: Logistic regression classification\n",
    "\n",
    "Remember that logistic regression is used for classification even though the function gives a probability output. In this task, you classify each element in `X`, given `weight_vector` using `prob_threshold` as the threshold, and output the classification result as an `np.ndarray`.\n",
    "\n",
    "If the probability predicted by the `weight_vector` exceeds the `prob_threshold`, we should classify it as fraud (`y = 1`). Otherwise, we should classify it as legitimate (`y = 0`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_classification(X: np.ndarray, weight_vector: np.ndarray, prob_threshold: np.float64=0.5):\n",
    "    '''\n",
    "    Do classification task using logistic regression.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: np.ndarray\n",
    "        (m, n) training dataset (features).\n",
    "    weight_vector: np.ndarray\n",
    "        (n,) weight parameters.\n",
    "    prob_threshold: np.float64\n",
    "        the threshold for a prediction to be considered fraudulent.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Classification result as an (m,) np.ndarray\n",
    "    '''\n",
    "\n",
    "    y_pred = sigmoid(np.dot(X, weight_vector))\n",
    "    y_pred[y_pred >= prob_threshold] = 1\n",
    "    y_pred[y_pred < prob_threshold] = 0\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = [[111.1, 10, 0], [111.2, 20, 0], [111.3, 10, 0], [111.4, 10, 0], [111.5, 10, 0], [211.6, 80, 1],[111.4, 10, 0], [111.5, 80, 1], [211.6, 80, 1]]\n",
    "df1 = pd.DataFrame(data1, columns = ['V1', 'V2', 'Class'])\n",
    "X1 = df1.iloc[:, :-1].to_numpy()\n",
    "y1 = df1.iloc[:, -1].to_numpy()\n",
    "w1 = np.transpose([-0.000002, 0.000003])\n",
    "expected1 = np.transpose([0, 0, 0, 0, 0, 0, 0, 1, 0])\n",
    "result1 = logistic_regression_classification(X1, w1)\n",
    "\n",
    "assert result1.shape == expected1.shape and (result1 == expected1).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.4: Logistic regression using batch gradient descent\n",
    "\n",
    "In this task, you need to implement a fixed learning rate algorithm for logistic regression. This function takes `X_train`, `y_train`, `max_num_epochs`, `threshold`, and `alpha` as arguments, and output the final `weight_vector` you obtained. You can make use of the previous functions you have implemented earlier.\n",
    "\n",
    "Specifically, an epoch refers to one complete pass through the dataset. Hence, one iteration of gradient descent, which simultaneously updates the weight vector based on the cost function computed on the entire dataset, corresponds to one epoch. Batch gradient descent is terminated when the number of update rounds exceeds the maximum number of epochs (`max_num_epochs`) given, or when the computed error falls below the threshold value (`threshold`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_batch_gradient_descent(X_train: np.ndarray, y_train: np.ndarray, max_num_epochs: int = 250, threshold: np.float64 = 0.05, alpha: np.float64 = 1e-5):\n",
    "    '''\n",
    "    Initialize your weight to zeros. Write your terminating condition, and run the weight update for some iterations.\n",
    "    Get the resulting weight vector. Use that to do predictions, and output the final weight vector.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train: np.ndarray\n",
    "        (m, n) training dataset (features).\n",
    "    y_train: np.ndarray\n",
    "        (m,) training dataset (corresponding targets).\n",
    "    max_num_epochs: int\n",
    "        this should be one of the terminating conditions. \n",
    "        That means if you initialize num_update_rounds to 0, \n",
    "        then you should stop updating the weights when num_update_rounds >= max_num_epochs.\n",
    "    threshold: np.float64\n",
    "        terminating when error <= threshold value, or if you reach the max number of update rounds first.\n",
    "    alpha: np.float64\n",
    "        logistic regression learning rate.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    The final (n,) weight parameters\n",
    "    '''\n",
    "\n",
    "    cur_num_epochs = 0\n",
    "    weight_vector = np.zeros(X_train.shape[1])\n",
    "    error = cost_function(X_train, y_train, weight_vector)\n",
    "    while cur_num_epochs < max_num_epochs and error > threshold:\n",
    "        weight_vector = weight_update(X_train, y_train, alpha, weight_vector)\n",
    "        error = cost_function(X_train, y_train, weight_vector)\n",
    "        cur_num_epochs += 1\n",
    "    return weight_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = [[111.1, 10, 0], [111.2, 20, 0], [111.3, 10, 0], [111.4, 10, 0], [111.5, 10, 0], [211.6, 80, 1],[111.4, 10, 0], [111.5, 80, 1], [211.6, 80, 1]]\n",
    "df1 = pd.DataFrame(data1, columns = ['V1', 'V2', 'Class'])\n",
    "X1 = df1.iloc[:, :-1].to_numpy()\n",
    "y1 = df1.iloc[:, -1].to_numpy()\n",
    "max_num_epochs1 = 20\n",
    "expected1 = np.transpose([-0.0011538251272737235, 0.0019538526326272542])\n",
    "\n",
    "assert np.array_equal(np.round(logistic_regression_batch_gradient_descent(X1, y1, max_num_epochs1), 5), np.round(expected1, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.5: Logistic regression using stochastic gradient descent\n",
    "\n",
    "In this task, you need to implement logistic regression using stochastic gradient descent. This function takes `X_train`, `y_train`, `max_num_iterations`, `threshold`, and `alpha` as arguments, and output the final `weight_vector` you obtained. You can make use of theÂ previous functions you have implemented earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_update_stochastic(X: np.ndarray, y: np.ndarray, alpha: np.float64, weight_vector: np.ndarray) -> np.ndarray:\n",
    "    '''\n",
    "    Do the weight update for one step in gradient descent.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: np.ndarray\n",
    "        (1, n) training dataset (features).\n",
    "    y: np.ndarray\n",
    "        one y in training dataset (corresponding targets).\n",
    "    alpha: np.float64\n",
    "        logistic regression learning rate.\n",
    "    weight_vector: np.ndarray\n",
    "        (n, 1) vector of weight parameters.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    New (n,) weight parameters after one round of update.\n",
    "    '''\n",
    "\n",
    "    y_pred = sigmoid(np.dot(X, weight_vector))\n",
    "    gradient = np.dot(X.T, (y_pred - y))\n",
    "    weight_vector -= alpha * gradient\n",
    "    return weight_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_stochastic_gradient_descent(X_train: np.ndarray, y_train: np.ndarray, max_num_iterations: int=250, threshold: np.float64=0.05, alpha: np.float64=1e-5) -> np.ndarray:\n",
    "    '''\n",
    "    Initialize your weight to zeros. Write a terminating condition, and run the weight update for some iterations.\n",
    "    Get the resulting weight vector.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train: np.ndarray\n",
    "        (m, n) training dataset (features).\n",
    "    y_train: np.ndarray\n",
    "        (m,) training dataset (corresponding targets).\n",
    "    max_num_iterations: int\n",
    "        this should be one of the terminating conditions. \n",
    "        The gradient descent step should happen at most max_num_iterations times.\n",
    "    threshold: np.float64\n",
    "        terminating when error <= threshold value, or if you reach the max number of update rounds first.\n",
    "    alpha: np.float64\n",
    "        logistic regression learning rate.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    The final (n,) weight parameters\n",
    "    '''\n",
    "    \n",
    "    cur_num_iterations = 0\n",
    "    weight_vector = np.zeros(X_train.shape[1])\n",
    "    error = cost_function(X_train, y_train, weight_vector)\n",
    "\n",
    "    while cur_num_iterations < max_num_iterations and error > threshold:\n",
    "        # shuffle to avoid bias due to the way the data is organized\n",
    "        indices = np.arange(X_train.shape[0])\n",
    "        np.random.shuffle(indices)\n",
    "        X_train = X_train[indices]\n",
    "        y_train = y_train[indices]\n",
    "\n",
    "        for i in range(X_train.shape[0]):\n",
    "            weight_vector = weight_update_stochastic(X_train[i], y_train[i], alpha, weight_vector)\n",
    "\n",
    "        error = cost_function(X_train, y_train, weight_vector)\n",
    "        cur_num_iterations += 1\n",
    "\n",
    "    return weight_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = [[111.1, 10, 0], [111.2, 20, 0], [111.3, 10, 0], [111.4, 10, 0], [111.5, 10, 0], [211.6, 80, 1],[111.4, 10, 0], [111.5, 80, 1], [211.6, 80, 1]]\n",
    "df1 = pd.DataFrame(data1, columns = ['V1', 'V2', 'Class'])\n",
    "X1 = df1.iloc[:, :-1].to_numpy()\n",
    "y1 = df1.iloc[:, -1].to_numpy()\n",
    "expected1 = cost_function(X1, y1, np.transpose(np.zeros(X1.shape[1])))\n",
    "\n",
    "assert cost_function(X1, y1, logistic_regression_stochastic_gradient_descent(X1, y1)) < expected1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.6.1: Stochastic gradient descent vs batch gradient descent\n",
    "\n",
    "Recall that it is ideal for gradient descent to run until convergence, but convergence might be hard to obtain.\n",
    "\n",
    "In this task, we will explore the relationship between the cross entropy loss vs the number of update rounds and/or the runtime it takes to run the update for both batch gradient descent (task 3.4) and stochastic gradient descent (task 3.5) using sample of `credit_card` dataset.\n",
    "\n",
    "To guide us in our explorations, let's start by collecting empirical results from running stochastic gradient descent and batch gradient descent. You will need to make the following 2 plots with the stated specifications:\n",
    "\n",
    "1. Plot of cross entropy loss against number of update rounds\n",
    "    - $x$-axis label: \"Number of Update Rounds\"\n",
    "    - $y$-axis label: \"Cross Entropy Loss\"\n",
    "    - Title: \"Plot of cross entropy loss against number of update rounds\"\n",
    "    - Legend to specify the data points corresponding to \"Batch Gradient Descent\" and \"Stochastic Gradient Descent\" respectively\n",
    "2. Plot of cross entropy loss against runtime (sec)\n",
    "    - $x$-axis label: \"Runtime (sec)\"\n",
    "    - $y$-axis label: \"Cross Entropy Loss\"\n",
    "    - Title: \"Plot of cross entropy loss against runtime (sec)\"\n",
    "    - Legend to specify the data points corresponding to \"Batch Gradient Descent\" and \"Stochastic Gradient Descent\" respectively\n",
    "\n",
    "For both plots, put both methods (batch gradient descent and stochastic gradient descent) in one diagram. Also ensure that the runtime is the cumulative runtime not the runtime per individual update round. \n",
    "\n",
    "You can refer to following script for creating these plots. Feel free to modify it if necessary.\n",
    "\n",
    "Once you are done, paste your plots in the corresponding answer box in Coursemology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h7/yhzp4v912nv1xqw0v1gx37880000gn/T/ipykernel_74285/3209328314.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z))\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "\n",
    "X_sample, y_sample = random_undersampling(credit_df).values[:, :-1], random_undersampling(credit_df).values[:, -1:].flatten()\n",
    "num_interations = 1000\n",
    "batch_times = []\n",
    "batch_costs = []\n",
    "\n",
    "for i in range(50, num_interations + 1, 50):\n",
    "    start = time()\n",
    "    weight_vector = logistic_regression_batch_gradient_descent(X_sample, y_sample, i, 0, 1e-5)\n",
    "    stop = time()\n",
    "    batch_times.append(stop - start)\n",
    "    batch_costs.append(cost_function(X_sample, y_sample, weight_vector))\n",
    "plt.plot(batch_times, batch_costs, 'g^', label=\"Batch Gradient Descent\")\n",
    "\n",
    "stochastic_times = []\n",
    "stochastic_costs = []\n",
    "for i in range(50, num_interations + 1, 50):\n",
    "    start = time()\n",
    "    weight_vector = logistic_regression_stochastic_gradient_descent(X_sample, y_sample, i, 0, 1e-5)\n",
    "    stop = time()\n",
    "    stochastic_times.append(stop - start)\n",
    "    stochastic_costs.append(cost_function(X_sample, y_sample, weight_vector))\n",
    "plt.plot(stochastic_times, stochastic_costs, 'bs', label=\"Stochastic Gradient Descent\")\n",
    "\n",
    "plt.xlabel('Runtime (sec)')\n",
    "plt.ylabel('Cross Entropy Loss')\n",
    "plt.legend()\n",
    "plt.title('Plot of cross entropy loss against runtime (sec)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.6.2: Stochastic gradient descent vs batch gradient descent\n",
    "\n",
    "With reference to the 2 plots that you have made in task 3.6.1, what do you observe about the relationship between the cross entropy loss vs the number of update rounds and/or the runtime it takes to run the update for both batch gradient descent (task 3.4) and stochastic gradient descent (task 3.5)? Explain your observations in terms of the effect of size of data, number of update rounds, runtime and whether the algorithm will be stuck in a local minima and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-class logistic regression\n",
    "\n",
    "Now that you have helped the credit card companies recognize fraudulent transactions, customers are less likely to be charged for items that they did not purchase. They are very satisfied with how well your model performed. They want to offer you a meal, at a restaurant of your choice. You decide to make your choice based on the occupancy of the restaurant. Luckily for you, you have some starting data `restaurant_data.csv` to work with. The dataset provides three input features \"max_capacity\", \"feedback_score\", and \"average_expense\", and an output feature \"occupancy\" that takes the value of \"none\", \"some\", or \"full\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.1: Multi-class logistic regression using batch gradient descent\n",
    "\n",
    "In this task, you need to implement a fixed learning rate algorithm for multi-class logistic regression. This function takes `X_train`, `y_train`, `max_num_epochs`, `threshold`, `alpha`, and `class_i` as arguments, and output the final `weight_vector` you obtained. Here, argument `class_i` is the class that you want to train in this round. You can make use of theÂ previous functions you have implemented earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_class_logistic_regression_batch_gradient_descent(X_train: np.ndarray, y_train: np.ndarray, max_num_epochs: int, threshold: np.float64, alpha: np.float64, class_i: str) -> np.ndarray:\n",
    "    '''\n",
    "    Initialize your weight to zeros. Write your terminating condition, and run the weight update for some iterations.\n",
    "    Get the resulting weight vector. Output the resulting weight vector.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train: np.ndarray\n",
    "        (m, n) training dataset (features).\n",
    "    y_train: np.ndarray\n",
    "        (m,) training dataset (corresponding targets).\n",
    "    alpha: np.float64\n",
    "        logistic regression learning rate.\n",
    "    max_num_epochs: int\n",
    "        this should be one of the terminating conditions. \n",
    "        That means if you initialize num_update_rounds to 0, \n",
    "        then you should stop updating the weights when num_update_rounds >= max_num_epochs.\n",
    "    threshold: float\n",
    "        terminating when error <= threshold value, or if you reach the max number of update rounds first.\n",
    "    class_i: string\n",
    "        one of 'none', 'full', 'some'.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    The final (n,) weight parameters\n",
    "    '''\n",
    "\n",
    "    # TODO: add your solution here and remove `raise NotImplementedError`\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = [[26, 9, 69, 'full'],\n",
    "        [54, 3, 16, 'some'],\n",
    "        [59, 7, 50, 'some' ],\n",
    "        [33, 0, 45, 'full']]\n",
    "df1 = pd.DataFrame(data1, columns = ['max_capcity', 'feedback_score', 'average_expense', 'occupancy'])\n",
    "X1 = df1.iloc[:, :-1].to_numpy()\n",
    "y1 = df1.iloc[:, -1].to_numpy()\n",
    "max_num_epochs1 = 20\n",
    "expected1 = np.transpose([6.75, 0.125, -6.0])\n",
    "assert np.array_equal(multi_class_logistic_regression_batch_gradient_descent(X1, y1, max_num_epochs1, 0.05, 1, 'some'), expected1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.2: Do classification on multi-class problem\n",
    "\n",
    "Finally, you classify each element in `X`, given three weight vectors `weight_vector_none`, `weight_vector_some`, and `weight_vector_full`, and output a list of classification results in the form of an array (`['some', 'none', 'full', 'none', ...]`). Note that in the case of a tie, you can choose the class in the order of priority: `'none'`, `'some'`, `'full'`. You can make use of theÂ previous functions you have implemented earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_class_logistic_regression_classification(X: np.ndarray, weight_vector_none: np.ndarray, weight_vector_some: np.ndarray, weight_vector_full: np.ndarray):\n",
    "    '''\n",
    "    Do classification task using logistic regression.\n",
    "    In the case of a tie, break the tie in the priority 'none' > 'some' > 'full'.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: np.ndarray\n",
    "        (m, n) training dataset (features)\n",
    "    weight_vector_none: np.ndarray\n",
    "        (n,) weight parameters for the 'none' class.\n",
    "    weight_vector_some: np.ndarray\n",
    "        (n,) weight parameters for the 'some' class.\n",
    "    weight_vector_full: np.ndarray\n",
    "        (n,) weight parameters for the 'full' class.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Classification result as an (m,) np.ndarray. Eg ['some', 'none', 'full', ... ,'none'].\n",
    "    '''\n",
    "\n",
    "    # TODO: add your solution here and remove `raise NotImplementedError`\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = [[26, 9, 69, 'full'],\n",
    "        [54, 3, 16, 'some'],\n",
    "        [59, 7, 50, 'some' ],\n",
    "        [33, 0, 45, 'full']]\n",
    "df1 = pd.DataFrame(data1, columns = ['max_capcity', 'feedback_score', 'average_expense', 'occupancy'])\n",
    "X1 = df1.iloc[:, :-1].to_numpy()\n",
    "\n",
    "w11 = np.transpose([0.0013351567670329624, 2.5757816929896605e-05, -0.001189020140476165])\n",
    "w12 = np.transpose([2.5757816929896605e-05, -0.001189020140476165, 0.0013351567670329624])\n",
    "w13 = np.transpose([2.5757816929896605e-05, 0.0013351567670329624, -0.001189020140476165])\n",
    "expected1 = np.transpose(['some', 'none', 'some', 'some'])\n",
    "\n",
    "result1 = multi_class_logistic_regression_classification(X1, w11, w12, w13)\n",
    "\n",
    "assert result1.shape == expected1.shape and (result1 == expected1).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine\n",
    "\n",
    "Now let's apply the support vector machine we learnt in lecture to the credit card dataset. Here is a quick recap of what SVM is about: We are given a training dataset of $n$ points of the form $(x^{(i)}, y^{(i)})$, where the $y^{(i)}$ are either 1 or 0, each indicating the class to which the point $x^{(i)}$ belongs.\n",
    "\n",
    "Each $x^{(i)}$ $n$-dimensional real vector. We want to find the \"maximum-margin hyperplane\" that divides the group of points $x^{(i)}$ for which $y_{i}=1$ from the group of points for which $y_{i}=0$, which is defined so that the distance between the hyperplane and the nearest point $x^{(i)}$ from either group is maximized.\n",
    "\n",
    "<figure align=\"center\">\n",
    "<img src=\"imgs/svm.png\" alt=\"visualisation of support vector machine\" width=\"50%\">\n",
    "<figcaption style=\"text-align:center\">Figure 7: Visualisation of support vector machine.</figcaption>\n",
    "</figure>\n",
    "\n",
    "How can we do construct such a \"maximum-margin hyperplane\"? Recall that in the lecture, we formulated SVMs as the following minimization problem:\n",
    "\n",
    "$$\n",
    "\\min_{w} C \\left[\\sum_{i = 1}^{m} y^{(i)}\\ Cost_1(\\mathbf{w}^{T} \\mathbf{x}^{(i)}) + (1 - y^{(i)})\\ Cost_0(\\mathbf{w}^{T} \\mathbf{x}^{(i)})\\right] + \\frac{1}{2} \\sum_{i = 1}^{n} \\mathbf{w}_i^2 $$\n",
    "\n",
    "where $Cost_1(\\mathbf{w}^T \\mathbf{x}^{(i)}) = \\max(0, 1 - \\mathbf{w}^T \\mathbf{x}^{(i)})$ and $Cost_0(\\mathbf{w}^T \\mathbf{x}^{(i)}) = \\max(0, 1 + \\mathbf{w}^T \\mathbf{x}^{(i)})$, with the hypothesis function $h_\\mathbf{w}(\\mathbf{w}) = 1$ if $\\mathbf{w}^T \\mathbf{x} \\geq 0$, and 0 otherwise.\n",
    "\n",
    "In particular, the summand term is known as the *hinge loss*. Thus, when $y^{(i)} = 0$, the more negative $\\mathbf{w}^T \\mathbf{x}$ is, the more $Cost_1$ penalises the model. Similarly, $Cost_0$ penalises the model the further $\\mathbf{w}^T \\mathbf{x}$ strays from the negative region. Notice how the cost function aligns with the goal of the hypothesis function?\n",
    "\n",
    "Furthermore, we also add a regularization parameter $C$ to the cost function to balance the trade-off between margin maximization and loss.\n",
    "\n",
    "Thus, solving this minimization problem allows us to find the weight vector $\\mathbf{w}$ that provides the maximum-margin hyperplane while balancing the trade-off of overfitting.\n",
    "\n",
    "With this set up in mind, how can we implement this in code? Thankfully, with modern machine learning libraries, much of the tedious work of solving this minimization problem has been done. As machine learning practitioners, one popular library that we commonly use (aside from [NumPy](https://numpy.org/doc/stable/index.html)) is [scikit-learn](https://scikit-learn.org/), which is built on top of NumPy and provides simple and efficient tools to implement common machine learning algorithms.\n",
    "\n",
    "In particular, we will look at how we can use the [scikit-learn](https://scikit-learn.org/) library to implement SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5.1: Linear SVM\n",
    "\n",
    "Take a look at the Python library [scikit-learn](https://scikit-learn.org/stable/modules/svm.html). It has been imported for your use. In this task, the function takes `X`, `y`. Note that here `X` and `y` are not training set but the entire dataset, so you are asked to do a train test data split with `test_size` of 0.3, and `random_state` of 42, **using what's provided in the library** (not what you implemented in task 2.1). Then, create an instance of a Linear SVM classifier using the default parameters, train the Linear SVM classifier, and output the predictions as well as the accuracy score. Note that the default kernel in sklearn is RBF, so you need to specify the kernel type you want to use.\n",
    "\n",
    "**For this task, you should make full use of the library functions, instead of doing computations on your own.**\n",
    "\n",
    "In particular, you can look into the scikit-learn methods to perform a [train-test data split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) and employ the [SVC model](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_svm(X: np.ndarray, y: np.ndarray):\n",
    "    '''\n",
    "    Do classification using linear svm. Given X and y, note that here X and y are not training sets, but rather the\n",
    "    entire dataset. Do a train test data split with test_size=0.3, and random_state=42.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: np.ndarray\n",
    "        (m, n) whole dataset (features)\n",
    "    y: np.ndarray\n",
    "        (m,) whole dataset (corresponding targets)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pred: np.ndarray\n",
    "        The predictions.\n",
    "    acc: np.float64\n",
    "        The accuracy on a scale up to 100.\n",
    "    '''\n",
    "\n",
    "    # TODO: add your solution here and remove `raise NotImplementedError`\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small data\n",
    "data1 = [[111.1, 10, 0], [111.2, 20, 0], [111.3, 10, 0], [111.4, 10, 0], [111.5, 10, 0], [211.6, 80, 1],\n",
    "        [111.4, 10, 0], [111.5, 80, 1], [211.6, 80, 1]]\n",
    "df1 = pd.DataFrame(data1, columns = ['V1', 'V2', 'Class'])\n",
    "X1 = df1.iloc[:, :-1].to_numpy()\n",
    "y1 = df1.iloc[:, -1].to_numpy()\n",
    "expected1_y = np.transpose([0, 0, 1])\n",
    "expected1_accuracy = 66.66666666666666\n",
    "result1 = linear_svm(X1, y1)\n",
    "assert (result1[0] == expected1_y).all() and (result1[0]).shape == expected1_y.shape and round(result1[1], 5) == round(expected1_accuracy, 5)\n",
    "\n",
    "# subset of credit card data\n",
    "class_0 = credit_df[credit_df['Class'] == 0]\n",
    "class_1 = credit_df[credit_df['Class'] == 1]\n",
    "\n",
    "data_0 = class_0.sample(n=15, random_state=42)\n",
    "data_1 = class_1.sample(n=50, random_state=42)\n",
    "data_100 = pd.concat([data_1, data_0], axis=0)\n",
    "X = data_100.iloc[:, :-1].to_numpy()\n",
    "y = data_100.iloc[:, -1].to_numpy()\n",
    "\n",
    "expected_pred = np.transpose([1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
    "expected_accuracy = 80.0\n",
    "result = linear_svm(X, y.ravel())\n",
    "assert (result[0] == expected_pred).all() and (result[0]).shape == expected_pred.shape and round(result[1], 5) == round(expected_accuracy, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5.2: Gaussian Kernel SVM\n",
    "\n",
    "Similarly, you will use scikit-learn to implement a Gaussian Kernel SVM. In this task, the function takes `X`, `y`. Note that here `X` and `y` are not training set but the entire dataset, so you are asked to do a train test data split with `test_size` of 0.3, and `random_state` of 42, using what's provided in the library. Then, create an instance of a Gaussian Kernel SVM classifier using the default parameters, train the Gaussian Kernel SVM classifier, and output the predictions as well as the accuracy score.\n",
    "\n",
    "**For this task, you should make full use of the library functions, instead of doing computations on your own.**\n",
    "\n",
    "As before, you can look into the scikit-learn methods to perform a [train-test data split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) and employ the [SVC model](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_kernel_svm(X: np.ndarray, y: np.ndarray):\n",
    "    '''\n",
    "    Do classification using Gaussian Kernel svm. Given X and y, note that here X and y are not training sets, but\n",
    "    rather the entire dataset. Do a train test data split with test_size=0.3, and random_state=42.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: np.ndarray\n",
    "        (m, n) whole dataset (features)\n",
    "    y: np.ndarray\n",
    "        (m,) whole dataset (corresponding targets)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pred: np.ndarray\n",
    "        The predictions.\n",
    "    acc: np.float64\n",
    "        The accuracy on a scale up to 100.\n",
    "    '''\n",
    "\n",
    "    # TODO: add your solution here and remove `raise NotImplementedError`\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small data\n",
    "data1 = [[111.1, 10, -1], [111.2, 20, -1], [111.3, 10, -1], [111.4, 10, -1], [111.5, 10, -1], [211.6, 80, 1],\n",
    "        [111.4, 10, -1], [111.5, 80, 1], [211.6, 80, 1]]\n",
    "df1 = pd.DataFrame(data1, columns = ['V1', 'V2', 'Class'])\n",
    "X1 = df1.iloc[:, :-1].to_numpy()\n",
    "y1 = df1.iloc[:, -1].to_numpy()\n",
    "expected1_y = np.transpose([-1, -1, 1])\n",
    "expected1_accuracy = 66.66666666666666\n",
    "result1 = gaussian_kernel_svm(X1, y1)\n",
    "assert (result1[0] == expected1_y).all() and (result1[0]).shape == expected1_y.shape and round(result1[1], 5) == round(expected1_accuracy, 5)\n",
    "\n",
    "\n",
    "# subset of credit card data\n",
    "class_0 = credit_df[credit_df['Class'] == 0]\n",
    "class_1 = credit_df[credit_df['Class'] == 1]\n",
    "\n",
    "data_0 = class_0.sample(n=15, random_state=42)\n",
    "data_1 = class_1.sample(n=50, random_state=42)\n",
    "data_100 = pd.concat([data_1, data_0], axis=0)\n",
    "X = data_100.iloc[:, :-1].to_numpy()\n",
    "y = data_100.iloc[:, -1].to_numpy()\n",
    "\n",
    "expected_pred = np.transpose([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
    "expected_accuracy = 80.0\n",
    "result = gaussian_kernel_svm(X, y.ravel())\n",
    "assert (result[0] == expected_pred).all() and (result[0]).shape == expected_pred.shape and round(result[1], 5) == round(expected_accuracy, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5.3: Linear SVM vs Gaussian Kernel SVM\n",
    "\n",
    "Based on your observations, when using support vector machines, how do you think we should choose between linear kernel vs Gaussian kernel? In other words, which kernel is better in what cases?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "\n",
    "Once you are done, please submit your work to Coursemology, by copying the right snippets of code into the corresponding box that says \"Your answer,\"and click \"Save.\" After you save, you can still make changes to your submission.\n",
    "\n",
    "Once you are satisfied with what you have uploaded, click \"Finalize submission.\" Note that once your submission is finalized, it is considered to be submitted for grading and cannot be changed. If you need to undo this action, you will have to email your assigned tutor for help. Please do not finalize your submission until you are sure that you want to submit your solutions for grading.\n",
    "\n",
    "*Have fun and enjoy coding.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "a665b5d41d17b532ea9890333293a1b812fa0b73c9c25c950b3cedf1bebd0438"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
