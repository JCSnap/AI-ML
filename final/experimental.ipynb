{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, datasets\n",
    "from PIL import Image\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "class Model:  \n",
    "    \"\"\"\n",
    "    This class represents an AI model.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Constructor for Model class.\n",
    "  \n",
    "        Parameters\n",
    "        ----------\n",
    "        self : object\n",
    "            The instance of the object passed by Python.\n",
    "        \"\"\"\n",
    "        # initialize neural network sequence\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 8, kernel_size=3, stride=1, padding=1),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "\n",
    "            nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 3)  \n",
    "        )\n",
    "\n",
    "        # initialize hyperparameters\n",
    "        self.learning_rate = 0.002\n",
    "        self.batch_size = 32\n",
    "        self.epochs = 25\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the model using the input data.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray of shape (n_samples, channel, height, width)\n",
    "            Training data.\n",
    "        y : ndarray of shape (n_samples,)\n",
    "            Target values.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns an instance of the trained model.\n",
    "        \"\"\"\n",
    "\n",
    "        X, y = Model.preprocess(X, y)\n",
    "        X, y = Model.balance_dataset(X, y)\n",
    "        #X, y = Model.feature_engineer(X, y)\n",
    "\n",
    "        \n",
    "\n",
    "        # Increase the weight of the minority classes more significantly\n",
    "        #class_weights = torch.tensor([total_count / (len(class_counts) * class_count) for class_count in class_counts])\n",
    "        class_weights = torch.tensor([1, 1, 1])\n",
    "\n",
    "        class_weights = class_weights / class_weights.sum()\n",
    "\n",
    "        print('Class weights:', class_weights)\n",
    "        Model.print_class_counts(y)\n",
    "\n",
    "        # print percentage of each label\n",
    "        Model.print_label_percentage(y)\n",
    "\n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "        y_tensor = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "        # Calculate weights for each sample\n",
    "        class_sample_counts = torch.tensor([(y_tensor == t).sum() for t in torch.unique(y_tensor, sorted=True)])\n",
    "        class_weights = 1. / class_sample_counts.float()\n",
    "        weights = class_weights[y_tensor.long()]\n",
    "\n",
    "        # Create a weighted sampler to handle imbalanced classes\n",
    "        sampler = WeightedRandomSampler(weights, len(weights))\n",
    "\n",
    "        # Create a dataset and data loader\n",
    "        dataset = TensorDataset(X_tensor, y_tensor)\n",
    "        dataloader = DataLoader(dataset, batch_size=self.batch_size if self.batch_size else len(dataset), sampler=sampler)\n",
    "\n",
    "        # Define loss function and optimizer for classification\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(self.cnn.parameters(), lr=self.learning_rate)\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            for inputs, targets in dataloader:\n",
    "                # Zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = self.cnn(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "                # Backward and optimize\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            print(f'Epoch {epoch+1}/{self.epochs}, Loss: {loss.item()}')\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Use the trained model to make predictions.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray of shape (n_samples, channel, height, width)\n",
    "            Input data.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        ndarray of shape (n_samples,)\n",
    "        Predicted target values per element in X.\n",
    "           \n",
    "        \"\"\"\n",
    "        X = Model.preprocess_predict(X)\n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "        dataset = TensorDataset(X_tensor)\n",
    "        dataloader = DataLoader(dataset, batch_size=len(dataset), shuffle=False)\n",
    "        predictions = []\n",
    "        for inputs in dataloader:\n",
    "            outputs = self.cnn(inputs[0])\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            predictions += predicted.tolist()\n",
    "\n",
    "        \n",
    "        return np.array(predictions)\n",
    "        \n",
    "    \n",
    "    @staticmethod\n",
    "    def preprocess(images, labels):\n",
    "        # remove images where label is nan\n",
    "        images = images[~np.isnan(labels)]\n",
    "        labels = labels[~np.isnan(labels)]\n",
    "        \n",
    "        # replace nan with 0\n",
    "        images = np.nan_to_num(images)\n",
    "\n",
    "        # cap min to 0 and max to 255\n",
    "        images = np.clip(images, 0, 255)\n",
    "\n",
    "        print('Shape:', images.shape)\n",
    "\n",
    "        # remove all blues and greens\n",
    "        images[:, 1, :, :] = 0\n",
    "        images[:, 2, :, :] = 0\n",
    "\n",
    "        # normalize the images\n",
    "        images = images / 255.0\n",
    "\n",
    "        return images, labels\n",
    "    \n",
    "    @staticmethod\n",
    "    def preprocess_predict(images):\n",
    "        # replace nan values with 0\n",
    "        images = np.nan_to_num(images)\n",
    "\n",
    "        # cap min to 0 and max to 255\n",
    "        images = np.clip(images, 0, 255)\n",
    "        \n",
    "        # remove all blues and greens\n",
    "        images[:, 1, :, :] = 0\n",
    "        images[:, 2, :, :] = 0\n",
    "        \n",
    "        # normalize the images\n",
    "        images = images / 255.0\n",
    "\n",
    "        return images\n",
    "\n",
    "    @staticmethod\n",
    "    def feature_engineer(images, labels):\n",
    "        T = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomRotation(10),\n",
    "        ])\n",
    "\n",
    "        # crete 10 different transformations\n",
    "        T_list = [\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomRotation(20),\n",
    "            transforms.RandomVerticalFlip(),\n",
    "            transforms.RandomRotation(40),\n",
    "            transforms.RandomInvert(),\n",
    "            transforms.ColorJitter(brightness=0.5),\n",
    "            transforms.ColorJitter(contrast=0.5),\n",
    "            transforms.ColorJitter(saturation=0.5),\n",
    "            transforms.ColorJitter(hue=0.5),\n",
    "            transforms.RandomGrayscale(p=0.1),\n",
    "        ]\n",
    "\n",
    "        print(\"Shape before:\", images.shape)\n",
    "\n",
    "        augmented_images = []\n",
    "        augmented_labels = []\n",
    "\n",
    "        # get images and labels where label is 1 or 2\n",
    "        images_to_engineer = images[labels != 0]\n",
    "        labels_to_engineer = labels[labels != 0]\n",
    "\n",
    "        print(\"Number of images to engineer:\", len(images_to_engineer))\n",
    "\n",
    "        for (image, label) in zip(images_to_engineer, labels_to_engineer):\n",
    "            image = image.transpose(1, 2, 0)  # Convert to HWC format for PIL\n",
    "            for transform in T_list:\n",
    "                img_pil = Image.fromarray(image.astype('uint8'), 'RGB')\n",
    "\n",
    "                # Apply transformation\n",
    "                augmented_img = transform(img_pil)\n",
    "\n",
    "                # Convert back to CHW format and append\n",
    "                augmented_np = np.asarray(augmented_img).transpose(2, 0, 1)\n",
    "                augmented_images.append(augmented_np)\n",
    "                augmented_labels.append(label)\n",
    "\n",
    "        images = np.concatenate((images, augmented_images), axis=0)\n",
    "        labels = np.concatenate((labels, augmented_labels), axis=0)\n",
    "\n",
    "        print('Shape:', images.shape)\n",
    "\n",
    "        return images, labels\n",
    "    \n",
    "    @staticmethod\n",
    "    def balance_dataset(images, labels, min_proportions=[0.1, 0.9]):\n",
    "        unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "        total_samples = len(labels)\n",
    "        \n",
    "        # Determine minimum count for each label based on proportions\n",
    "        min_counts = [int(total_samples * p) for p in min_proportions]\n",
    "        \n",
    "        # Sort labels by their count (ascending)\n",
    "        sorted_indices = np.argsort(counts)\n",
    "        \n",
    "        for idx, min_count in zip(sorted_indices, min_counts):\n",
    "            label = unique_labels[idx]\n",
    "            current_count = counts[idx]\n",
    "            \n",
    "            if current_count < min_count:\n",
    "                # Calculate the number of samples to add\n",
    "                add_count = min_count - current_count\n",
    "                \n",
    "                # Get indices of the current label\n",
    "                label_indices = np.where(labels == label)[0]\n",
    "                \n",
    "                # Randomly select indices to duplicate\n",
    "                add_indices = np.random.choice(label_indices, add_count)\n",
    "                \n",
    "                # Add the images and labels\n",
    "                images = np.concatenate((images, images[add_indices]))\n",
    "                labels = np.concatenate((labels, labels[add_indices]))\n",
    "\n",
    "        return images, labels\n",
    "\n",
    "    @staticmethod\n",
    "    def print_label_percentage(y):\n",
    "        total_count = len(y)\n",
    "        unique_labels, counts = np.unique(y, return_counts=True)\n",
    "        for label, count in zip(unique_labels, counts):\n",
    "            print(f'Label {label}: {count / total_count * 100:.2f}%')\n",
    "\n",
    "    @staticmethod\n",
    "    def print_class_counts(y):\n",
    "        unique_labels, counts = np.unique(y, return_counts=True)\n",
    "        print('Class counts:', dict(zip(unique_labels, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "with open('data.npy', 'rb') as f:\n",
    "    data = np.load(f, allow_pickle=True).item()\n",
    "    X = data['image']\n",
    "    y = data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=1)\n",
    "\n",
    "# Filter test data that contains no labels\n",
    "# In Coursemology, the test data is guaranteed to have labels\n",
    "nan_indices = np.argwhere(np.isnan(y_test)).squeeze()\n",
    "mask = np.ones(y_test.shape, bool)\n",
    "mask[nan_indices] = False\n",
    "X_test = X_test[mask]\n",
    "y_test = y_test[mask]\n",
    "\n",
    "# Train and predict\n",
    "model = Model()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate model predition\n",
    "# Learn more: https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics\n",
    "print(\"F1 Score (macro): {0:.2f}\".format(f1_score(y_test, y_pred, average='macro'))) # You may encounter errors, you are expected to figure out what's the issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print first 20 predictions beside ground truth\n",
    "#for i in range(20):\n",
    "#    print(f'Prediction: {y_pred[i]}, Ground Truth: {y_test[i]}')\n",
    "\n",
    "print(f'Total predictions: {len(y_pred)}')\n",
    "print(f'Total ground truth: {len(y_test)}')\n",
    "\n",
    "# get the indices where true label is 1\n",
    "label_1_indices_true = np.where(y_test == 1)[0]\n",
    "# get the indices where predicted label is 1\n",
    "label_1_indices_pred = np.where(y_pred == 1)[0]\n",
    "\n",
    "# get the indices where true label is 2\n",
    "label_2_indices_true = np.where(y_test == 2)[0]\n",
    "# get the indices where predicted label is 2\n",
    "label_2_indices_pred = np.where(y_pred == 2)[0]\n",
    "\n",
    "# get the indices where true label is 0\n",
    "label_0_indices_true = np.where(y_test == 0)[0]\n",
    "# get the indices where predicted label is 0\n",
    "label_0_indices_pred = np.where(y_pred == 0)[0]\n",
    "\n",
    "# print number of true and predicted for each label\n",
    "for label in range(3):\n",
    "    label_indices_true = np.where(y_test == label)[0]\n",
    "    label_indices_pred = np.where(y_pred == label)[0]\n",
    "    print(f'Label {label}: {len(label_indices_true)} true, {len(label_indices_pred)} predicted')\n",
    "\n",
    "print(\"=====================================\")\n",
    "\n",
    "# out of the images with label 0, how many did we predict correctly\n",
    "correct_count = 0\n",
    "for label_0_index in label_0_indices_true:\n",
    "    if label_0_index in label_0_indices_pred:\n",
    "        correct_count += 1\n",
    "print(f'Label 0: {correct_count} correct out of {len(label_0_indices_true)}')\n",
    "\n",
    "# out of the images with label 1, how many did we predict correctly\n",
    "correct_count = 0\n",
    "for label_1_index in label_1_indices_true:\n",
    "    if label_1_index in label_1_indices_pred:\n",
    "        correct_count += 1\n",
    "print(f'Label 1: {correct_count} correct out of {len(label_1_indices_true)}')\n",
    "\n",
    "# out of the images with label 2, how many did we predict correctly\n",
    "correct_count = 0\n",
    "for label_2_index in label_2_indices_true:\n",
    "    if label_2_index in label_2_indices_pred:\n",
    "        correct_count += 1\n",
    "print(f'Label 2: {correct_count} correct out of {len(label_2_indices_true)}')\n",
    "\n",
    "# Convert y_test and y_pred to binary format (1 for label '1' and 0 for all other labels)\n",
    "y_test_binary = (y_test == 1).astype(int)\n",
    "y_pred_binary = (y_pred == 1).astype(int)\n",
    "\n",
    "# Calculate Precision, Recall, and F1 Score for label '1' (binary classification)\n",
    "precision = precision_score(y_test_binary, y_pred_binary, pos_label=1)\n",
    "recall = recall_score(y_test_binary, y_pred_binary, pos_label=1)\n",
    "f1 = f1_score(y_test_binary, y_pred_binary, pos_label=1)\n",
    "\n",
    "print(f\"Precision for label 1: {precision:.2f}\")\n",
    "print(f\"Recall for label 1: {recall:.2f}\")\n",
    "print(f\"F1 Score for label 1: {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_states = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "f1_scores = []\n",
    "for r in random_states:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=r)\n",
    "    nan_indices = np.argwhere(np.isnan(y_test)).squeeze()\n",
    "    mask = np.ones(y_test.shape, bool)\n",
    "    mask[nan_indices] = False\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "    model = Model()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    f1_scores.append(f1_score(y_test, y_pred, average='macro'))\n",
    "\n",
    "print(f'Average F1 Score: {np.mean(f1_scores):.2f}')\n",
    "print(f'Nax F1 Score: {np.max(f1_scores):.2f}')\n",
    "print(f'Min F1 Score: {np.min(f1_scores):.2f}')\n",
    "\n",
    "# 0.68 ave, 0.86 max, 0.56 min"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "environment_CS2109S",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
