{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d017333",
   "metadata": {},
   "source": [
    "# Final Assessment Scratch Pad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d00386",
   "metadata": {},
   "source": [
    "## Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea516aa7",
   "metadata": {},
   "source": [
    "1. Please use only this Jupyter notebook to work on your model, and **do not use any extra files**. If you need to define helper classes or functions, feel free to do so in this notebook.\n",
    "2. This template is intended to be general, but it may not cover every use case. The sections are given so that it will be easier for us to grade your submission. If your specific use case isn't addressed, **you may add new Markdown or code blocks to this notebook**. However, please **don't delete any existing blocks**.\n",
    "3. If you don't think a particular section of this template is necessary for your work, **you may skip it**. Be sure to explain clearly why you decided to do so."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022cb4cd",
   "metadata": {},
   "source": [
    "## Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c14a2d8",
   "metadata": {},
   "source": [
    "**[TODO]**\n",
    "\n",
    "Please provide a summary of the ideas and steps that led you to your final model. Someone reading this summary should understand why you chose to approach the problem in a particular way and able to replicate your final model at a high level. Please ensure that your summary is detailed enough to provide an overview of your thought process and approach but also concise enough to be easily understandable. Also, please follow the guidelines given in the `main.ipynb`.\n",
    "\n",
    "This report should not be longer than **1-2 pages of A4 paper (up to around 1,000 words)**. Marks will be deducted if you do not follow instructions and you include too many words here. \n",
    "\n",
    "**[DELETE EVERYTHING FROM THE PREVIOUS TODO TO HERE BEFORE SUBMISSION]**\n",
    "\n",
    "##### Overview\n",
    "**[TODO]**\n",
    "\n",
    "##### 1. Descriptive Analysis\n",
    "The first step is to get an intuition of what is the kind of images I am dealing with, which is why I plot the first 10 images. However, there are some errors, which is why I did some transformation and changing type to uint8 to be able to plot.\n",
    "\n",
    "The next step is to understand what is the nature of the labels, namely:\n",
    "1. How many nans are there? This will affect how we choose to process the data later on.\n",
    "2. What is the make up of each labels as a percentage of the whole dataset? This will affect whether or not under/over sampling is used.\n",
    "\n",
    "##### 2. Detection and Handling of Missing Values\n",
    "**[TODO]**\n",
    "\n",
    "##### 3. Detection and Handling of Outliers\n",
    "**[TODO]**\n",
    "\n",
    "##### 4. Detection and Handling of Class Imbalance \n",
    "**[TODO]**\n",
    "\n",
    "##### 5. Understanding Relationship Between Variables\n",
    "**[TODO]**\n",
    "\n",
    "##### 6. Data Visualization\n",
    "**[TODO]** \n",
    "##### 7. General Preprocessing\n",
    "**[TODO]**\n",
    " \n",
    "##### 8. Feature Selection \n",
    "**[TODO]**\n",
    "\n",
    "##### 9. Feature Engineering\n",
    "**[TODO]**\n",
    "\n",
    "##### 10. Creating Models\n",
    "**[TODO]**\n",
    "\n",
    "##### 11. Model Evaluation\n",
    "**[TODO]**\n",
    "\n",
    "##### 12. Hyperparameters Search\n",
    "**[TODO]**\n",
    "\n",
    "##### Conclusion\n",
    "**[TODO]**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49dcaf29",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27103374",
   "metadata": {},
   "source": [
    "# Workings (Not Graded)\n",
    "\n",
    "You will do your working below. Note that anything below this section will not be graded, but we might counter-check what you wrote in the report above with your workings to make sure that you actually did what you claimed to have done. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4c6cd4",
   "metadata": {},
   "source": [
    "## Import Packages\n",
    "\n",
    "Here, we import some packages necessary to run this notebook. In addition, you may import other packages as well. Do note that when submitting your model, you may only use packages that are available in Coursemology (see `main.ipynb`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cded1ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748c35d7",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "\n",
    "The dataset `data/images.npy` is of size $(N, C, H, W)$, where $N$, $C$, $H$, and $W$ correspond to the number of data, image channels, image width, and image height, respectively.\n",
    "\n",
    "A code snippet that loads the data is provided below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09da291",
   "metadata": {},
   "source": [
    "### Load Image Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6297e25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data.npy', 'rb') as f:\n",
    "    data = np.load(f, allow_pickle=True).item()\n",
    "    images = data['image']\n",
    "    labels = data['label']\n",
    "    \n",
    "print('Shape:', images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229af1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with subplots\n",
    "plt.figure(figsize=(15, 15))  # Adjust the size as needed\n",
    "\n",
    "# Loop through the first 10 images\n",
    "for i in range(10):\n",
    "    # Access the image\n",
    "    image = images[i]\n",
    "\n",
    "    # Convert to uint8\n",
    "    image = np.array(image, dtype='uint8')\n",
    "\n",
    "    # Rearrange the axes from [channels, height, width] to [height, width, channels]\n",
    "    image = np.transpose(image, (1, 2, 0))\n",
    "\n",
    "    # Plot the image\n",
    "    plt.subplot(2, 5, i + 1)  # Adjust the layout (rows, columns, index) as needed\n",
    "    plt.imshow(image)\n",
    "    plt.title(f\"Label: {labels[i]}\")\n",
    "    plt.axis('off')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb651acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# count the number of nans in labels\n",
    "nan_count = 0\n",
    "for label in labels:\n",
    "    if np.isnan(label):\n",
    "        nan_count += 1\n",
    "\n",
    "total_count = len(labels)\n",
    "\n",
    "print('NaN count:', nan_count)\n",
    "# print nan count percentage\n",
    "print('NaN count percentage:', nan_count / len(labels) * 100, \"%\")\n",
    "\n",
    "# remove nans and plot the label count\n",
    "labels = labels[~np.isnan(labels)]\n",
    "label_count = {}\n",
    "for label in labels:\n",
    "    if label not in label_count:\n",
    "        label_count[label] = 0\n",
    "    label_count[label] += 1\n",
    "\n",
    "bars = plt.bar(label_count.keys(), label_count.values())\n",
    "\n",
    "plt.title('Label Count')\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Add the percentage to each bar\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    percentage = f'{100 * height / total_count:.2f}%'\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, height, percentage, ha='center', va='bottom')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe832b6",
   "metadata": {},
   "source": [
    "## Data Exploration & Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6a464c",
   "metadata": {},
   "source": [
    "### 1. Descriptive Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb61967",
   "metadata": {},
   "source": [
    "### 2. Detection and Handling of Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb9cdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('NaN count percentage:', nan_count / len(images) * 100, \"%\") # arund 100%\n",
    "\n",
    "# remove images where label is nan\n",
    "images = images[~np.isnan(labels)]\n",
    "labels = labels[~np.isnan(labels)]\n",
    "\n",
    "print('Shape:', images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880d7884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the max nan values in an image\n",
    "max_nan_count = 0\n",
    "for image in images:\n",
    "    nan_count = np.isnan(image).sum()\n",
    "    if nan_count > max_nan_count:\n",
    "        max_nan_count = nan_count\n",
    "\n",
    "print('Max NaN count:', max_nan_count)\n",
    "# replace nan values with 0\n",
    "images = np.nan_to_num(images)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adcb9cd",
   "metadata": {},
   "source": [
    "### 3. Detection and Handling of Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1c17a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d4916043",
   "metadata": {},
   "source": [
    "### 4. Detection and Handling of Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4cddb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3ab20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_1_indices = np.where(labels == 1)[0]\n",
    "label_1_count = len(label_1_indices)\n",
    "add_count = 300 - label_1_count\n",
    "add_indices = np.random.choice(label_1_indices, add_count)\n",
    "images = np.concatenate((images, images[add_indices]))\n",
    "labels = np.concatenate((labels, labels[add_indices]))\n",
    "# oversample the data with label 2 to 300\n",
    "# get the indices of label 2\n",
    "label_2_indices = np.where(labels == 2)[0]\n",
    "# get the number of images with label 2\n",
    "label_2_count = len(label_2_indices)\n",
    "# get the number of images to add\n",
    "add_count = 300 - label_2_count\n",
    "# get the indices to add\n",
    "add_indices = np.random.choice(label_2_indices, add_count)\n",
    "# add the images and labels\n",
    "images = np.concatenate((images, images[add_indices]))\n",
    "labels = np.concatenate((labels, labels[add_indices]))\n",
    "print('New shape:', images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2552a795",
   "metadata": {},
   "source": [
    "### 5. Understanding Relationship Between Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ddbbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each label 0, 1, 2, print out 5 images with that label, side by side\n",
    "for label in range(3):\n",
    "    label_indices = np.where(labels == label)[0]\n",
    "    for i in range(10):\n",
    "        image = images[label_indices[i]]\n",
    "        image = np.array(image, dtype='uint8')\n",
    "        image = np.transpose(image, (1, 2, 0))\n",
    "        plt.subplot(2, 5, i + 1)\n",
    "        plt.imshow(image)\n",
    "        plt.title(f\"Label: {label}\")\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757fb315",
   "metadata": {},
   "source": [
    "### 6. Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f82e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i want to analyze the difference between the images with label 0 and 1\n",
    "# get the indices of label 0 and 1\n",
    "label_0_indices = np.where(labels == 0)[0]\n",
    "label_1_indices = np.where(labels == 1)[0]\n",
    "# get the images with label 0 and 1\n",
    "label_0_images = images[label_0_indices]\n",
    "label_1_images = images[label_1_indices]\n",
    "# get the mean of each image\n",
    "label_0_means = np.mean(label_0_images, axis=(1, 2))\n",
    "label_1_means = np.mean(label_1_images, axis=(1, 2))\n",
    "# get the max of each image\n",
    "label_0_maxes = np.max(label_0_images, axis=(1, 2))\n",
    "label_1_maxes = np.max(label_1_images, axis=(1, 2))\n",
    "\n",
    "# plot the means\n",
    "plt.scatter(label_0_means, label_0_maxes, label='Label 0')\n",
    "plt.scatter(label_1_means, label_1_maxes, label='Label 1')\n",
    "plt.title('Mean vs Max')\n",
    "plt.xlabel('Mean')\n",
    "plt.ylabel('Max')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a1c68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shape before:', images.shape)\n",
    "\n",
    "# replace all B with 0, G with 0\n",
    "images[:, 1, :, :] = 0\n",
    "images[:, 2, :, :] = 0\n",
    "\n",
    "# plot the first 10 images\n",
    "for i in range(10):\n",
    "    image = images[i]\n",
    "    image = np.array(image, dtype='uint8')\n",
    "    image = np.transpose(image, (1, 2, 0))\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    plt.imshow(image)\n",
    "    plt.title(f\"Label: {labels[i]}\")\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7eebcf",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3e3383",
   "metadata": {},
   "source": [
    "### 7. General Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19174365",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb3aa527",
   "metadata": {},
   "source": [
    "### 8. Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85808bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4921e8ca",
   "metadata": {},
   "source": [
    "### 9. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcde626",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fa676c3f",
   "metadata": {},
   "source": [
    "## Modeling & Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589b37e4",
   "metadata": {},
   "source": [
    "### 10. Creating models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8dffd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, datasets\n",
    "from PIL import Image\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "class Model:  \n",
    "    \"\"\"\n",
    "    This class represents an AI model.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Constructor for Model class.\n",
    "  \n",
    "        Parameters\n",
    "        ----------\n",
    "        self : object\n",
    "            The instance of the object passed by Python.\n",
    "        \"\"\"\n",
    "        # initialize neural network sequence\n",
    "        self.cnn = nn.Sequential(\n",
    "            # convolutional layer and other layers suitable for a 16x16 image\n",
    "            nn.Conv2d(3, 8, kernel_size=3, stride=1, padding=1),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.ReLU(),\n",
    "            # dropout layer\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 3)  \n",
    "        )\n",
    "\n",
    "        # initialize hyperparameters\n",
    "        self.learning_rate = 0.001\n",
    "        self.batch_size = 32\n",
    "        self.epochs = 30\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the model using the input data.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray of shape (n_samples, channel, height, width)\n",
    "            Training data.\n",
    "        y : ndarray of shape (n_samples,)\n",
    "            Target values.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns an instance of the trained model.\n",
    "        \"\"\"\n",
    "\n",
    "        X, y = Model.preprocess(X, y)\n",
    "        #X, y = Model.balance_dataset(X, y)\n",
    "        #X, y = Model.feature_engineer(X, y)\n",
    "\n",
    "        \n",
    "\n",
    "        # Increase the weight of the minority classes more significantly\n",
    "        #class_weights = torch.tensor([total_count / (len(class_counts) * class_count) for class_count in class_counts])\n",
    "        class_weights = torch.tensor([1, 1, 1])\n",
    "\n",
    "        class_weights = class_weights / class_weights.sum()\n",
    "\n",
    "        print('Class weights:', class_weights)\n",
    "        Model.print_class_counts(y)\n",
    "\n",
    "        # print percentage of each label\n",
    "        Model.print_label_percentage(y)\n",
    "\n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "        y_tensor = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "        # Calculate weights for each sample\n",
    "        class_sample_counts = torch.tensor([(y_tensor == t).sum() for t in torch.unique(y_tensor, sorted=True)])\n",
    "        class_weights = 1. / class_sample_counts.float()\n",
    "        weights = class_weights[y_tensor.long()]\n",
    "\n",
    "        # Create a weighted sampler to handle imbalanced classes\n",
    "        sampler = WeightedRandomSampler(weights, len(weights))\n",
    "\n",
    "\n",
    "        # Create a dataset and data loader\n",
    "        dataset = TensorDataset(X_tensor, y_tensor)\n",
    "        dataloader = DataLoader(dataset, batch_size=self.batch_size if self.batch_size else len(dataset), sampler=sampler)\n",
    "\n",
    "        # Define loss function and optimizer for classification\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(self.cnn.parameters(), lr=self.learning_rate)\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            for inputs, targets in dataloader:\n",
    "                # Zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = self.cnn(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "                # Backward and optimize\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            print(f'Epoch {epoch+1}/{self.epochs}, Loss: {loss.item()}')\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Use the trained model to make predictions.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray of shape (n_samples, channel, height, width)\n",
    "            Input data.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        ndarray of shape (n_samples,)\n",
    "        Predicted target values per element in X.\n",
    "           \n",
    "        \"\"\"\n",
    "        X = Model.preprocess_predict(X)\n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "        dataset = TensorDataset(X_tensor)\n",
    "        dataloader = DataLoader(dataset, batch_size=len(dataset), shuffle=False)\n",
    "        predictions = []\n",
    "        for inputs in dataloader:\n",
    "            outputs = self.cnn(inputs[0])\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            predictions += predicted.tolist()\n",
    "\n",
    "        \n",
    "        return np.array(predictions)\n",
    "        \n",
    "    \n",
    "    @staticmethod\n",
    "    def preprocess(images, labels):\n",
    "        # remove images where label is nan\n",
    "        images = images[~np.isnan(labels)]\n",
    "        labels = labels[~np.isnan(labels)]\n",
    "        \n",
    "        # replace nan values with 0\n",
    "        images = np.nan_to_num(images)\n",
    "\n",
    "        print('Shape:', images.shape)\n",
    "\n",
    "    \n",
    "\n",
    "        # normalize the images\n",
    "        images = images / 255.0\n",
    "\n",
    "        return images, labels\n",
    "    \n",
    "    @staticmethod\n",
    "    def preprocess_predict(images):\n",
    "        # replace nan values with 0\n",
    "        images = np.nan_to_num(images)\n",
    "      \n",
    "        \n",
    "        # normalize the images\n",
    "        images = images / 255.0\n",
    "\n",
    "        return images\n",
    "\n",
    "    @staticmethod\n",
    "    def feature_engineer(images, labels):\n",
    "        T = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomRotation(10),\n",
    "        ])\n",
    "\n",
    "        # crete 10 different transformations\n",
    "        T_list = [\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomRotation(20),\n",
    "            transforms.RandomVerticalFlip(),\n",
    "            transforms.RandomRotation(40),\n",
    "            transforms.RandomInvert(),\n",
    "            transforms.ColorJitter(brightness=0.5),\n",
    "            transforms.ColorJitter(contrast=0.5),\n",
    "            transforms.ColorJitter(saturation=0.5),\n",
    "            transforms.ColorJitter(hue=0.5),\n",
    "            transforms.RandomGrayscale(p=0.1),\n",
    "        ]\n",
    "\n",
    "        print(\"Shape before:\", images.shape)\n",
    "\n",
    "        augmented_images = []\n",
    "        augmented_labels = []\n",
    "\n",
    "        # get images and labels where label is 1 or 2\n",
    "        images_to_engineer = images[labels != 0]\n",
    "        labels_to_engineer = labels[labels != 0]\n",
    "\n",
    "        print(\"Number of images to engineer:\", len(images_to_engineer))\n",
    "\n",
    "        for (image, label) in zip(images_to_engineer, labels_to_engineer):\n",
    "            image = image.transpose(1, 2, 0)  # Convert to HWC format for PIL\n",
    "            for transform in T_list:\n",
    "                img_pil = Image.fromarray(image.astype('uint8'), 'RGB')\n",
    "\n",
    "                # Apply transformation\n",
    "                augmented_img = transform(img_pil)\n",
    "\n",
    "                # Convert back to CHW format and append\n",
    "                augmented_np = np.asarray(augmented_img).transpose(2, 0, 1)\n",
    "                augmented_images.append(augmented_np)\n",
    "                augmented_labels.append(label)\n",
    "\n",
    "        images = np.concatenate((images, augmented_images), axis=0)\n",
    "        labels = np.concatenate((labels, augmented_labels), axis=0)\n",
    "\n",
    "        print('Shape:', images.shape)\n",
    "\n",
    "        return images, labels\n",
    "    \n",
    "    @staticmethod\n",
    "    def balance_dataset(images, labels, min_proportions=[0.1, 0.9]):\n",
    "        unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "        total_samples = len(labels)\n",
    "        \n",
    "        # Determine minimum count for each label based on proportions\n",
    "        min_counts = [int(total_samples * p) for p in min_proportions]\n",
    "        \n",
    "        # Sort labels by their count (ascending)\n",
    "        sorted_indices = np.argsort(counts)\n",
    "        \n",
    "        for idx, min_count in zip(sorted_indices, min_counts):\n",
    "            label = unique_labels[idx]\n",
    "            current_count = counts[idx]\n",
    "            \n",
    "            if current_count < min_count:\n",
    "                # Calculate the number of samples to add\n",
    "                add_count = min_count - current_count\n",
    "                \n",
    "                # Get indices of the current label\n",
    "                label_indices = np.where(labels == label)[0]\n",
    "                \n",
    "                # Randomly select indices to duplicate\n",
    "                add_indices = np.random.choice(label_indices, add_count)\n",
    "                \n",
    "                # Add the images and labels\n",
    "                images = np.concatenate((images, images[add_indices]))\n",
    "                labels = np.concatenate((labels, labels[add_indices]))\n",
    "\n",
    "        return images, labels\n",
    "\n",
    "    @staticmethod\n",
    "    def print_label_percentage(y):\n",
    "        total_count = len(y)\n",
    "        unique_labels, counts = np.unique(y, return_counts=True)\n",
    "        for label, count in zip(unique_labels, counts):\n",
    "            print(f'Label {label}: {count / total_count * 100:.2f}%')\n",
    "\n",
    "    @staticmethod\n",
    "    def print_class_counts(y):\n",
    "        unique_labels, counts = np.unique(y, return_counts=True)\n",
    "        print('Class counts:', dict(zip(unique_labels, counts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495bf3c0",
   "metadata": {},
   "source": [
    "### 11. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688a22d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "with open('data.npy', 'rb') as f:\n",
    "    data = np.load(f, allow_pickle=True).item()\n",
    "    X = data['image']\n",
    "    y = data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9522df30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "\n",
    "# Filter test data that contains no labels\n",
    "# In Coursemology, the test data is guaranteed to have labels\n",
    "nan_indices = np.argwhere(np.isnan(y_test)).squeeze()\n",
    "mask = np.ones(y_test.shape, bool)\n",
    "mask[nan_indices] = False\n",
    "X_test = X_test[mask]\n",
    "y_test = y_test[mask]\n",
    "\n",
    "# Train and predict\n",
    "model = Model()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate model predition\n",
    "# Learn more: https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics\n",
    "print(\"F1 Score (macro): {0:.2f}\".format(f1_score(y_test, y_pred, average='macro'))) # You may encounter errors, you are expected to figure out what's the issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ddb3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print first 20 predictions beside ground truth\n",
    "#for i in range(20):\n",
    "#    print(f'Prediction: {y_pred[i]}, Ground Truth: {y_test[i]}')\n",
    "\n",
    "print(f'Total predictions: {len(y_pred)}')\n",
    "print(f'Total ground truth: {len(y_test)}')\n",
    "\n",
    "# get the indices where true label is 1\n",
    "label_1_indices_true = np.where(y_test == 1)[0]\n",
    "# get the indices where predicted label is 1\n",
    "label_1_indices_pred = np.where(y_pred == 1)[0]\n",
    "\n",
    "# get the indices where true label is 2\n",
    "label_2_indices_true = np.where(y_test == 2)[0]\n",
    "# get the indices where predicted label is 2\n",
    "label_2_indices_pred = np.where(y_pred == 2)[0]\n",
    "\n",
    "# get the indices where true label is 0\n",
    "label_0_indices_true = np.where(y_test == 0)[0]\n",
    "# get the indices where predicted label is 0\n",
    "label_0_indices_pred = np.where(y_pred == 0)[0]\n",
    "\n",
    "# print number of true and predicted for each label\n",
    "for label in range(3):\n",
    "    label_indices_true = np.where(y_test == label)[0]\n",
    "    label_indices_pred = np.where(y_pred == label)[0]\n",
    "    print(f'Label {label}: {len(label_indices_true)} true, {len(label_indices_pred)} predicted')\n",
    "\n",
    "print(\"=====================================\")\n",
    "\n",
    "# out of the images with label 0, how many did we predict correctly\n",
    "correct_count = 0\n",
    "for label_0_index in label_0_indices_true:\n",
    "    if label_0_index in label_0_indices_pred:\n",
    "        correct_count += 1\n",
    "print(f'Label 0: {correct_count} correct out of {len(label_0_indices_true)}')\n",
    "\n",
    "# out of the images with label 1, how many did we predict correctly\n",
    "correct_count = 0\n",
    "for label_1_index in label_1_indices_true:\n",
    "    if label_1_index in label_1_indices_pred:\n",
    "        correct_count += 1\n",
    "print(f'Label 1: {correct_count} correct out of {len(label_1_indices_true)}')\n",
    "\n",
    "# out of the images with label 2, how many did we predict correctly\n",
    "correct_count = 0\n",
    "for label_2_index in label_2_indices_true:\n",
    "    if label_2_index in label_2_indices_pred:\n",
    "        correct_count += 1\n",
    "print(f'Label 2: {correct_count} correct out of {len(label_2_indices_true)}')\n",
    "\n",
    "# Convert y_test and y_pred to binary format (1 for label '1' and 0 for all other labels)\n",
    "y_test_binary = (y_test == 1).astype(int)\n",
    "y_pred_binary = (y_pred == 1).astype(int)\n",
    "\n",
    "# Calculate Precision, Recall, and F1 Score for label '1' (binary classification)\n",
    "precision = precision_score(y_test_binary, y_pred_binary, pos_label=1)\n",
    "recall = recall_score(y_test_binary, y_pred_binary, pos_label=1)\n",
    "f1 = f1_score(y_test_binary, y_pred_binary, pos_label=1)\n",
    "\n",
    "print(f\"Precision for label 1: {precision:.2f}\")\n",
    "print(f\"Recall for label 1: {recall:.2f}\")\n",
    "print(f\"F1 Score for label 1: {f1:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa31404",
   "metadata": {},
   "source": [
    "### 12. Hyperparameters Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81addd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print number of \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.99)\n",
    "\n",
    "# print the count of y_test where label is 2\n",
    "print(np.count_nonzero(y_test == 0))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
