{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d017333",
   "metadata": {},
   "source": [
    "# Final Assessment Scratch Pad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d00386",
   "metadata": {},
   "source": [
    "## Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea516aa7",
   "metadata": {},
   "source": [
    "1. Please use only this Jupyter notebook to work on your model, and **do not use any extra files**. If you need to define helper classes or functions, feel free to do so in this notebook.\n",
    "2. This template is intended to be general, but it may not cover every use case. The sections are given so that it will be easier for us to grade your submission. If your specific use case isn't addressed, **you may add new Markdown or code blocks to this notebook**. However, please **don't delete any existing blocks**.\n",
    "3. If you don't think a particular section of this template is necessary for your work, **you may skip it**. Be sure to explain clearly why you decided to do so."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022cb4cd",
   "metadata": {},
   "source": [
    "## Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c14a2d8",
   "metadata": {},
   "source": [
    "**[TODO]**\n",
    "\n",
    "Please provide a summary of the ideas and steps that led you to your final model. Someone reading this summary should understand why you chose to approach the problem in a particular way and able to replicate your final model at a high level. Please ensure that your summary is detailed enough to provide an overview of your thought process and approach but also concise enough to be easily understandable. Also, please follow the guidelines given in the `main.ipynb`.\n",
    "\n",
    "This report should not be longer than **1-2 pages of A4 paper (up to around 1,000 words)**. Marks will be deducted if you do not follow instructions and you include too many words here. \n",
    "\n",
    "**[DELETE EVERYTHING FROM THE PREVIOUS TODO TO HERE BEFORE SUBMISSION]**\n",
    "\n",
    "##### Overview\n",
    "**[TODO]**\n",
    "\n",
    "##### 1. Descriptive Analysis\n",
    "The first step is to get an intuition of what is the kind of images I am dealing with, which is why I plot the first 10 images. However, there are some errors, which is why I did some transformation and changing type to uint8 to be able to plot.\n",
    "\n",
    "The next step is to understand what is the nature of the labels, namely:\n",
    "1. How many nans are there? This will affect how we choose to process the data later on.\n",
    "2. What is the make up of each labels as a percentage of the whole dataset? This will affect whether or not under/over sampling is used.\n",
    "\n",
    "##### 2. Detection and Handling of Missing Values\n",
    "**[TODO]**\n",
    "\n",
    "##### 3. Detection and Handling of Outliers\n",
    "**[TODO]**\n",
    "\n",
    "##### 4. Detection and Handling of Class Imbalance \n",
    "**[TODO]**\n",
    "\n",
    "##### 5. Understanding Relationship Between Variables\n",
    "**[TODO]**\n",
    "\n",
    "##### 6. Data Visualization\n",
    "**[TODO]** \n",
    "##### 7. General Preprocessing\n",
    "**[TODO]**\n",
    " \n",
    "##### 8. Feature Selection \n",
    "**[TODO]**\n",
    "\n",
    "##### 9. Feature Engineering\n",
    "**[TODO]**\n",
    "\n",
    "##### 10. Creating Models\n",
    "**[TODO]**\n",
    "\n",
    "##### 11. Model Evaluation\n",
    "**[TODO]**\n",
    "\n",
    "##### 12. Hyperparameters Search\n",
    "**[TODO]**\n",
    "\n",
    "##### Conclusion\n",
    "**[TODO]**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49dcaf29",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27103374",
   "metadata": {},
   "source": [
    "# Workings (Not Graded)\n",
    "\n",
    "You will do your working below. Note that anything below this section will not be graded, but we might counter-check what you wrote in the report above with your workings to make sure that you actually did what you claimed to have done. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4c6cd4",
   "metadata": {},
   "source": [
    "## Import Packages\n",
    "\n",
    "Here, we import some packages necessary to run this notebook. In addition, you may import other packages as well. Do note that when submitting your model, you may only use packages that are available in Coursemology (see `main.ipynb`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cded1ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748c35d7",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "\n",
    "The dataset `data/images.npy` is of size $(N, C, H, W)$, where $N$, $C$, $H$, and $W$ correspond to the number of data, image channels, image width, and image height, respectively.\n",
    "\n",
    "A code snippet that loads the data is provided below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09da291",
   "metadata": {},
   "source": [
    "### Load Image Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6297e25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data.npy', 'rb') as f:\n",
    "    data = np.load(f, allow_pickle=True).item()\n",
    "    images = data['image']\n",
    "    labels = data['label']\n",
    "    \n",
    "print('Shape:', images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229af1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with subplots\n",
    "plt.figure(figsize=(15, 15))  # Adjust the size as needed\n",
    "\n",
    "# Loop through the first 10 images\n",
    "for i in range(10):\n",
    "    # Access the image\n",
    "    image = images[i]\n",
    "\n",
    "    # Convert to uint8\n",
    "    image = np.array(image, dtype='uint8')\n",
    "\n",
    "    # Rearrange the axes from [channels, height, width] to [height, width, channels]\n",
    "    image = np.transpose(image, (1, 2, 0))\n",
    "\n",
    "    # Plot the image\n",
    "    plt.subplot(2, 5, i + 1)  # Adjust the layout (rows, columns, index) as needed\n",
    "    plt.imshow(image)\n",
    "    plt.title(f\"Label: {labels[i]}\")\n",
    "    plt.axis('off')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb651acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# count the number of nans in labels\n",
    "nan_count = 0\n",
    "for label in labels:\n",
    "    if np.isnan(label):\n",
    "        nan_count += 1\n",
    "\n",
    "total_count = len(labels)\n",
    "\n",
    "print('NaN count:', nan_count)\n",
    "# print nan count percentage\n",
    "print('NaN count percentage:', nan_count / len(labels) * 100, \"%\")\n",
    "\n",
    "# remove nans and plot the label count\n",
    "labels = labels[~np.isnan(labels)]\n",
    "label_count = {}\n",
    "for label in labels:\n",
    "    if label not in label_count:\n",
    "        label_count[label] = 0\n",
    "    label_count[label] += 1\n",
    "\n",
    "bars = plt.bar(label_count.keys(), label_count.values())\n",
    "\n",
    "plt.title('Label Count')\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Add the percentage to each bar\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    percentage = f'{100 * height / total_count:.2f}%'\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, height, percentage, ha='center', va='bottom')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe832b6",
   "metadata": {},
   "source": [
    "## Data Exploration & Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6a464c",
   "metadata": {},
   "source": [
    "### 1. Descriptive Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb61967",
   "metadata": {},
   "source": [
    "### 2. Detection and Handling of Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb9cdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('NaN count percentage:', nan_count / len(images) * 100, \"%\") # arund 100%\n",
    "\n",
    "# remove images where label is nan\n",
    "images = images[~np.isnan(labels)]\n",
    "labels = labels[~np.isnan(labels)]\n",
    "\n",
    "print('Shape:', images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880d7884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the max nan values in an image\n",
    "max_nan_count = 0\n",
    "for image in images:\n",
    "    nan_count = np.isnan(image).sum()\n",
    "    if nan_count > max_nan_count:\n",
    "        max_nan_count = nan_count\n",
    "\n",
    "print('Max NaN count:', max_nan_count)\n",
    "# replace nan values with 0\n",
    "images = np.nan_to_num(images)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adcb9cd",
   "metadata": {},
   "source": [
    "### 3. Detection and Handling of Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1c17a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d4916043",
   "metadata": {},
   "source": [
    "### 4. Detection and Handling of Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4cddb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3ab20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_1_indices = np.where(labels == 1)[0]\n",
    "label_1_count = len(label_1_indices)\n",
    "add_count = 300 - label_1_count\n",
    "add_indices = np.random.choice(label_1_indices, add_count)\n",
    "images = np.concatenate((images, images[add_indices]))\n",
    "labels = np.concatenate((labels, labels[add_indices]))\n",
    "# oversample the data with label 2 to 300\n",
    "# get the indices of label 2\n",
    "label_2_indices = np.where(labels == 2)[0]\n",
    "# get the number of images with label 2\n",
    "label_2_count = len(label_2_indices)\n",
    "# get the number of images to add\n",
    "add_count = 300 - label_2_count\n",
    "# get the indices to add\n",
    "add_indices = np.random.choice(label_2_indices, add_count)\n",
    "# add the images and labels\n",
    "images = np.concatenate((images, images[add_indices]))\n",
    "labels = np.concatenate((labels, labels[add_indices]))\n",
    "print('New shape:', images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2552a795",
   "metadata": {},
   "source": [
    "### 5. Understanding Relationship Between Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ddbbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each label 0, 1, 2, print out 5 images with that label, side by side\n",
    "for label in range(3):\n",
    "    label_indices = np.where(labels == label)[0]\n",
    "    for i in range(5):\n",
    "        image = images[label_indices[i]]\n",
    "        image = np.array(image, dtype='uint8')\n",
    "        image = np.transpose(image, (1, 2, 0))\n",
    "        plt.subplot(1, 5, i + 1)\n",
    "        plt.imshow(image)\n",
    "        plt.title(f\"Label: {label}\")\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757fb315",
   "metadata": {},
   "source": [
    "### 6. Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f82e42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2a7eebcf",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3e3383",
   "metadata": {},
   "source": [
    "### 7. General Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19174365",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb3aa527",
   "metadata": {},
   "source": [
    "### 8. Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85808bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4921e8ca",
   "metadata": {},
   "source": [
    "### 9. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcde626",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fa676c3f",
   "metadata": {},
   "source": [
    "## Modeling & Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589b37e4",
   "metadata": {},
   "source": [
    "### 10. Creating models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8dffd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, datasets\n",
    "from PIL import Image\n",
    "\n",
    "class Model:  \n",
    "    \"\"\"\n",
    "    This class represents an AI model.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Constructor for Model class.\n",
    "  \n",
    "        Parameters\n",
    "        ----------\n",
    "        self : object\n",
    "            The instance of the object passed by Python.\n",
    "        \"\"\"\n",
    "        # initialize neural network sequence\n",
    "        self.cnn = nn.Sequential(\n",
    "            # convolutional layer and other layers suitable for a 16x16 image\n",
    "            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 3)  \n",
    "        )\n",
    "\n",
    "        # initialize hyperparameters\n",
    "        self.learning_rate = 0.003\n",
    "        self.batch_size = 32\n",
    "        self.epochs = 50\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the model using the input data.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray of shape (n_samples, channel, height, width)\n",
    "            Training data.\n",
    "        y : ndarray of shape (n_samples,)\n",
    "            Target values.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns an instance of the trained model.\n",
    "        \"\"\"\n",
    "\n",
    "        X, y = Model.preprocess(X, y)\n",
    "        X, y = Model.feature_engineer(X, y)\n",
    "\n",
    "        total_count = len(y)\n",
    "\n",
    "        class_counts = torch.bincount(torch.tensor(y, dtype=torch.long))\n",
    "\n",
    "        # if the number of classes is 2, add a 1 to the end of class_counts\n",
    "        if len(class_counts) == 2:\n",
    "            class_counts = torch.cat((class_counts, torch.tensor([1])))\n",
    "        print(\"🚀 ~ file: scratchpad.ipynb:72 ~ class_counts:\", class_counts)\n",
    "\n",
    "        # Increase the weight of the minority classes more significantly\n",
    "        class_weights = torch.tensor([total_count / (len(class_counts) * class_count) for class_count in class_counts])\n",
    "\n",
    "        # Optionally, normalize the weights\n",
    "        class_weights = class_weights / class_weights.sum()\n",
    "        print('Class counts:', class_counts)\n",
    "\n",
    "        print('Class weights:', class_weights)\n",
    "\n",
    "        # print percentage of each label\n",
    "        unique_labels, counts = np.unique(y, return_counts=True)\n",
    "        for label, count in zip(unique_labels, counts):\n",
    "            print(f'Label {label}: {count / total_count * 100:.2f}%')\n",
    "\n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "        y_tensor = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "        # Create a dataset and data loader\n",
    "        dataset = TensorDataset(X_tensor, y_tensor)\n",
    "        dataloader = DataLoader(dataset, batch_size=len(dataset) if self.batch_size is None else self.batch_size, shuffle=True)\n",
    "\n",
    "        # Define loss function and optimizer for classification\n",
    "        criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "        optimizer = optim.Adam(self.cnn.parameters(), lr=self.learning_rate)\n",
    "\n",
    "        # Train the model\n",
    "        for epoch in range(self.epochs):\n",
    "            for inputs, targets in dataloader:\n",
    "                # Zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = self.cnn(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "                # Backward and optimize\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            print(f'Epoch {epoch+1}/{self.epochs}, Loss: {loss.item()}')\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Use the trained model to make predictions.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray of shape (n_samples, channel, height, width)\n",
    "            Input data.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        ndarray of shape (n_samples,)\n",
    "        Predicted target values per element in X.\n",
    "           \n",
    "        \"\"\"\n",
    "        X = Model.preprocess_predict(X)\n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "        dataset = TensorDataset(X_tensor)\n",
    "        dataloader = DataLoader(dataset, batch_size=len(dataset), shuffle=False)\n",
    "        predictions = []\n",
    "        for inputs in dataloader:\n",
    "            outputs = self.cnn(inputs[0])\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            predictions += predicted.tolist()\n",
    "        \n",
    "        return np.array(predictions)\n",
    "        \n",
    "    \n",
    "    @staticmethod\n",
    "    def preprocess(images, labels):\n",
    "        # remove images where label is nan\n",
    "        images = images[~np.isnan(labels)]\n",
    "        labels = labels[~np.isnan(labels)]\n",
    "        \n",
    "        # replace nan values with 0\n",
    "        images = np.nan_to_num(images)\n",
    "\n",
    "        # remove images where label is 2\n",
    "        images = images[labels != 2]\n",
    "        labels = labels[labels != 2]\n",
    "\n",
    "        # balance the dataset using sklearn\n",
    "        images, labels = Model.balance_dataset(images, labels)\n",
    "\n",
    "        # normalize the images\n",
    "        images = images / 255.0\n",
    "\n",
    "        return images, labels\n",
    "    \n",
    "    @staticmethod\n",
    "    def preprocess_predict(images):\n",
    "        # replace nan values with 0\n",
    "        images = np.nan_to_num(images)\n",
    "        \n",
    "        # normalize the images\n",
    "        images = images / 255.0\n",
    "\n",
    "        return images\n",
    "\n",
    "    @staticmethod\n",
    "    def feature_engineer(images, labels):\n",
    "        T = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomRotation(10),\n",
    "        ])\n",
    "\n",
    "        augmented_images = []\n",
    "        augmented_labels = []\n",
    "\n",
    "        # get images and labels where label is 1 or 2\n",
    "        images_to_engineer = images[labels != 0]\n",
    "        labels_to_engineer = labels[labels != 0]\n",
    "\n",
    "        for image, label in zip(images_to_engineer, labels_to_engineer):\n",
    "            image = image.transpose(1, 2, 0)\n",
    "            img_pil = Image.fromarray(image.astype('uint8'), 'RGB')\n",
    "        \n",
    "            # Apply transformations\n",
    "            augmented_img = T(img_pil)\n",
    "\n",
    "            # Convert transformed image to numpy array and append\n",
    "            augmented_np = np.asarray(augmented_img).transpose(2, 0, 1)\n",
    "\n",
    "            augmented_images.append(augmented_np)\n",
    "            augmented_labels.append(label)\n",
    "\n",
    "        images = np.concatenate((images, augmented_images), axis=0)\n",
    "        labels = np.concatenate((labels, augmented_labels), axis=0)\n",
    "\n",
    "        return images, labels\n",
    "    \n",
    "    @staticmethod\n",
    "    def balance_dataset(images, labels, min_proportions=[0.1, 0.5]):\n",
    "        unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "        total_samples = len(labels)\n",
    "        \n",
    "        # Determine minimum count for each label based on proportions\n",
    "        min_counts = [int(total_samples * p) for p in min_proportions]\n",
    "        \n",
    "        # Sort labels by their count (ascending)\n",
    "        sorted_indices = np.argsort(counts)\n",
    "        \n",
    "        for idx, min_count in zip(sorted_indices, min_counts):\n",
    "            label = unique_labels[idx]\n",
    "            current_count = counts[idx]\n",
    "            \n",
    "            if current_count < min_count:\n",
    "                # Calculate the number of samples to add\n",
    "                add_count = min_count - current_count\n",
    "                \n",
    "                # Get indices of the current label\n",
    "                label_indices = np.where(labels == label)[0]\n",
    "                \n",
    "                # Randomly select indices to duplicate\n",
    "                add_indices = np.random.choice(label_indices, add_count)\n",
    "                \n",
    "                # Add the images and labels\n",
    "                images = np.concatenate((images, images[add_indices]))\n",
    "                labels = np.concatenate((labels, labels[add_indices]))\n",
    "                \n",
    "                # Update total samples\n",
    "                total_samples += add_count\n",
    "\n",
    "        return images, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495bf3c0",
   "metadata": {},
   "source": [
    "### 11. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688a22d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "with open('data.npy', 'rb') as f:\n",
    "    data = np.load(f, allow_pickle=True).item()\n",
    "    X = data['image']\n",
    "    y = data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9522df30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "\n",
    "# Filter test data that contains no labels\n",
    "# In Coursemology, the test data is guaranteed to have labels\n",
    "nan_indices = np.argwhere(np.isnan(y_test)).squeeze()\n",
    "mask = np.ones(y_test.shape, bool)\n",
    "mask[nan_indices] = False\n",
    "X_test = X_test[mask]\n",
    "y_test = y_test[mask]\n",
    "\n",
    "# Train and predict\n",
    "model = Model()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate model predition\n",
    "# Learn more: https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics\n",
    "print(\"F1 Score (macro): {0:.2f}\".format(f1_score(y_test, y_pred, average='macro'))) # You may encounter errors, you are expected to figure out what's the issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ddb3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print first 20 predictions beside ground truth\n",
    "#for i in range(20):\n",
    "#    print(f'Prediction: {y_pred[i]}, Ground Truth: {y_test[i]}')\n",
    "\n",
    "# count total number of prediction and ground truth for each label\n",
    "unique_labels, counts_truth = np.unique(y_test, return_counts=True)\n",
    "_, counts_pred = np.unique(y_pred, return_counts=True)\n",
    "\n",
    "for label, count_truth, count_pred in zip(unique_labels, counts_truth, counts_pred):\n",
    "    print(f'Label {label}: {count_truth} ground truth, {count_pred} predictions')\n",
    "\n",
    "# for the predictions that predict 1, print the percentage where the ground truth is 1\n",
    "label_1_indices = np.where(y_pred == 1)[0]\n",
    "label_1_count = len(label_1_indices)\n",
    "label_1_truth_count = 0\n",
    "for i in label_1_indices:\n",
    "    if y_test[i] == 1:\n",
    "        label_1_truth_count += 1\n",
    "\n",
    "label_2_indices = np.where(y_pred == 2)[0]\n",
    "label_2_count = len(label_2_indices)\n",
    "label_2_truth_count = 0\n",
    "for i in label_2_indices:\n",
    "    if y_test[i] == 2:\n",
    "        label_2_truth_count += 1\n",
    "\n",
    "print(f'Label 1: {label_1_truth_count / label_1_count * 100:.2f}%')\n",
    "# print(f'Label 2: {label_2_truth_count / label_2_count * 100:.2f}%')\n",
    "\n",
    "# print the predictions where the ground truth is 1\n",
    "for i in label_1_indices:\n",
    "    print(f'Prediction: {y_pred[i]}, Ground Truth: {y_test[i]}')\n",
    "\n",
    "print(f'Label 2: {label_2_count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa31404",
   "metadata": {},
   "source": [
    "### 12. Hyperparameters Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81addd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print number of \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.99)\n",
    "\n",
    "# print the count of y_test where label is 2\n",
    "print(np.count_nonzero(y_test == 0))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
