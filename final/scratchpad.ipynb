{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d017333",
   "metadata": {},
   "source": [
    "# Final Assessment Scratch Pad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d00386",
   "metadata": {},
   "source": [
    "## Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea516aa7",
   "metadata": {},
   "source": [
    "1. Please use only this Jupyter notebook to work on your model, and **do not use any extra files**. If you need to define helper classes or functions, feel free to do so in this notebook.\n",
    "2. This template is intended to be general, but it may not cover every use case. The sections are given so that it will be easier for us to grade your submission. If your specific use case isn't addressed, **you may add new Markdown or code blocks to this notebook**. However, please **don't delete any existing blocks**.\n",
    "3. If you don't think a particular section of this template is necessary for your work, **you may skip it**. Be sure to explain clearly why you decided to do so."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022cb4cd",
   "metadata": {},
   "source": [
    "## Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c14a2d8",
   "metadata": {},
   "source": [
    "##### Overview\n",
    "\n",
    "##### 1. Descriptive Analysis\n",
    "I plot the first 10 images to get an intuitive sense of what the images are.\n",
    "\n",
    "##### 2. Detection and Handling of Missing Values\n",
    "The next step is to understand what is the nature of the labels, namely:\n",
    "1. How many nans are there?\n",
    "2. What is the make up of each labels as a percentage of the whole dataset? This will affect whether or not under/over sampling is used (or other strategies)\n",
    "\n",
    "Given that about 10% of true labels were NaNs, these images were excluded from the dataset to avoid misleading the model. However, since all dataset samples contained some NaNs in RGB values, rather than discarding these samples, NaNs were replaced with zeros to maintain neutrality and data integrity.\n",
    "\n",
    "##### 3. Detection and Handling of Outliers\n",
    "I print out the min and max of the values, which contain -10000 and 10000, that is weird since based on contextual knowledge, RGB values lie between 0 and 255. To prob further, I printed out the percentage of values in images that are between 0 and 255, which turns out to be around 98%. This confirms my suspicion that the values are supposed to be between 0 and 255.\n",
    "\n",
    "I cap the values to 0 - 255 instead of replacing it with 0 or mean etc. This is so that I still retain some information.\n",
    "\n",
    "##### 4. Detection and Handling of Class Imbalance \n",
    "**[TODO]**\n",
    "By plotting the graph, label 1 outweighs the other classes.\n",
    "\n",
    "Here are some of the potential strategies I considered:\n",
    "1. Oversample the minority clss (label 1 and 2)\n",
    "2. Undersample the majority class (label 0)\n",
    "3. Feature engineer to add more variations of the minority class to dataset\n",
    "4. Customize loss function to add weights to minority class\n",
    "5. Random sample during training\n",
    "\n",
    "I have tried all 5, for undersampling, I have tried duplicating samples with label 1 and 2 to a larger proportion, to equal proportion, to more proportion than label 0.\n",
    "I have also tried customizing the loss function to add weights to the minority class, either through am algorithm (based on the proportion of samples they have), or hardcoded based on empirical iterations.  \n",
    "The same goes to trying to customize the loss function. The goal is to \"punish\" the model more if it predicts 1 or 2 wrongly, since the model has a tendency to just predict 0 for everything.\n",
    "In the end, I found that using random sampling during the batching stage to tbe the most consistent and stable one. Thus I do away with the rest of the strategies.\n",
    "\n",
    "##### 5. Understanding Relationship Between Variables\n",
    "Since this is an image data with RGB properties, there is not much to unpack for the relationship due to our contextual knowledge of what RGB entails. I do not intend to compress the image so there is not much need to study the relationship between the variables.\n",
    "Tried to investigate the mean max of the RGB values but there is not much gain.\n",
    "##### 6. Data Visualization\n",
    "I tried to plot a few images for each of the labels side by side to see if I can identify some differences between images of the different labels through eye power, which would help in the contextual udnerstanding bit. Unfortunately, they all look similar to the naked eye.\n",
    "Furthermore, I also tried to visualize the RGB values in graphs but I don't think I could get any meaningful information out of it.\n",
    "##### 7. General Preprocessing\n",
    " After dealing with the nan values and capping to 0-255, I also normalize the RGB values by dividing by 255 to make the gradient descent smoother.\n",
    "##### 8. Feature Selection \n",
    "Initially when I thought the red pixels are the important ones and the rest are noise, I tried to mask all B and G to 0 and only keep the R. However, I realize that there is not any noticible enhancement to the model, which I assume that the neural network has already take into account. Thus in the end, there is no feature selection.\n",
    "##### 9. Feature Engineering\n",
    "Engineering features for the minority class (like flips and rotations) didn't significantly enhance performance, leading to a decision against aggressive feature engineering.\n",
    "\n",
    "##### 10. Creating Models\n",
    "I start off with around 2 convolution layers, with standard industrial practices (pooling after convolution, using relu, kernel size etc.). For the filters, I have experimented with small filters (4, 8), and slowly incremented it to larger ones by power of 2 (32, 64, 128, 256, 512). I realize that the higher filter numbers did not result in better performance, but it increases the training time by a lot. Later on, I realize that the model is also prone to overfitting, which is why it is unable to predict many of the 1s and 2s correctly. Thus I proceeded to add dropout layers internally. The values for the dropouts are selected via hyperparameter search.  \n",
    "For the optimizer, I choose to use the industrial reccomendation, without much testing involved. The batch size is chosen arbitrarily.  \n",
    "Throughout the test, I have tweaked the layers multiple times, either by removing and adding layers, or by tweaking the features of the layers.\n",
    "\n",
    "##### 11. Model Evaluation\n",
    "Focus was on the proportion of predictions for each label, aiming for a distribution close to the actual label distribution. The accuracy of predictions for each label was also scrutinized. These aspects influenced the weight assignment in the loss function.\n",
    "##### 12. Hyperparameters Search\n",
    "Optimization included learning rate, dropout rates, and epochs. Averaging f1 scores over multiple random train-test splits was used to minimize variance and identify robust parameters. \n",
    "\n",
    "Lastly, I also want to investigate with random sampling, would feature engineering improve the model.\n",
    "\n",
    "##### Conclusion\n",
    "In the end, I select the parameters that give me the highest average f1 score, and I realizes that the performance does not change even without oversampling and feature engineering. So some of the functions left in are unused by the end model.\n",
    "https://chat.openai.com/share/522e7233-6f35-41b7-b2fa-3e626972054f\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49dcaf29",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27103374",
   "metadata": {},
   "source": [
    "# Workings (Not Graded)\n",
    "\n",
    "You will do your working below. Note that anything below this section will not be graded, but we might counter-check what you wrote in the report above with your workings to make sure that you actually did what you claimed to have done. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4c6cd4",
   "metadata": {},
   "source": [
    "## Import Packages\n",
    "\n",
    "Here, we import some packages necessary to run this notebook. In addition, you may import other packages as well. Do note that when submitting your model, you may only use packages that are available in Coursemology (see `main.ipynb`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cded1ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748c35d7",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "\n",
    "The dataset `data/images.npy` is of size $(N, C, H, W)$, where $N$, $C$, $H$, and $W$ correspond to the number of data, image channels, image width, and image height, respectively.\n",
    "\n",
    "A code snippet that loads the data is provided below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09da291",
   "metadata": {},
   "source": [
    "### Load Image Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6297e25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data.npy', 'rb') as f:\n",
    "    data = np.load(f, allow_pickle=True).item()\n",
    "    images = data['image']\n",
    "    labels = data['label']\n",
    "    \n",
    "print('Shape:', images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229af1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with subplots\n",
    "plt.figure(figsize=(15, 15))  # Adjust the size as needed\n",
    "\n",
    "# Loop through the first 10 images\n",
    "for i in range(10):\n",
    "    # Access the image\n",
    "    image = images[i]\n",
    "\n",
    "    # Convert to uint8\n",
    "    image = np.array(image, dtype='uint8')\n",
    "\n",
    "    # Rearrange the axes from [channels, height, width] to [height, width, channels]\n",
    "    image = np.transpose(image, (1, 2, 0))\n",
    "\n",
    "    # Plot the image\n",
    "    plt.subplot(2, 5, i + 1)  # Adjust the layout (rows, columns, index) as needed\n",
    "    plt.imshow(image)\n",
    "    plt.title(f\"Label: {labels[i]}\")\n",
    "    plt.axis('off')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb651acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# count the number of nans in labels\n",
    "nan_count = 0\n",
    "for label in labels:\n",
    "    if np.isnan(label):\n",
    "        nan_count += 1\n",
    "\n",
    "total_count = len(labels)\n",
    "\n",
    "print('NaN count:', nan_count)\n",
    "# print nan count percentage\n",
    "print('NaN count percentage:', nan_count / len(labels) * 100, \"%\")\n",
    "\n",
    "# remove nans and plot the label count\n",
    "labels = labels[~np.isnan(labels)]\n",
    "label_count = {}\n",
    "for label in labels:\n",
    "    if label not in label_count:\n",
    "        label_count[label] = 0\n",
    "    label_count[label] += 1\n",
    "\n",
    "bars = plt.bar(label_count.keys(), label_count.values())\n",
    "\n",
    "plt.title('Label Count')\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Add the percentage to each bar\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    percentage = f'{100 * height / total_count:.2f}%'\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, height, percentage, ha='center', va='bottom')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe832b6",
   "metadata": {},
   "source": [
    "## Data Exploration & Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6a464c",
   "metadata": {},
   "source": [
    "### 1. Descriptive Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb61967",
   "metadata": {},
   "source": [
    "### 2. Detection and Handling of Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb9cdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('NaN count percentage:', nan_count / len(images) * 100, \"%\") # arund 100%\n",
    "\n",
    "# remove images where label is nan\n",
    "images = images[~np.isnan(labels)]\n",
    "labels = labels[~np.isnan(labels)]\n",
    "\n",
    "print('Shape:', images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880d7884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the max nan values in an image\n",
    "max_nan_count = 0\n",
    "for image in images:\n",
    "    nan_count = np.isnan(image).sum()\n",
    "    if nan_count > max_nan_count:\n",
    "        max_nan_count = nan_count\n",
    "\n",
    "print('Max NaN count:', max_nan_count)\n",
    "# replace nan values with 0\n",
    "images = np.nan_to_num(images)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adcb9cd",
   "metadata": {},
   "source": [
    "### 3. Detection and Handling of Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1c17a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# range of values in images\n",
    "print('Min:', images.min())\n",
    "print('Max:', images.max())\n",
    "\n",
    "# count the percentage of values between 0 and 255 in images\n",
    "print('Percentage of values between 0 and 255:', np.sum((images >= 0) & (images <= 255)) / images.size * 100, \"%\")\n",
    "\n",
    "# cap the values between 0 and 255\n",
    "images = np.clip(images, 0, 255)\n",
    "\n",
    "# count the percentage of values between 0 and 255 in images\n",
    "print('Percentage of values between 0 and 255:', np.sum((images >= 0) & (images <= 255)) / images.size * 100, \"%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4916043",
   "metadata": {},
   "source": [
    "### 4. Detection and Handling of Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4cddb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3ab20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_1_indices = np.where(labels == 1)[0]\n",
    "label_1_count = len(label_1_indices)\n",
    "add_count = 300 - label_1_count\n",
    "add_indices = np.random.choice(label_1_indices, add_count)\n",
    "images = np.concatenate((images, images[add_indices]))\n",
    "labels = np.concatenate((labels, labels[add_indices]))\n",
    "# oversample the data with label 2 to 300\n",
    "# get the indices of label 2\n",
    "label_2_indices = np.where(labels == 2)[0]\n",
    "# get the number of images with label 2\n",
    "label_2_count = len(label_2_indices)\n",
    "# get the number of images to add\n",
    "add_count = 300 - label_2_count\n",
    "# get the indices to add\n",
    "add_indices = np.random.choice(label_2_indices, add_count)\n",
    "# add the images and labels\n",
    "images = np.concatenate((images, images[add_indices]))\n",
    "labels = np.concatenate((labels, labels[add_indices]))\n",
    "print('New shape:', images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2552a795",
   "metadata": {},
   "source": [
    "### 5. Understanding Relationship Between Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ddbbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each label 0, 1, 2, print out 5 images with that label, side by side\n",
    "for label in range(3):\n",
    "    label_indices = np.where(labels == label)[0]\n",
    "    for i in range(10):\n",
    "        image = images[label_indices[i]]\n",
    "        image = np.array(image, dtype='uint8')\n",
    "        image = np.transpose(image, (1, 2, 0))\n",
    "        plt.subplot(2, 5, i + 1)\n",
    "        plt.imshow(image)\n",
    "        plt.title(f\"Label: {label}\")\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757fb315",
   "metadata": {},
   "source": [
    "### 6. Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f82e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i want to analyze the difference between the images with label 0 and 1\n",
    "# get the indices of label 0 and 1\n",
    "label_0_indices = np.where(labels == 0)[0]\n",
    "label_1_indices = np.where(labels == 1)[0]\n",
    "# get the images with label 0 and 1\n",
    "label_0_images = images[label_0_indices]\n",
    "label_1_images = images[label_1_indices]\n",
    "# get the mean of each image\n",
    "label_0_means = np.mean(label_0_images, axis=(1, 2))\n",
    "label_1_means = np.mean(label_1_images, axis=(1, 2))\n",
    "# get the max of each image\n",
    "label_0_maxes = np.max(label_0_images, axis=(1, 2))\n",
    "label_1_maxes = np.max(label_1_images, axis=(1, 2))\n",
    "\n",
    "# plot the means\n",
    "plt.scatter(label_0_means, label_0_maxes, label='Label 0')\n",
    "plt.scatter(label_1_means, label_1_maxes, label='Label 1')\n",
    "plt.title('Mean vs Max')\n",
    "plt.xlabel('Mean')\n",
    "plt.ylabel('Max')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a1c68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shape before:', images.shape)\n",
    "\n",
    "# replace all B with 0, G with 0\n",
    "images[:, 1, :, :] = 0\n",
    "images[:, 2, :, :] = 0\n",
    "\n",
    "# plot the first 10 images\n",
    "for i in range(10):\n",
    "    image = images[i]\n",
    "    image = np.array(image, dtype='uint8')\n",
    "    image = np.transpose(image, (1, 2, 0))\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    plt.imshow(image)\n",
    "    plt.title(f\"Label: {labels[i]}\")\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7eebcf",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3e3383",
   "metadata": {},
   "source": [
    "### 7. General Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19174365",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb3aa527",
   "metadata": {},
   "source": [
    "### 8. Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85808bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4921e8ca",
   "metadata": {},
   "source": [
    "### 9. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcde626",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fa676c3f",
   "metadata": {},
   "source": [
    "## Modeling & Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589b37e4",
   "metadata": {},
   "source": [
    "### 10. Creating models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8dffd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, datasets\n",
    "from PIL import Image\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "class Model:  \n",
    "    \"\"\"\n",
    "    This class represents an AI model.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.0015, dropout=0.15, epochs=20):\n",
    "        \"\"\"\n",
    "        Constructor for Model class.\n",
    "  \n",
    "        Parameters\n",
    "        ----------\n",
    "        self : object\n",
    "            The instance of the object passed by Python.\n",
    "        \"\"\"\n",
    "        # initialize neural network sequence\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 8, kernel_size=3, stride=1, padding=1),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 3)  \n",
    "        )\n",
    "\n",
    "        # initialize hyperparameters\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = 32\n",
    "        self.epochs = epochs\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the model using the input data.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray of shape (n_samples, channel, height, width)\n",
    "            Training data.\n",
    "        y : ndarray of shape (n_samples,)\n",
    "            Target values.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns an instance of the trained model.\n",
    "        \"\"\"\n",
    "\n",
    "        X, y = Model.preprocess(X, y)\n",
    "        #X, y = Model.balance_dataset(X, y)\n",
    "        #X, y = Model.feature_engineer(X, y)\n",
    "\n",
    "        \n",
    "\n",
    "        # Increase the weight of the minority classes more significantly\n",
    "        #class_weights = torch.tensor([total_count / (len(class_counts) * class_count) for class_count in class_counts])\n",
    "\n",
    "        # alternative to increasing weight, since we already random sampled the data\n",
    "        class_weights = torch.tensor([1, 1, 1])\n",
    "\n",
    "        class_weights = class_weights / class_weights.sum()\n",
    "\n",
    "        print('Class weights:', class_weights)\n",
    "        Model.print_class_counts(y)\n",
    "\n",
    "        # print percentage of each label\n",
    "        Model.print_label_percentage(y)\n",
    "\n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "        y_tensor = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "        # Calculate weights for each sample\n",
    "        class_sample_counts = torch.tensor([(y_tensor == t).sum() for t in torch.unique(y_tensor, sorted=True)])\n",
    "        class_weights = 1. / class_sample_counts.float()\n",
    "        print(\"🚀 ~ file: scratchpad.ipynb:89 ~ class_weights:\", class_weights)\n",
    "        weights = class_weights[y_tensor.long()]\n",
    "\n",
    "        # Create a weighted sampler to handle imbalanced classes\n",
    "        sampler = WeightedRandomSampler(weights, len(weights))\n",
    "\n",
    "\n",
    "        # Create a dataset and data loader\n",
    "        dataset = TensorDataset(X_tensor, y_tensor)\n",
    "        dataloader = DataLoader(dataset, batch_size=self.batch_size if self.batch_size else len(dataset), sampler=sampler)\n",
    "\n",
    "        # Define loss function and optimizer for classification\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(self.cnn.parameters(), lr=self.learning_rate)\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            for inputs, targets in dataloader:\n",
    "                # Zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = self.cnn(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "                # Backward and optimize\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            print(f'Epoch {epoch+1}/{self.epochs}, Loss: {loss.item()}')\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Use the trained model to make predictions.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray of shape (n_samples, channel, height, width)\n",
    "            Input data.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        ndarray of shape (n_samples,)\n",
    "        Predicted target values per element in X.\n",
    "           \n",
    "        \"\"\"\n",
    "        X = Model.preprocess_predict(X)\n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "        dataset = TensorDataset(X_tensor)\n",
    "        dataloader = DataLoader(dataset, batch_size=len(dataset), shuffle=False)\n",
    "        predictions = []\n",
    "        for inputs in dataloader:\n",
    "            outputs = self.cnn(inputs[0])\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            predictions += predicted.tolist()\n",
    "\n",
    "        \n",
    "        return np.array(predictions)\n",
    "        \n",
    "    \n",
    "    @staticmethod\n",
    "    def preprocess(images, labels):\n",
    "        # remove images where label is nan\n",
    "        images = images[~np.isnan(labels)]\n",
    "        labels = labels[~np.isnan(labels)]\n",
    "        \n",
    "        # replace nan with 0\n",
    "        images = np.nan_to_num(images)\n",
    "\n",
    "        # cap min to 0 and max to 255\n",
    "        images = np.clip(images, 0, 255)\n",
    "\n",
    "        print('Shape:', images.shape)\n",
    "\n",
    "        # normalize the images\n",
    "        images = images / 255.0\n",
    "\n",
    "        return images, labels\n",
    "    \n",
    "    @staticmethod\n",
    "    def preprocess_predict(images):\n",
    "        # replace nan values with 0\n",
    "        images = np.nan_to_num(images)\n",
    "\n",
    "        # cap min to 0 and max to 255\n",
    "        images = np.clip(images, 0, 255)\n",
    "        \n",
    "        # normalize the images\n",
    "        images = images / 255.0\n",
    "\n",
    "        return images\n",
    "\n",
    "    @staticmethod\n",
    "    def feature_engineer(images, labels):\n",
    "        T = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomRotation(10),\n",
    "        ])\n",
    "\n",
    "        # crete 10 different transformations\n",
    "        T_list = [\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomRotation(20),\n",
    "            transforms.RandomVerticalFlip(),\n",
    "            transforms.RandomRotation(40),\n",
    "            transforms.RandomInvert(),\n",
    "            transforms.ColorJitter(brightness=0.5),\n",
    "            transforms.ColorJitter(contrast=0.5),\n",
    "            transforms.ColorJitter(saturation=0.5),\n",
    "            transforms.ColorJitter(hue=0.5),\n",
    "            transforms.RandomGrayscale(p=0.1),\n",
    "        ]\n",
    "\n",
    "        print(\"Shape before:\", images.shape)\n",
    "\n",
    "        augmented_images = []\n",
    "        augmented_labels = []\n",
    "\n",
    "        # get images and labels where label is 1 or 2\n",
    "        images_to_engineer = images[labels != 0]\n",
    "        labels_to_engineer = labels[labels != 0]\n",
    "\n",
    "        print(\"Number of images to engineer:\", len(images_to_engineer))\n",
    "\n",
    "        for (image, label) in zip(images_to_engineer, labels_to_engineer):\n",
    "            image = image.transpose(1, 2, 0)  # Convert to HWC format for PIL\n",
    "            for transform in T_list:\n",
    "                img_pil = Image.fromarray(image.astype('uint8'), 'RGB')\n",
    "\n",
    "                # Apply transformation\n",
    "                augmented_img = transform(img_pil)\n",
    "\n",
    "                # Convert back to CHW format and append\n",
    "                augmented_np = np.asarray(augmented_img).transpose(2, 0, 1)\n",
    "                augmented_images.append(augmented_np)\n",
    "                augmented_labels.append(label)\n",
    "\n",
    "        images = np.concatenate((images, augmented_images), axis=0)\n",
    "        labels = np.concatenate((labels, augmented_labels), axis=0)\n",
    "\n",
    "        print('Shape:', images.shape)\n",
    "\n",
    "        return images, labels\n",
    "    \n",
    "    @staticmethod\n",
    "    def balance_dataset(images, labels, min_proportions=[0.1, 0.9]):\n",
    "        unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "        total_samples = len(labels)\n",
    "        \n",
    "        # Determine minimum count for each label based on proportions\n",
    "        min_counts = [int(total_samples * p) for p in min_proportions]\n",
    "        \n",
    "        # Sort labels by their count (ascending)\n",
    "        sorted_indices = np.argsort(counts)\n",
    "        \n",
    "        for idx, min_count in zip(sorted_indices, min_counts):\n",
    "            label = unique_labels[idx]\n",
    "            current_count = counts[idx]\n",
    "            \n",
    "            if current_count < min_count:\n",
    "                # Calculate the number of samples to add\n",
    "                add_count = min_count - current_count\n",
    "                \n",
    "                # Get indices of the current label\n",
    "                label_indices = np.where(labels == label)[0]\n",
    "                \n",
    "                # Randomly select indices to duplicate\n",
    "                add_indices = np.random.choice(label_indices, add_count)\n",
    "                \n",
    "                # Add the images and labels\n",
    "                images = np.concatenate((images, images[add_indices]))\n",
    "                labels = np.concatenate((labels, labels[add_indices]))\n",
    "\n",
    "        return images, labels\n",
    "\n",
    "    @staticmethod\n",
    "    def print_label_percentage(y):\n",
    "        total_count = len(y)\n",
    "        unique_labels, counts = np.unique(y, return_counts=True)\n",
    "        for label, count in zip(unique_labels, counts):\n",
    "            print(f'Label {label}: {count / total_count * 100:.2f}%')\n",
    "\n",
    "    @staticmethod\n",
    "    def print_class_counts(y):\n",
    "        unique_labels, counts = np.unique(y, return_counts=True)\n",
    "        print('Class counts:', dict(zip(unique_labels, counts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495bf3c0",
   "metadata": {},
   "source": [
    "### 11. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688a22d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "with open('data.npy', 'rb') as f:\n",
    "    data = np.load(f, allow_pickle=True).item()\n",
    "    X = data['image']\n",
    "    y = data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9522df30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "\n",
    "# Filter test data that contains no labels\n",
    "# In Coursemology, the test data is guaranteed to have labels\n",
    "nan_indices = np.argwhere(np.isnan(y_test)).squeeze()\n",
    "mask = np.ones(y_test.shape, bool)\n",
    "mask[nan_indices] = False\n",
    "X_test = X_test[mask]\n",
    "y_test = y_test[mask]\n",
    "\n",
    "# Train and predict\n",
    "model = Model()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate model predition\n",
    "# Learn more: https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics\n",
    "print(\"F1 Score (macro): {0:.2f}\".format(f1_score(y_test, y_pred, average='macro'))) # You may encounter errors, you are expected to figure out what's the issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ddb3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print first 20 predictions beside ground truth\n",
    "#for i in range(20):\n",
    "#    print(f'Prediction: {y_pred[i]}, Ground Truth: {y_test[i]}')\n",
    "\n",
    "print(f'Total predictions: {len(y_pred)}')\n",
    "print(f'Total ground truth: {len(y_test)}')\n",
    "\n",
    "# get the indices where true label is 1\n",
    "label_1_indices_true = np.where(y_test == 1)[0]\n",
    "# get the indices where predicted label is 1\n",
    "label_1_indices_pred = np.where(y_pred == 1)[0]\n",
    "\n",
    "# get the indices where true label is 2\n",
    "label_2_indices_true = np.where(y_test == 2)[0]\n",
    "# get the indices where predicted label is 2\n",
    "label_2_indices_pred = np.where(y_pred == 2)[0]\n",
    "\n",
    "# get the indices where true label is 0\n",
    "label_0_indices_true = np.where(y_test == 0)[0]\n",
    "# get the indices where predicted label is 0\n",
    "label_0_indices_pred = np.where(y_pred == 0)[0]\n",
    "\n",
    "# print number of true and predicted for each label\n",
    "for label in range(3):\n",
    "    label_indices_true = np.where(y_test == label)[0]\n",
    "    label_indices_pred = np.where(y_pred == label)[0]\n",
    "    print(f'Label {label}: {len(label_indices_true)} true, {len(label_indices_pred)} predicted')\n",
    "\n",
    "print(\"=====================================\")\n",
    "\n",
    "# out of the images with label 0, how many did we predict correctly\n",
    "correct_count = 0\n",
    "for label_0_index in label_0_indices_true:\n",
    "    if label_0_index in label_0_indices_pred:\n",
    "        correct_count += 1\n",
    "print(f'Label 0: {correct_count} correct out of {len(label_0_indices_true)}')\n",
    "\n",
    "# out of the images with label 1, how many did we predict correctly\n",
    "correct_count = 0\n",
    "for label_1_index in label_1_indices_true:\n",
    "    if label_1_index in label_1_indices_pred:\n",
    "        correct_count += 1\n",
    "print(f'Label 1: {correct_count} correct out of {len(label_1_indices_true)}')\n",
    "\n",
    "# out of the images with label 2, how many did we predict correctly\n",
    "correct_count = 0\n",
    "for label_2_index in label_2_indices_true:\n",
    "    if label_2_index in label_2_indices_pred:\n",
    "        correct_count += 1\n",
    "print(f'Label 2: {correct_count} correct out of {len(label_2_indices_true)}')\n",
    "\n",
    "# Convert y_test and y_pred to binary format (1 for label '1' and 0 for all other labels)\n",
    "y_test_binary = (y_test == 1).astype(int)\n",
    "y_pred_binary = (y_pred == 1).astype(int)\n",
    "\n",
    "# Calculate Precision, Recall, and F1 Score for label '1' (binary classification)\n",
    "precision = precision_score(y_test_binary, y_pred_binary, pos_label=1)\n",
    "recall = recall_score(y_test_binary, y_pred_binary, pos_label=1)\n",
    "f1 = f1_score(y_test_binary, y_pred_binary, pos_label=1)\n",
    "\n",
    "print(f\"Precision for label 1: {precision:.2f}\")\n",
    "print(f\"Recall for label 1: {recall:.2f}\")\n",
    "print(f\"F1 Score for label 1: {f1:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0625c213",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def evaluate_model_f1(model, X_val, y_val):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the validation set using F1 score.\n",
    "\n",
    "    Parameters:\n",
    "    model (torch.nn.Module): The trained model.\n",
    "    X_val (np.ndarray): Validation input data.\n",
    "    y_val (np.ndarray): Validation target data.\n",
    "    average (str): The averaging method for F1 score calculation.\n",
    "\n",
    "    Returns:\n",
    "    float: F1 score of the model on the validation set.\n",
    "    \"\"\"\n",
    "\n",
    "    with torch.no_grad():  # No need to track gradients for validation\n",
    "        for i in range(len(X_val)):\n",
    "            predictions = model.predict(X_val)\n",
    "    f1 = f1_score(y_val, predictions, average='macro')\n",
    "    return f1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa31404",
   "metadata": {},
   "source": [
    "### 12. Hyperparameters Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81addd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "\n",
    "# Define ranges for hyperparameters\n",
    "random_state = [42, 43, 50, 51, 52, 53]\n",
    "#learning_rates = [0.001, 0.002, 0.003, 0.004, 0.005]\n",
    "#epochs = [20, 25, 30, 35, 40]\n",
    "#dropouts = [0.2, 0.25, 0.3, 0.35]\n",
    "\n",
    "learning_rates = [0.0015, 0.002, 0.0025]\n",
    "epochs = [20, 25, 30]\n",
    "dropouts = [0.15, 0.2, 0.25]\n",
    "\n",
    "best_avg_f1 = 0\n",
    "best_params = {}\n",
    "f1_scores = {}\n",
    "\n",
    "# Iterate over each combination of hyperparameters\n",
    "for lr in learning_rates:\n",
    "    for epoch in epochs:\n",
    "        for dropout in dropouts:\n",
    "            current_param_key = f\"LR: {lr}, Epochs: {epoch}, Dropout: {dropout}\"\n",
    "            f1_scores[current_param_key] = []\n",
    "\n",
    "            # Iterate over each random state\n",
    "            for rs in random_state:\n",
    "                X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=rs)\n",
    "                nan_indices = np.argwhere(np.isnan(y_test)).squeeze()\n",
    "                mask = np.ones(y_test.shape, bool)\n",
    "                mask[nan_indices] = False\n",
    "                X_test = X_test[mask]\n",
    "                y_test = y_test[mask]\n",
    "\n",
    "                print(f'Random State: {rs}, Learning Rate: {lr}, Epochs: {epoch}, Dropout: {dropout}')\n",
    "                # Create a new instance of the model with current parameters\n",
    "                model = Model(learning_rate=lr, dropout=dropout)\n",
    "                \n",
    "                # Train the model (you might need to modify the fit method to accept batch size)\n",
    "                model.fit(X_train, y_train)\n",
    "\n",
    "                # Evaluate the model\n",
    "                f1 = evaluate_model_f1(model, X_test, y_test)\n",
    "\n",
    "                # Store the F1 score for the current parameter combination and random state\n",
    "                f1_scores[current_param_key].append(f1)\n",
    "\n",
    "# Average the F1 scores for each parameter combination and find the best parameters\n",
    "for params, scores in f1_scores.items():\n",
    "    avg_f1 = mean(scores)\n",
    "    if avg_f1 > best_avg_f1:\n",
    "        best_avg_f1 = avg_f1\n",
    "        best_params = params\n",
    "\n",
    "print(\"Best params:\", best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4786c1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random state from 1 to 30\n",
    "random_states = range(1, 31)\n",
    "f1_scores = []\n",
    "for r in random_states:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=r)\n",
    "    nan_indices = np.argwhere(np.isnan(y_test)).squeeze()\n",
    "    mask = np.ones(y_test.shape, bool)\n",
    "    mask[nan_indices] = False\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "    model = Model()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    f1_scores.append(f1_score(y_test, y_pred, average='macro'))\n",
    "\n",
    "print(f'Average F1 Score: {np.mean(f1_scores):.2f}')\n",
    "print(f'Nax F1 Score: {np.max(f1_scores):.2f}')\n",
    "print(f'Min F1 Score: {np.min(f1_scores):.2f}')\n",
    "# print percentage that is greater than 0.72\n",
    "print(f'Percentage greater than 0.72: {np.sum(np.array(f1_scores) > 0.72) / len(f1_scores) * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2240ed7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f1_scores)\n",
    "# dropout 0.2, epochs 25, lr 0.002\n",
    "# dropout 0.15, epochs 20, lr 0.0015\n",
    "# {'LR: 0.001, Epochs: 20, Dropout: 0.2': [0.4290123456790123, 0.42907157335018026, 0.6346958087091563, 0.6779798086116009, 0.7363465160075329, 0.7902589688762176], 'LR: 0.001, Epochs: 20, Dropout: 0.25': [0.47154225085259566, 0.6412891880419623, 0.6304584561760517, 0.6519717999559375, 0.6439074375304421, 0.7298374607791648], 'LR: 0.001, Epochs: 20, Dropout: 0.3': [0.45526269942009506, 0.528621739188989, 0.625323487600325, 0.6832603689262831, 0.7066936249731949, 0.6547142337095012], 'LR: 0.001, Epochs: 20, Dropout: 0.35': [0.3933808612994841, 0.6161857401470043, 0.5917379515030029, 0.6657403365827634, 0.7513366066557556, 0.7004013042387761], 'LR: 0.001, Epochs: 25, Dropout: 0.2': [0.4699940582293523, 0.6074631402948776, 0.698690880239785, 0.7299043062200957, 0.7939065273121674, 0.8005701615457713], 'LR: 0.001, Epochs: 25, Dropout: 0.25': [0.47502128477657846, 0.5897210531356873, 0.6139634831687811, 0.6823791125160987, 0.6796211162041206, 0.7139925481252706], 'LR: 0.001, Epochs: 25, Dropout: 0.3': [0.4609870643486858, 0.5734053574071334, 0.6288562467997951, 0.7268431983385254, 0.682183908045977, 0.6206733048838312], 'LR: 0.001, Epochs: 25, Dropout: 0.35': [0.43531202435312033, 0.5063236047107015, 0.6490366095298729, 0.6737299604683117, 0.7910197598473264, 0.7333333333333334], 'LR: 0.001, Epochs: 30, Dropout: 0.2': [0.4670349621330013, 0.6512357618740597, 0.6625966625966626, 0.73970680084493, 0.7930228975005095, 0.6777595538789569], 'LR: 0.001, Epochs: 30, Dropout: 0.25': [0.47154225085259566, 0.7291524653487844, 0.7160007297938332, 0.7289321789321789, 0.771820541342232, 0.6836806993328732], 'LR: 0.001, Epochs: 30, Dropout: 0.3': [0.46741423040982516, 0.6059964018615163, 0.568103818103818, 0.6982045277127243, 0.7331083585364407, 0.7522820800598579], 'LR: 0.001, Epochs: 30, Dropout: 0.35': [0.4318181818181818, 0.5076194192074021, 0.6224886224886225, 0.6816326530612246, 0.7003174603174602, 0.6638579889144223], 'LR: 0.001, Epochs: 35, Dropout: 0.2': [0.4742441132350715, 0.5733743214800043, 0.6571169394596726, 0.669226830517153, 0.7217993915631555, 0.7730425017077095], 'LR: 0.001, Epochs: 35, Dropout: 0.25': [0.4348845598845599, 0.5634108527131784, 0.6277380968764478, 0.6810950300652818, 0.7684089082820447, 0.7044858523119393], 'LR: 0.001, Epochs: 35, Dropout: 0.3': [0.4635996659242762, 0.5603024260551678, 0.6452991452991452, 0.5918215321885046, 0.76757139485485, 0.7346556062650095], 'LR: 0.001, Epochs: 35, Dropout: 0.35': [0.4612650515015233, 0.504985754985755, 0.6181882161801976, 0.6712320483749056, 0.6823301793119562, 0.6186868686868687], 'LR: 0.001, Epochs: 40, Dropout: 0.2': [0.4807984270390005, 0.6617211753663995, 0.6274455974670107, 0.7910457516339869, 0.7065918653576437, 0.6971830985915494], 'LR: 0.001, Epochs: 40, Dropout: 0.25': [0.4340722662870313, 0.6215784230637703, 0.7140613847251963, 0.754156954156954, 0.6989992721979621, 0.8085659871012423], 'LR: 0.001, Epochs: 40, Dropout: 0.3': [0.46205623710780586, 0.5759983470453435, 0.6807198463249419, 0.7473544973544973, 0.7942686357243319, 0.6989533630906571], 'LR: 0.001, Epochs: 40, Dropout: 0.35': [0.42065464183744067, 0.5468202543796497, 0.6176086504470784, 0.6802671523982999, 0.768098568098568, 0.6350665310619543], 'LR: 0.002, Epochs: 20, Dropout: 0.2': [0.4406018518518519, 0.6939968048720341, 0.7732703844701879, 0.65441400304414, 0.8016664339244984, 0.7492753623188405], 'LR: 0.002, Epochs: 20, Dropout: 0.25': [0.46724286302241175, 0.6041938287701, 0.6789865871833086, 0.7702506153501902, 0.6503641793863842, 0.6681547619047619], 'LR: 0.002, Epochs: 20, Dropout: 0.3': [0.482078853046595, 0.6095142805166455, 0.7305940406149175, 0.6868946868946869, 0.7004272237048751, 0.7435542431137145], 'LR: 0.002, Epochs: 20, Dropout: 0.35': [0.4224520577461754, 0.602431598766174, 0.6332607116920843, 0.6215780998389694, 0.7090857366380997, 0.7860148032723023], 'LR: 0.002, Epochs: 25, Dropout: 0.2': [0.5040084388185654, 0.6492926284437827, 0.7495726495726496, 0.71011119278472, 0.8089998608361503, 0.7784469721875205], 'LR: 0.002, Epochs: 25, Dropout: 0.25': [0.5271789991276074, 0.6025910364145658, 0.6711089546606286, 0.7187590187590187, 0.7374001452432825, 0.7044858523119393], 'LR: 0.002, Epochs: 25, Dropout: 0.3': [0.4619883040935672, 0.4296296296296296, 0.6278186143957956, 0.68986271725164, 0.7830310010557634, 0.7333517207563359], 'LR: 0.002, Epochs: 25, Dropout: 0.35': [0.433540757694967, 0.38342342342342345, 0.5944018245905038, 0.7114396541904231, 0.7536143077155614, 0.6907020872865276], 'LR: 0.002, Epochs: 30, Dropout: 0.2': [0.47965753542885753, 0.4635008452435838, 0.6931242854809733, 0.7210176991150443, 0.7281873491723599, 0.7044905058150092], 'LR: 0.002, Epochs: 30, Dropout: 0.25': [0.5096251266464032, 0.4624926661637751, 0.6586604884477225, 0.6016726403823177, 0.7931153641679959, 0.6646644824657202], 'LR: 0.002, Epochs: 30, Dropout: 0.3': [0.49157686114207855, 0.6248765066192452, 0.6699404761904763, 0.6801923543881504, 0.6895805344081206, 0.6645173453996983], 'LR: 0.002, Epochs: 30, Dropout: 0.35': [0.4379699248120301, 0.579909906175296, 0.5891383083436063, 0.6853843474533129, 0.8155172413793105, 0.7409337778403843], 'LR: 0.002, Epochs: 35, Dropout: 0.2': [0.48297811878374536, 0.6372759856630824, 0.7248989239285937, 0.749213357160377, 0.749698119717095, 0.6801795685602268], 'LR: 0.002, Epochs: 35, Dropout: 0.25': [0.4270186496915988, 0.449813258636788, 0.6260348583877996, 0.7058919819739865, 0.8653483045639908, 0.7618357487922706], 'LR: 0.002, Epochs: 35, Dropout: 0.3': [0.5168611301761611, 0.5873015873015873, 0.6738679759956355, 0.7109118086696563, 0.7405017921146954, 0.6399342076786564], 'LR: 0.002, Epochs: 35, Dropout: 0.35': [0.42465753424657526, 0.559371250299976, 0.6360750360750361, 0.6432447283467271, 0.6939000745712155, 0.7544027401810925], 'LR: 0.002, Epochs: 40, Dropout: 0.2': [0.50525167523765, 0.6254017342793038, 0.6394938394938395, 0.7565909308791103, 0.8292817104539304, 0.737012987012987], 'LR: 0.002, Epochs: 40, Dropout: 0.25': [0.4311155913978495, 0.6975784898086794, 0.6998998553466117, 0.7571895424836601, 0.7753490394999828, 0.7242445054945055], 'LR: 0.002, Epochs: 40, Dropout: 0.3': [0.45896877269426284, 0.5952294841987428, 0.662917186173, 0.6350488084984208, 0.7175965665236053, 0.7716392625809666], 'LR: 0.002, Epochs: 40, Dropout: 0.35': [0.4285745758977748, 0.613152400835073, 0.5847092778960276, 0.6730457889427454, 0.7618799329325645, 0.7043961507048756], 'LR: 0.003, Epochs: 20, Dropout: 0.2': [0.4648198725451944, 0.675846786957898, 0.6284521198469311, 0.7672771672771672, 0.7848610498831877, 0.6208250910237666], 'LR: 0.003, Epochs: 20, Dropout: 0.25': [0.48526941374101834, 0.6637709936295192, 0.7001307650490002, 0.6828947835659246, 0.671872571872572, 0.6610137855638215], 'LR: 0.003, Epochs: 20, Dropout: 0.3': [0.4488611634992021, 0.6637709936295192, 0.6986111111111111, 0.7525641025641026, 0.7688020168884028, 0.7584325396825395], 'LR: 0.003, Epochs: 20, Dropout: 0.35': [0.4273643879814479, 0.5651464590285502, 0.6913980570031525, 0.65230383043976, 0.6457556935817805, 0.7791054409680095], 'LR: 0.003, Epochs: 25, Dropout: 0.2': [0.45526269942009506, 0.6421326483567147, 0.679766081871345, 0.6981133704056156, 0.8496703885492313, 0.7095489604292422], 'LR: 0.003, Epochs: 25, Dropout: 0.25': [0.45858279651383094, 0.5653235653235653, 0.7238550403107364, 0.7769771955361476, 0.7351795989050891, 0.7298374607791648], 'LR: 0.003, Epochs: 25, Dropout: 0.3': [0.4765654908394859, 0.5468202543796497, 0.6421356421356421, 0.7223250163536118, 0.7287254855926499, 0.680232834402471], 'LR: 0.003, Epochs: 25, Dropout: 0.35': [0.4987346700408799, 0.5960900140646976, 0.602596003723053, 0.6802503716828056, 0.6968512325034064, 0.7002525252525252], 'LR: 0.003, Epochs: 30, Dropout: 0.2': [0.4638151425762045, 0.5875022349365278, 0.6882325363338021, 0.7214232449131778, 0.7671764400285732, 0.6324868745312332], 'LR: 0.003, Epochs: 30, Dropout: 0.25': [0.49007899483734524, 0.6555955715619581, 0.680368149258704, 0.7047886692356218, 0.7266992266992266, 0.7202876293200947], 'LR: 0.003, Epochs: 30, Dropout: 0.3': [0.4772893772893773, 0.5348797602016487, 0.7140670064398877, 0.6761904761904761, 0.6849849849849851, 0.845891422010825], 'LR: 0.003, Epochs: 30, Dropout: 0.35': [0.46751727804359383, 0.5842821339061941, 0.6088336783988958, 0.731454196028187, 0.6850810750154296, 0.7972903951867721], 'LR: 0.003, Epochs: 35, Dropout: 0.2': [0.4848679383712399, 0.5950274938434031, 0.7197069420496752, 0.6910310620487826, 0.6933417530162557, 0.6405753549013163], 'LR: 0.003, Epochs: 35, Dropout: 0.25': [0.4387569031989164, 0.5881370091896407, 0.6752864157119477, 0.7128344671201815, 0.7315423976608186, 0.7821743792790564], 'LR: 0.003, Epochs: 35, Dropout: 0.3': [0.4572043010752688, 0.5318382892153384, 0.6608969154808041, 0.6500184628369468, 0.721125730994152, 0.7861223571749889], 'LR: 0.003, Epochs: 35, Dropout: 0.35': [0.4664516774161292, 0.5141712599018101, 0.6218330089889723, 0.8452325678298035, 0.6899980677507602, 0.7425364758698092], 'LR: 0.003, Epochs: 40, Dropout: 0.2': [0.4757860681427561, 0.4588164251207729, 0.691796157059315, 0.7289321789321789, 0.7954725523486136, 0.7407889409451096], 'LR: 0.003, Epochs: 40, Dropout: 0.25': [0.4386430678466076, 0.4481242487505535, 0.6546919084232518, 0.7085482501385992, 0.7338643790849672, 0.7121157801384985], 'LR: 0.003, Epochs: 40, Dropout: 0.3': [0.4612650515015233, 0.5836036692789016, 0.6512345679012346, 0.7474569146700295, 0.7854345508612468, 0.6402563637462966], 'LR: 0.003, Epochs: 40, Dropout: 0.35': [0.46838443306063154, 0.6112731208592721, 0.6844793250138462, 0.7441999808263828, 0.7976624590930341, 0.5148148148148148], 'LR: 0.004, Epochs: 20, Dropout: 0.2': [0.5203270497388144, 0.5503934057699512, 0.6930521863531753, 0.656127672387835, 0.6509319254459643, 0.7485871935211142], 'LR: 0.004, Epochs: 20, Dropout: 0.25': [0.4646063281824871, 0.633459595959596, 0.7071259709557584, 0.8187937131975463, 0.6258464473056747, 0.7263920099875155], 'LR: 0.004, Epochs: 20, Dropout: 0.3': [0.49157686114207855, 0.5592557201252853, 0.6324469181612038, 0.7270150484436199, 0.7765316924705571, 0.6217506071164608], 'LR: 0.004, Epochs: 20, Dropout: 0.35': [0.46085070597087596, 0.6025910364145658, 0.6404614542545577, 0.7091678011760276, 0.7374929987200702, 0.6960067872125585], 'LR: 0.004, Epochs: 25, Dropout: 0.2': [0.5324792190955541, 0.7062832062832062, 0.687631681623098, 0.7701900683645105, 0.7820149712306574, 0.712739054844318], 'LR: 0.004, Epochs: 25, Dropout: 0.25': [0.49512987012987014, 0.5434962314612205, 0.7140613847251963, 0.7477519272492268, 0.7017632773730335, 0.7735042735042734], 'LR: 0.004, Epochs: 25, Dropout: 0.3': [0.4506884639711964, 0.6706015739629185, 0.5353094926933587, 0.7505912405173442, 0.7905662621936712, 0.6904563892801984], 'LR: 0.004, Epochs: 25, Dropout: 0.35': [0.43047682933646375, 0.7465656031494213, 0.6076409414340448, 0.71011119278472, 0.7153776571687019, 0.6646399062548577], 'LR: 0.004, Epochs: 30, Dropout: 0.2': [0.459366807824158, 0.608457711442786, 0.6955605599673396, 0.790703292986398, 0.7018526743653934, 0.757431813040942], 'LR: 0.004, Epochs: 30, Dropout: 0.25': [0.45800252723934864, 0.6426608251905453, 0.6100000870026708, 0.7189994914823289, 0.6498403072569233, 0.7008064728440674], 'LR: 0.004, Epochs: 30, Dropout: 0.3': [0.45401303307762103, 0.6454490928175138, 0.626858553554396, 0.6780626780626781, 0.781559330709921, 0.704399888610415], 'LR: 0.004, Epochs: 30, Dropout: 0.35': [0.4296535341311461, 0.5104177288331463, 0.6887239744382602, 0.7094158755681993, 0.7257418909592822, 0.6721440999964491], 'LR: 0.004, Epochs: 35, Dropout: 0.2': [0.4812409812409812, 0.6041938287701, 0.7091688818004607, 0.7466049057795557, 0.7527550463720676, 0.7508450154190064], 'LR: 0.004, Epochs: 35, Dropout: 0.25': [0.46548400541835994, 0.6003787878787878, 0.7102453102453102, 0.7085858585858585, 0.7068060574771984, 0.6045843045843046], 'LR: 0.004, Epochs: 35, Dropout: 0.3': [0.43725497291885307, 0.5638263707915449, 0.6769994764055238, 0.6845182347098056, 0.7972949297452608, 0.6294878899728487], 'LR: 0.004, Epochs: 35, Dropout: 0.35': [0.48932522985818916, 0.6041938287701, 0.6059240852780179, 0.6016261882598516, 0.7211458850803112, 0.6301158301158302], 'LR: 0.004, Epochs: 40, Dropout: 0.2': [0.5040084388185654, 0.685368093350355, 0.737705899970051, 0.7176353058706, 0.7483551874856221, 0.733221310665217], 'LR: 0.004, Epochs: 40, Dropout: 0.25': [0.470306673345578, 0.6087538433713148, 0.7541937897493453, 0.7610119047619048, 0.8037264363379014, 0.7584325396825395], 'LR: 0.004, Epochs: 40, Dropout: 0.3': [0.4612650515015233, 0.5277963168953157, 0.6517974562035037, 0.7318090307736554, 0.7185469037400368, 0.8215492277992277], 'LR: 0.004, Epochs: 40, Dropout: 0.35': [0.41058201058201055, 0.7771145378507341, 0.4919958610991258, 0.7252808988764045, 0.6710142026922848, 0.6744380823528465], 'LR: 0.005, Epochs: 20, Dropout: 0.2': [0.49220552151746233, 0.6713214653732722, 0.6676989676989676, 0.6992694166573002, 0.7428571428571429, 0.7236092299056752], 'LR: 0.005, Epochs: 20, Dropout: 0.25': [0.448063198962839, 0.4580799370669571, 0.6485090787939364, 0.7332710190777489, 0.7402218296825223, 0.7263920099875155], 'LR: 0.005, Epochs: 20, Dropout: 0.3': [0.45526269942009506, 0.5790794979079498, 0.603566948089377, 0.7434537125812293, 0.7009804377985308, 0.6236302108233265], 'LR: 0.005, Epochs: 20, Dropout: 0.35': [0.4735811628925471, 0.5670410397683124, 0.659421596921597, 0.7517661296422359, 0.5845052995528229, 0.6474414474414475], 'LR: 0.005, Epochs: 25, Dropout: 0.2': [0.4845089550971904, 0.6323286849602638, 0.7036593228601532, 0.740076411044153, 0.7444468100205804, 0.6468189151038516], 'LR: 0.005, Epochs: 25, Dropout: 0.25': [0.49371662643552044, 0.6984126984126985, 0.6485814062895813, 0.8205715361209572, 0.7030368809721423, 0.7580733442802408], 'LR: 0.005, Epochs: 25, Dropout: 0.3': [0.4966475095785441, 0.648306088604596, 0.6955962135668051, 0.6098620392189601, 0.7812548922671282, 0.8496296296296296], 'LR: 0.005, Epochs: 25, Dropout: 0.35': [0.47389488840892735, 0.5378193065559232, 0.5496291821873217, 0.7702506153501902, 0.7038754311286346, 0.5880139184487011], 'LR: 0.005, Epochs: 30, Dropout: 0.2': [0.45008389261744974, 0.6195700061679443, 0.659881086576929, 0.6987785645814651, 0.8101713378383368, 0.7675641431738992], 'LR: 0.005, Epochs: 30, Dropout: 0.25': [0.482832618025751, 0.5659632475303615, 0.721055441165256, 0.6235897363921111, 0.7047846889952153, 0.7679712981082844], 'LR: 0.005, Epochs: 30, Dropout: 0.3': [0.48579438244649303, 0.5771403737505433, 0.6942245209203635, 0.7963472834067548, 0.6951790402770796, 0.6328181248655876], 'LR: 0.005, Epochs: 30, Dropout: 0.35': [0.41391042627670366, 0.527951095512515, 0.6873991602585271, 0.7094158755681993, 0.7242926155969633, 0.7227278763547956], 'LR: 0.005, Epochs: 35, Dropout: 0.2': [0.4812409812409812, 0.637037037037037, 0.638539887391662, 0.8495972945312152, 0.7774428040121718, 0.7924065769805679], 'LR: 0.005, Epochs: 35, Dropout: 0.25': [0.4765654908394859, 0.7414506875391025, 0.687631681623098, 0.7283034566220407, 0.7142892304230265, 0.7231933561047486], 'LR: 0.005, Epochs: 35, Dropout: 0.3': [0.4822180671237275, 0.5286227246360137, 0.6927236971484759, 0.6638140796653517, 0.7357336349256318, 0.6600125633777538], 'LR: 0.005, Epochs: 35, Dropout: 0.35': [0.44403586426058334, 0.5503934057699512, 0.6229136656102948, 0.668501767013585, 0.7427323883845623, 0.647736777873764], 'LR: 0.005, Epochs: 40, Dropout: 0.2': [0.45418646676929253, 0.40308248800817803, 0.7799494590539368, 0.746518259444812, 0.7452176578786021, 0.7038809144072302], 'LR: 0.005, Epochs: 40, Dropout: 0.25': [0.4608849166942513, 0.5722579377064427, 0.6765211974531399, 0.6498771498771498, 0.7105974354551204, 0.6843106291890374], 'LR: 0.005, Epochs: 40, Dropout: 0.3': [0.4797101449275362, 0.7901066391468868, 0.6793650793650795, 0.6767083753385124, 0.6446110348549373, 0.667888214196939], 'LR: 0.005, Epochs: 40, Dropout: 0.35': [0.35862896157217333, 0.558652495529572, 0.7068710440021353, 0.7278585214069085, 0.6624354494573116, 0.7722385141739979]}\n",
    "# second"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
