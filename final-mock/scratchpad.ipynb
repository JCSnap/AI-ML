{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d017333",
   "metadata": {},
   "source": [
    "# Final Assessment Scratch Pad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d00386",
   "metadata": {},
   "source": [
    "## Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea516aa7",
   "metadata": {},
   "source": [
    "1. Please use only this Jupyter notebook to work on your model, and **do not use any extra files**. If you need to define helper classes or functions, feel free to do so in this notebook.\n",
    "2. This template is intended to be general, but it may not cover every use case. The sections are given so that it will be easier for us to grade your submission. If your specific use case isn't addressed, **you may add new Markdown or code blocks to this notebook**. However, please **don't delete any existing blocks**.\n",
    "3. If you don't think a particular section of this template is necessary for your work, **you may skip it**. Be sure to explain clearly why you decided to do so."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022cb4cd",
   "metadata": {},
   "source": [
    "## Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c14a2d8",
   "metadata": {},
   "source": [
    "**[TODO]**\n",
    "\n",
    "Please provide a summary of the ideas and steps that led you to your final model. Someone reading this summary should understand why you chose to approach the problem in a particular way and able to replicate your final model at a high level. Please ensure that your summary is detailed enough to provide an overview of your thought process and approach but also concise enough to be easily understandable. Also, please follow the guidelines given in the `main.ipynb`.\n",
    "\n",
    "This report should not be longer than **1-2 pages of A4 paper (up to around 1,000 words)**. Marks will be deducted if you do not follow instructions and you include too many words here. \n",
    "\n",
    "**[DELETE EVERYTHING FROM THE PREVIOUS TODO TO HERE BEFORE SUBMISSION]**\n",
    "\n",
    "##### Overview\n",
    "**[TODO]**\n",
    "\n",
    "##### 1. Descriptive Analysis\n",
    "**[TODO]**\n",
    "\n",
    "##### 2. Detection and Handling of Missing Values\n",
    "**[TODO]**\n",
    "\n",
    "##### 3. Detection and Handling of Outliers\n",
    "**[TODO]**\n",
    "\n",
    "##### 4. Detection and Handling of Class Imbalance \n",
    "**[TODO]**\n",
    "\n",
    "##### 5. Understanding Relationship Between Variables\n",
    "**[TODO]**\n",
    "\n",
    "##### 6. Data Visualization\n",
    "**[TODO]** \n",
    "##### 7. General Preprocessing\n",
    "**[TODO]**\n",
    " \n",
    "##### 8. Feature Selection \n",
    "**[TODO]**\n",
    "\n",
    "##### 9. Feature Engineering\n",
    "**[TODO]**\n",
    "\n",
    "##### 10. Creating Models\n",
    "**[TODO]**\n",
    "\n",
    "##### 11. Model Evaluation\n",
    "**[TODO]**\n",
    "\n",
    "##### 12. Hyperparameters Search\n",
    "**[TODO]**\n",
    "\n",
    "##### Conclusion\n",
    "**[TODO]**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49dcaf29",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27103374",
   "metadata": {},
   "source": [
    "# Workings (Not Graded)\n",
    "\n",
    "You will do your working below. Note that anything below this section will not be graded, but we might counter-check what you wrote in the report above with your workings to make sure that you actually did what you claimed to have done. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4c6cd4",
   "metadata": {},
   "source": [
    "## Import Packages\n",
    "\n",
    "Here, we import some packages necessary to run this notebook. In addition, you may import other packages as well. Do note that when submitting your model, you may only use packages that are available in Coursemology (see `main.ipynb`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cded1ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from util import show_images, dict_train_test_split\n",
    "from sklearn.impute import KNNImputer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748c35d7",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "\n",
    "The dataset provided is multimodal and contains two components, images and tabular data. The tabular dataset `tabular.csv` contains $N$ entries and $F$ columns, including the target feature. On the other hand, the image dataset `images.npy` is of size $(N, H, W)$, where $N$, $H$, and $W$ correspond to the number of data, image width, and image height, respectively. Each image corresponds to the data in the same index of the tabular dataset. These datasets can be found in the `data/` folder in the given file structure.\n",
    "\n",
    "A code snippet that loads and displays some of the data is provided below.\n",
    "\n",
    "### Load Tabular Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88be725",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join('data', 'tabular.csv'))\n",
    "cols = [\"V9\", \"V12\", \"V19\", \"V20\", \"V21\", \"V23\", \"V24\", \"V29\", \"V31\", \"V36\", \"V37\", \"V46\", \"V47\", \"V51\", \"V52\", \"V54\", \"V55\", \"V58\"]\n",
    "df.iloc[:, 50:].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09da291",
   "metadata": {},
   "source": [
    "### Load Image Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6297e25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('data', 'images.npy'), 'rb') as f:\n",
    "    images = np.load(f)\n",
    "    \n",
    "print('Shape:', images.shape)\n",
    "show_images(images[:10], n_row=2, n_col=5, figsize=[12,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe832b6",
   "metadata": {},
   "source": [
    "## Data Exploration & Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6a464c",
   "metadata": {},
   "source": [
    "### 1. Descriptive Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1f62dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head()\n",
    "# get summary statistics\n",
    "df.describe()\n",
    "# count number of occurances where V0 has value more than 8315\n",
    "\n",
    "# df[df['V0'] == 65715].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb61967",
   "metadata": {},
   "source": [
    "### 2. Detection and Handling of Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb9cdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.isna().any(axis=1)]\n",
    "cols = [\"V9\", \"V12\", \"V19\", \"V20\", \"V21\", \"V23\", \"V24\", \"V29\", \"V31\", \"V36\", \"V37\", \"V46\", \"V47\", \"V51\", \"V52\", \"V54\", \"V55\", \"V58\"]\n",
    "\n",
    "# Encoding and decoding functions\n",
    "def encode_categories(column):\n",
    "    return column.str.replace('C', '').astype(float)\n",
    "# \n",
    "# def decode_categories(column):\n",
    "#     return 'C' + column.round().astype(int).astype(str)\n",
    "# \n",
    "for col in cols:\n",
    "    df[col] = encode_categories(df[col])\n",
    "# \n",
    "# imputer = KNNImputer(n_neighbors=2)\n",
    "# df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
    "# \n",
    "# df_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3eb6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "cols_cat = [\"V9\", \"V12\", \"V19\", \"V20\", \"V21\", \"V23\", \"V24\", \"V29\", \"V31\", \"V36\", \"V37\", \"V46\", \"V47\", \"V51\", \"V52\", \"V54\", \"V55\", \"V58\"]\n",
    "## cols that are not categorical\n",
    "cols_num = [col for col in df.columns if col not in cols_cat]\n",
    "imputer = SimpleImputer(strategy=\"mean\")\n",
    "for col in cols_num:\n",
    "    df[col] = imputer.fit_transform(df[[col]])\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adcb9cd",
   "metadata": {},
   "source": [
    "### 3. Detection and Handling of Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1c17a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(cols_num)\n",
    "# For numerical columns, remove outliers that are 1.5 times the interquartile range\n",
    "for col in cols_num:\n",
    "    IQR = df[col].quantile(0.75) - df[col].quantile(0.25)\n",
    "    lower_bound = df[col].quantile(0.25) - (1.5 * IQR)\n",
    "    upper_bound = df[col].quantile(0.75) + (1.5 * IQR)\n",
    "    median_value = df[col].median()\n",
    "    \n",
    "    # Replace with median\n",
    "    df[col] = df[col].mask((df[col] < lower_bound) | (df[col] > upper_bound), median_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4916043",
   "metadata": {},
   "source": [
    "### 4. Detection and Handling of Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3ab20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot distribution of the target column\n",
    "sns.distplot(df['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2552a795",
   "metadata": {},
   "source": [
    "### 5. Understanding Relationship Between Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ddbbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "corr_matrix = df.corr()\n",
    "def custom_annotator(val):\n",
    "    if abs(val) > 0.7 and val != 1:  # Exclude self-correlation\n",
    "        return f'{val:.2f}'\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "# Create the heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(corr_matrix, annot=np.vectorize(custom_annotator)(corr_matrix), fmt=\"\", cmap='coolwarm')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757fb315",
   "metadata": {},
   "source": [
    "### 6. Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f82e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot correlation of variables with target\n",
    "plt.figure(figsize=(12, 8))\n",
    "corr_target = df.corr()['target'].sort_values(ascending=False)\n",
    "sns.barplot(x=corr_target.index, y=corr_target.values)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7eebcf",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3e3383",
   "metadata": {},
   "source": [
    "### 7. General Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19174365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the data\n",
    "# removing duplicates\n",
    "df = df.drop_duplicates()\n",
    "# drop column V2, V1, V20, V19, V13, V50, V5, V29, V55, V56, V59, V58, V57, V23, V3\n",
    "df.drop(columns=['V2', 'V1', 'V20', 'V19', 'V13', 'V50', 'V5', 'V29', 'V55', 'V56', 'V59', 'V58', 'V57', 'V23', 'V3'], inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3aa527",
   "metadata": {},
   "source": [
    "### 8. Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85808bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pca to reduce the dimensionality of the data\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=30)\n",
    "pca.fit(df)\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4921e8ca",
   "metadata": {},
   "source": [
    "### 9. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d43d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the images that are not in the tabular data\n",
    "images = images[df.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcde626",
   "metadata": {},
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa676c3f",
   "metadata": {},
   "source": [
    "## Modeling & Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589b37e4",
   "metadata": {},
   "source": [
    "### 10. Creating models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8dffd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.impute import SimpleImputer\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, tabular_data, image_data, labels):\n",
    "        self.tabular_data = tabular_data\n",
    "        self.image_data = image_data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tabular_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tabular_sample = self.tabular_data[idx]\n",
    "        image_sample = self.image_data[idx]\n",
    "        label_sample = self.labels[idx]\n",
    "\n",
    "        image_sample = torch.tensor(image_sample, dtype=torch.float32).unsqueeze(0)\n",
    "        \n",
    "        return tabular_sample, image_sample, label_sample\n",
    "    \n",
    "class Model(nn.Module):  \n",
    "    \"\"\"\n",
    "    This class represents an AI model.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Constructor for Model class.\n",
    "  \n",
    "        Parameters\n",
    "        ----------\n",
    "        self : object\n",
    "            The instance of the object passed by Python.\n",
    "        \"\"\"\n",
    "        super(Model, self).__init__()\n",
    "        # CNN for image data\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            # Add more CNN layers as needed...\n",
    "        )\n",
    "        \n",
    "        # Determine the size of the flattened CNN output\n",
    "        with torch.no_grad():\n",
    "            self.cnn_output_size = self._get_cnn_output_size(torch.zeros((1, 1, 8, 8)))\n",
    "\n",
    "        # Fully connected network for tabular data\n",
    "        self.fc_tabular = nn.Linear(30, 128)\n",
    "\n",
    "        # Combine CNN and tabular pathways\n",
    "        self.fc_combined = nn.Sequential(\n",
    "            nn.Linear(self.cnn_output_size + 128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def _get_cnn_output_size(self, sample_input):\n",
    "        return self.cnn(sample_input).view(sample_input.size(0), -1).shape[1]\n",
    "    \n",
    "    def forward(self, tabular_data, image_data):\n",
    "        # Process image data through CNN\n",
    "        print('image_data:', image_data.shape)\n",
    "        image_output = self.cnn(image_data)\n",
    "        image_output = image_output.view(image_output.size(0), -1)  # Flatten the output\n",
    "\n",
    "        # Process tabular data through fully connected network\n",
    "        tabular_output = self.fc_tabular(tabular_data)\n",
    "\n",
    "        # Combine the outputs from both pathways\n",
    "        combined_output = torch.cat((image_output, tabular_output), dim=1)\n",
    "\n",
    "        # Pass the combined output through additional layers\n",
    "        output = self.fc_combined(combined_output)\n",
    "        return output\n",
    "    \n",
    "    def fit(self, X_dict, y):\n",
    "        \"\"\"\n",
    "        Train the model using the input data.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X_dict : dictionary with the following entries:\n",
    "            - tabular: pandas Dataframe of shape (n_samples, n_features)\n",
    "            - images: ndarray of shape (n_samples, height, width)\n",
    "            Training data.\n",
    "        y : pandas Dataframe of shape (n_samples,)\n",
    "            Target values.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns an instance of the trained model.\n",
    "        \"\"\"\n",
    "        df = X_dict['tabular']\n",
    "        images = X_dict['images']\n",
    "        processed_df, y_aligned = self.process_df(df, y)\n",
    "        # remove the images that are not in the tabular data\n",
    "        images = images[processed_df.index]\n",
    "\n",
    "        # Convert to PyTorch tensors\n",
    "        X_tensor = torch.tensor(processed_df.values, dtype=torch.float32)\n",
    "        y_tensor = torch.tensor(y_aligned.values, dtype=torch.float32)\n",
    "\n",
    "\n",
    "        # print sizes\n",
    "        print('X_tensor:', X_tensor.shape)\n",
    "        print('y_tensor:', y.shape)\n",
    "        \n",
    "        # DataLoader\n",
    "        dataset = CustomDataset(X_tensor, images, y_tensor)\n",
    "        dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "        # Loss function and optimizer\n",
    "        criterion = torch.nn.MSELoss()\n",
    "        optimizer = torch.optim.SGD(self.parameters(), lr=0.01)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(50):\n",
    "            for tabular_inputs, image_inputs, labels in dataloader:\n",
    "                # Forward pass\n",
    "                outputs = self(tabular_inputs, image_inputs)\n",
    "                outputs = outputs.squeeze()\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # Backward and optimize\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            scheduler.step()\n",
    "            # Print loss every few epochs\n",
    "            print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X_dict):\n",
    "        \"\"\"\n",
    "        Use the trained model to make predictions.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X_dict : dictionary with the following entries:\n",
    "            - tabular: pandas Dataframe of shape (n_samples, n_features)\n",
    "            - images: ndarray of shape (n_samples, height, width)\n",
    "            Input data.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        pandas Dataframe of shape (n_samples,)\n",
    "           Predicted target values per element in X_dict.\n",
    "           \n",
    "        \"\"\"\n",
    "        processed_df = self.process_df_predict(X_dict['tabular'])\n",
    "        X_tensor = torch.tensor(processed_df.values, dtype=torch.float32)\n",
    "\n",
    "        # Process the image data\n",
    "        images = X_dict['images']\n",
    "        # Ensure images are in the correct format (as tensors)\n",
    "        images = [torch.tensor(image, dtype=torch.float32).unsqueeze(0) for image in images]\n",
    "        images_tensor = torch.stack(images)\n",
    "\n",
    "        # No gradient calculation needed\n",
    "        with torch.no_grad():\n",
    "            outputs = self(X_tensor, images_tensor)\n",
    "        \n",
    "        # Convert outputs to a pandas DataFrame\n",
    "        predictions = pd.DataFrame(outputs.numpy(), columns=['predictions'])\n",
    "        return predictions\n",
    "\n",
    "    def save(self, path):\n",
    "        \"\"\"\n",
    "        Save the trained model to a file.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        path : str\n",
    "            Path to the file to save the model.\n",
    "        \"\"\"\n",
    "        torch.save(self.model.state_dict(), path)\n",
    "    \n",
    "    def load(self, path):\n",
    "        \"\"\"\n",
    "        Load the trained model from a file.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        path : str\n",
    "            Path to the file to load the model from.\n",
    "        \"\"\"\n",
    "        self.model.load_state_dict(torch.load(path))\n",
    "\n",
    "    def process_df(self, df, y):\n",
    "        \"\"\"\n",
    "        Process the dataframe.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pandas Dataframe of shape (n_samples, n_features)\n",
    "            Input data.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        pandas Dataframe of shape (n_samples, n_features)\n",
    "           Processed data.\n",
    "        \"\"\"\n",
    "        # append target column to df\n",
    "        df['target'] = y\n",
    "        # convert categorical columns to numerical\n",
    "        cols = [\"V9\", \"V12\", \"V19\", \"V20\", \"V21\", \"V23\", \"V24\", \"V29\", \"V31\", \"V36\", \"V37\", \"V46\", \"V47\", \"V51\", \"V52\", \"V54\", \"V55\", \"V58\"]\n",
    "        def encode_categories(column):\n",
    "            return column.str.replace('C', '').astype(float)\n",
    "        for col in cols:\n",
    "            df[col] = encode_categories(df[col])\n",
    "        \n",
    "        # handle missing values\n",
    "        cols_num = [col for col in df.columns if col not in cols_cat]\n",
    "        imputer_mean = SimpleImputer(strategy=\"mean\")\n",
    "        # impute with highest frequency\n",
    "        imputer_freq = SimpleImputer(strategy=\"most_frequent\")\n",
    "        # all columns of df\n",
    "        for col in cols_num:\n",
    "            df[col] = imputer_mean.fit_transform(df[[col]])\n",
    "\n",
    "        for col in cols_cat:\n",
    "            df[col] = imputer_freq.fit_transform(df[[col]])\n",
    "\n",
    "        # handle outliers\n",
    "        for col in cols_num:\n",
    "            IQR = df[col].quantile(0.75) - df[col].quantile(0.25)\n",
    "            lower_bound = df[col].quantile(0.25) - (1.5 * IQR)\n",
    "            upper_bound = df[col].quantile(0.75) + (1.5 * IQR)\n",
    "            median_value = df[col].median()\n",
    "            df[col] = df[col].mask((df[col] < lower_bound) | (df[col] > upper_bound), median_value)\n",
    "        \n",
    "        # drop duplicates\n",
    "        df = df.drop_duplicates()\n",
    "\n",
    "        # drop the 5 least correlated columns with y\n",
    "        corr_target = df.corr()['target'].sort_values(ascending=False)\n",
    "        self.cols_to_drop = corr_target[-5:].index\n",
    "        df.drop(columns=self.cols_to_drop, inplace=True)\n",
    "\n",
    "        # Apply PCA\n",
    "        self.pca = PCA(n_components=30)\n",
    "        df_pca = self.pca.fit_transform(df.drop(columns=['target']))\n",
    "        df = pd.DataFrame(df_pca)\n",
    "\n",
    "        y_aligned = y.loc[df.index]\n",
    "\n",
    "        return df, y_aligned\n",
    "    \n",
    "    def process_df_predict(self, df):\n",
    "        \"\"\"\n",
    "        Process the dataframe.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pandas Dataframe of shape (n_samples, n_features)\n",
    "            Input data.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        pandas Dataframe of shape (n_samples, n_features)\n",
    "           Processed data.\n",
    "        \"\"\"\n",
    "        # convert categorical columns to numerical\n",
    "        cols = [\"V9\", \"V12\", \"V19\", \"V20\", \"V21\", \"V23\", \"V24\", \"V29\", \"V31\", \"V36\", \"V37\", \"V46\", \"V47\", \"V51\", \"V52\", \"V54\", \"V55\", \"V58\"]\n",
    "        # print \"one\" then the shape of the df\n",
    "        def encode_categories(column):\n",
    "            return column.str.replace('C', '').astype(float)\n",
    "        for col in cols:\n",
    "            df[col] = encode_categories(df[col])\n",
    "        \n",
    "        # handle missing values\n",
    "        cols_num = [col for col in df.columns if col not in cols_cat]\n",
    "        imputer_mean = SimpleImputer(strategy=\"mean\")\n",
    "        # impute with highest frequency\n",
    "        imputer_freq = SimpleImputer(strategy=\"most_frequent\")\n",
    "        # all columns of df\n",
    "        for col in cols_num:\n",
    "            df[col] = imputer_mean.fit_transform(df[[col]])\n",
    "\n",
    "        for col in cols_cat:\n",
    "            df[col] = imputer_freq.fit_transform(df[[col]])\n",
    "\n",
    "        # handle outliers\n",
    "        for col in cols_num:\n",
    "            IQR = df[col].quantile(0.75) - df[col].quantile(0.25)\n",
    "            lower_bound = df[col].quantile(0.25) - (1.5 * IQR)\n",
    "            upper_bound = df[col].quantile(0.75) + (1.5 * IQR)\n",
    "            median_value = df[col].median()\n",
    "            df[col] = df[col].mask((df[col] < lower_bound) | (df[col] > upper_bound), median_value)\n",
    "        \n",
    "        # drop the 5 least correlated columns with y\n",
    "        df.drop(columns=self.cols_to_drop, inplace=True)\n",
    "\n",
    "        # Apply PCA\n",
    "        df_pca = self.pca.fit_transform(df)\n",
    "        df = pd.DataFrame(df_pca)\n",
    "\n",
    "        return df\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495bf3c0",
   "metadata": {},
   "source": [
    "### 11. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9245ab47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, mean_squared_error, mean_absolute_error, r2_score\n",
    "from util import dict_train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cf7b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv(os.path.join('data', 'tabular.csv'))\n",
    "with open(os.path.join('data', 'images.npy'), 'rb') as f:\n",
    "    images = np.load(f)\n",
    "    \n",
    "# Exclude target column\n",
    "X_columns = [col for col in df.columns if col != 'target']\n",
    "\n",
    "# Create X_dict and y\n",
    "X_dict = {\n",
    "    'tabular': df[X_columns],\n",
    "    'images': images\n",
    "}\n",
    "y = df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdba6b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dict_train, y_train, X_dict_test, y_test = dict_train_test_split(X_dict, y, ratio=0.9)\n",
    "images = X_dict_train['images']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b027172a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train and test\n",
    "X_dict_train, y_train, X_dict_test, y_test = dict_train_test_split(X_dict, y, ratio=0.9)\n",
    "\n",
    "# Train and predict\n",
    "model = Model()\n",
    "model.fit(X_dict_train, y_train)\n",
    "y_pred = model.predict(X_dict_test)\n",
    "\n",
    "# Evaluate model predition\n",
    "# Learn more: https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics\n",
    "print(\"MSE: {0:.2f}\".format(mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa31404",
   "metadata": {},
   "source": [
    "### 12. Hyperparameters Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81addd51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
