{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from util import show_images, dict_train_test_split\n",
    "from sklearn.impute import KNNImputer\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, mean_squared_error, mean_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv(os.path.join('data', 'tabular.csv'))\n",
    "with open(os.path.join('data', 'images.npy'), 'rb') as f:\n",
    "    images = np.load(f)\n",
    "    \n",
    "# Exclude target column\n",
    "X_columns = [col for col in df.columns if col != 'target']\n",
    "\n",
    "# Create X_dict and y\n",
    "X_dict = {\n",
    "    'tabular': df[X_columns],\n",
    "    'images': images\n",
    "}\n",
    "y = df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class Model:  \n",
    "    \"\"\"\n",
    "    This class represents an AI model.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Constructor for Model class.\n",
    "  \n",
    "        Parameters\n",
    "        ----------\n",
    "        self : object\n",
    "            The instance of the object passed by Python.\n",
    "        \"\"\"\n",
    "        # initialize random forest regressor\n",
    "        self.cols_cat = [\"V9\", \"V12\", \"V19\", \"V20\", \"V21\", \"V23\", \"V24\", \"V29\", \"V31\", \"V36\", \"V37\", \"V46\", \"V47\", \"V51\", \"V52\", \"V54\", \"V55\", \"V58\"]\n",
    "        self.cols_cat_index = [i for i, col in enumerate(X_columns) if col in self.cols_cat]\n",
    "        self.model = HistGradientBoostingRegressor(verbose=2, max_iter=300)\n",
    "\n",
    "    def fit(self, X_dict, y):\n",
    "        \"\"\"\n",
    "        Train the model using the input data.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X_dict : dictionary with the following entries:\n",
    "            - tabular: pandas Dataframe of shape (n_samples, n_features)\n",
    "            - images: ndarray of shape (n_samples, height, width)\n",
    "            Training data.\n",
    "        y : pandas Dataframe of shape (n_samples,)\n",
    "            Target values.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns an instance of the trained model.\n",
    "        \"\"\"\n",
    "        tabular = X_dict['tabular']\n",
    "        tabular = self.preprocess_tabular(tabular)\n",
    "        \n",
    "        # filter y to match the tabular data\n",
    "        y = y.loc[tabular.index]\n",
    "\n",
    "        # train the model with random forest regressor\n",
    "        self.model.fit(tabular, y)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X_dict):\n",
    "        \"\"\"\n",
    "        Use the trained model to make predictions.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X_dict : dictionary with the following entries:\n",
    "            - tabular: pandas Dataframe of shape (n_samples, n_features)\n",
    "            - images: ndarray of shape (n_samples, height, width)\n",
    "            Input data.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        pandas Dataframe of shape (n_samples,)\n",
    "           Predicted target values per element in X_dict.\n",
    "           \n",
    "        \"\"\"\n",
    "        tabular = X_dict['tabular']\n",
    "        tabular = self.preprocess_tabular_predict(tabular)\n",
    "        \n",
    "        # make predictions\n",
    "        y_pred = self.model.predict(tabular)\n",
    "\n",
    "        df = pd.DataFrame(y_pred)\n",
    "        print(df.shape)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def preprocess_tabular(self, X):\n",
    "        \"\"\"\n",
    "        Preprocess the tabular data.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pandas Dataframe of shape (n_samples, n_features)\n",
    "            Input data.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        pandas Dataframe of shape (n_samples, n_features)\n",
    "            Preprocessed data.\n",
    "        \"\"\"\n",
    "        # encode categorical variables\n",
    "        encoder = LabelEncoder()\n",
    "        for col in X.columns:\n",
    "            if col in self.cols_cat:\n",
    "                X[col] = encoder.fit_transform(X[col].astype(str))\n",
    "\n",
    "        # drop categorical variables with too many categories by looking at the unique values\n",
    "        #self.drop_cols = []\n",
    "        #for col in X.columns:\n",
    "        #    if col in self.cols_cat:\n",
    "        #        if len(X[col].unique()) > 255:\n",
    "        #            X = X.drop(col, axis=1)\n",
    "        #            self.drop_cols.append(col)\n",
    "        \n",
    "        # replace missing values with mode\n",
    "        imputer = SimpleImputer(strategy='most_frequent')\n",
    "        df = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
    "\n",
    "        # cap the outlier to 3 std only for numerical variables\n",
    "        for col in df.columns:\n",
    "            if col not in self.cols_cat:\n",
    "                df[col] = df[col].apply(lambda x: 3 if x > 3 else x)\n",
    "                df[col] = df[col].apply(lambda x: -3 if x < -3 else x)\n",
    "\n",
    "        # apply feature scaling only to numerical variables\n",
    "        scaler = StandardScaler()\n",
    "        df[df.columns.difference(self.cols_cat)] = scaler.fit_transform(df[df.columns.difference(self.cols_cat)])\n",
    "\n",
    "        # use PCA to reduce dimension to 40\n",
    "        #self.pca = PCA(n_components=40)\n",
    "        #df = pd.DataFrame(self.pca.fit_transform(df), columns=[f\"V{i}\" for i in range(1, 41)])\n",
    "\n",
    "        # drop duplicates\n",
    "        df = df.drop_duplicates()\n",
    "\n",
    "        return df\n",
    "    \n",
    "    def preprocess_tabular_predict(self, X):\n",
    "        \"\"\"\n",
    "        Preprocess the tabular data.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pandas Dataframe of shape (n_samples, n_features)\n",
    "            Input data.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        pandas Dataframe of shape (n_samples, n_features)\n",
    "            Preprocessed data.\n",
    "        \"\"\"\n",
    "        # encode categorical variables\n",
    "        encoder = LabelEncoder()\n",
    "        for col in X.columns:\n",
    "            if col in self.cols_cat:\n",
    "                X[col] = encoder.fit_transform(X[col].astype(str))\n",
    "                \n",
    "        # drop categorical variables from self.drop_cols\n",
    "        #X = X.drop(self.drop_cols, axis=1)\n",
    "        \n",
    "        # replace missing values with mode\n",
    "        imputer = SimpleImputer(strategy='most_frequent')\n",
    "        df = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
    "\n",
    "        # cap the outlier to 3 std only for numerical variables\n",
    "        for col in df.columns:\n",
    "            if col not in self.cols_cat:\n",
    "                df[col] = df[col].apply(lambda x: 3 if x > 3 else x)\n",
    "                df[col] = df[col].apply(lambda x: -3 if x < -3 else x)\n",
    "\n",
    "        # apply feature scaling only to numerical variables\n",
    "        scaler = StandardScaler()\n",
    "        df[df.columns.difference(self.cols_cat)] = scaler.fit_transform(df[df.columns.difference(self.cols_cat)])\n",
    "\n",
    "        # use PCA to reduce dimension to 40\n",
    "        # df = pd.DataFrame(self.pca.fit_transform(df), columns=[f\"V{i}\" for i in range(1,41)])\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv(os.path.join('data', 'tabular.csv'))\n",
    "with open(os.path.join('data', 'images.npy'), 'rb') as f:\n",
    "    images = np.load(f)\n",
    "    \n",
    "# Exclude target column\n",
    "X_columns = [col for col in df.columns if col != 'target']\n",
    "\n",
    "# Create X_dict and y\n",
    "X_dict = {\n",
    "    'tabular': df[X_columns],\n",
    "    'images': images\n",
    "}\n",
    "y = df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train and test\n",
    "X_dict_train, y_train, X_dict_test, y_test = dict_train_test_split(X_dict, y, ratio=0.9)\n",
    "\n",
    "# Train and predict\n",
    "model = Model()\n",
    "model.fit(X_dict_train, y_train)\n",
    "y_pred = model.predict(X_dict_test)\n",
    "\n",
    "# Evaluate model predition\n",
    "# Learn more: https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics\n",
    "print(\"MSE: {0:.2f}\".format(mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the first 10 predictions with ground truth less than 50, and the corresponding ground truth\n",
    "print(\"First 10 predictions with ground truth less than 50:\")\n",
    "print(y_pred[y_test < 50].head(10))\n",
    "print(\"Ground truth:\")\n",
    "print(y_test[y_test < 50].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "################################\n",
    "## Benchmarking PCA dimension ##\n",
    "################################\n",
    "\n",
    "# Function to benchmark model\n",
    "def benchmark_model(X_dict, y, pca_dimensions):\n",
    "    mse_scores = []\n",
    "    times = []\n",
    "\n",
    "    # Split train and test only once\n",
    "    X_dict_train, y_train, X_dict_test, y_test = dict_train_test_split(X_dict, y, ratio=0.1)\n",
    "    \n",
    "    for dim in pca_dimensions:\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Train and predict using the same train and test data\n",
    "        model = Model()\n",
    "        model.fit(X_dict_train, y_train, n_pca_components=dim)\n",
    "        y_pred = model.predict(X_dict_test)\n",
    "\n",
    "        # Evaluate model prediction\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        mse_scores.append(mse)\n",
    "        times.append(time.time() - start_time)\n",
    "        print(f\"MSE with PCA dimension {dim}: {mse:.2f}\")\n",
    "\n",
    "    return mse_scores, times\n",
    "\n",
    "# Run benchmark\n",
    "pca_dimensions = [10, 15, 20, 25, 30]\n",
    "mse_scores, times = benchmark_model(X_dict, y, pca_dimensions)\n",
    "\n",
    "# [Plotting code remains the same]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "############################\n",
    "## Benchmarking max depth ##\n",
    "############################\n",
    "\n",
    "# Function to benchmark model\n",
    "def benchmark_model(X_dict, y, pca_dimensions):\n",
    "    mse_scores = []\n",
    "    times = []\n",
    "\n",
    "    # Split train and test only once\n",
    "    X_dict_train, y_train, X_dict_test, y_test = dict_train_test_split(X_dict, y, ratio=0.1)\n",
    "    \n",
    "    for dep in max_depths:\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Train and predict using the same train and test data\n",
    "        model = Model()\n",
    "        model.fit(X_dict_train, y_train, max_depth=dep)\n",
    "        y_pred = model.predict(X_dict_test)\n",
    "\n",
    "        # Evaluate model prediction\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        mse_scores.append(mse)\n",
    "        times.append(time.time() - start_time)\n",
    "        print(f\"MSE with Max Depth {dep}: {mse:.2f}\")\n",
    "\n",
    "    return mse_scores, times\n",
    "\n",
    "# Run benchmark\n",
    "max_depths = [10, 30, 50, 70, 90 ,110, 130, 150, 170]\n",
    "mse_scores, times = benchmark_model(X_dict, y, max_depths)\n",
    "\n",
    "# [Plotting code remains the same]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(max_depths, mse_scores, marker='o')\n",
    "plt.title(\"MSE vs Max Depths\")\n",
    "plt.xlabel(\"Max Depths\")\n",
    "plt.ylabel(\"Mean Squared Error\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(max_depths, times, marker='o', color='orange')\n",
    "plt.title(\"Training Time vs Max Depths\")\n",
    "plt.xlabel(\"Max Depths\")\n",
    "plt.ylabel(\"Time (seconds)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "environment_CS2109S",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
