{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up\n",
    "Run the cell below before running the rest of the cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "torch.manual_seed(2109)\n",
    "\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1 Back Propagation\n",
    "We provide the neural network architecture in PyTorch code below, as well as the loss function. Check your answers to part (c) by replacing `raise NotImplementedError()` with your answer.\n",
    "\n",
    "Note that the matrix dimensions are flipped in PyTorch, where the first dimension is the number of training samples. This is just a difference in conventions, and will not affect much. Also note that we don't have the bias term in `X`, this is because the `nn.Linear` layer has it own bias included, and it will not affect your answers, since we are only asking for weight gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Define the inputs and outputs, the network, and the loss function \n",
    "'''\n",
    "n = 16  # Number of training samples\n",
    "m_0 = 8  # Number of features for each training sample\n",
    "\n",
    "X = torch.randn((n, m_0))\n",
    "Y = torch.randint(0, 2, (n, 1), dtype=torch.float32)\n",
    "\n",
    "neural_net = nn.Sequential(OrderedDict([\n",
    "    ('lin1', nn.Linear(m_0, 1)),\n",
    "    ('lin2', nn.Linear(1, 1)),  # Dummy layer so that we can illustrate part (c)\n",
    "    ('sig', nn.Sigmoid())\n",
    "]))\n",
    "\n",
    "# Set dummy layer's weight to 1 to prevent it from affecting the first layer's gradients\n",
    "with torch.no_grad():\n",
    "    neural_net.lin2.weight[0, 0] = 1\n",
    "\n",
    "loss = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Obtain per-sample gradients for our dummy layer, divided by the output of `lin1`, to get the\n",
    "derivative of loss w.r.t. f.\n",
    "\n",
    "Not a common operation, but needed to illustrate part (c) of the tutorial.\n",
    "\n",
    "See https://pytorch.org/functorch/stable/notebooks/per_sample_grads.html for more details \n",
    "about per-sample gradients and how to implement an optimized version of calculating them\n",
    "using functorch, if you're interested.\n",
    "'''\n",
    "per_sample_gradients = torch.zeros((n, 1))\n",
    "lin1_output = torch.zeros((n, 1))\n",
    "\n",
    "for i in range(n):\n",
    "    y_hat = neural_net(X[i])\n",
    "    per_sample_gradients[i] = torch.autograd.grad(loss(y_hat, Y[i]), neural_net.lin2.weight)[0]\n",
    "    lin1_output[i] = neural_net.lin1(X[i])\n",
    "\n",
    "dLoss_df = 1 / n * per_sample_gradients / lin1_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Predict Y using our network, and calculate the loss of our prediction.\n",
    "\n",
    "Note that the gradients will automatically be calculated by PyTorch once you invoke the loss(...).backward() function.\n",
    "'''\n",
    "neural_net.zero_grad()\n",
    "\n",
    "Y_hat = neural_net(X)\n",
    "\n",
    "loss(Y_hat, Y).backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def part_c(Y, Y_hat):\n",
    "    return 1 / n * (Y_hat - Y)\n",
    "\n",
    "assert torch.allclose(part_c(Y, Y_hat), dLoss_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3 Potential Issues with Training Deep Neural Networks\n",
    "Run/Study the code below, and answer 3(a) and 3(b) of the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Create our 50-layer model.\n",
    "'''\n",
    "\n",
    "layer_dict = OrderedDict()\n",
    "for i in range(50):\n",
    "    layer_dict['lin{}'.format(i+1)] = nn.Linear(10, 10 if i < 49 else 5)\n",
    "    layer_dict['act{}'.format(i+1)] = nn.ReLU() # previously sigmoid\n",
    "\n",
    "deep_neural_net = nn.Sequential(layer_dict)\n",
    "\n",
    "deep_X = torch.randn(50, 10)\n",
    "deep_Y = torch.randn(50, 5)\n",
    "\n",
    "deep_Y_hat = deep_neural_net(deep_X)\n",
    "\n",
    "deep_loss = nn.L1Loss()\n",
    "deep_loss(deep_Y_hat, deep_Y).backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Visualize the max gradient magnitude for the first 10 layers. Note the y-axis scale for\n",
    "each plot you generate.\n",
    "\n",
    "Feel free to play around with the method of visualization here.\n",
    "'''\n",
    "\n",
    "max_grad_magnitude_per_layer = []\n",
    "for name, layer in deep_neural_net.named_modules():\n",
    "    if isinstance(layer, nn.Linear) and int(name[3:]) < 10:\n",
    "        max_grad_magnitude_per_layer.append(torch.max(torch.abs(layer.weight.grad)))\n",
    "\n",
    "plt.plot(max_grad_magnitude_per_layer)\n",
    "plt.gca().yaxis.set_major_formatter(FormatStrFormatter('%.1e'))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "open-mmlab",
   "language": "python",
   "name": "open-mmlab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
